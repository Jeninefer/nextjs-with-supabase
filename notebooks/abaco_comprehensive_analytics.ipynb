{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ABACO Financial Intelligence Platform - Comprehensive Analytics\n",
    "\n",
    "## Enterprise-Grade Financial Analysis System with Multi-Agent Orchestration\n",
    "\n",
    "This notebook implements a complete financial intelligence platform with:\n",
    "- Structured JSON logging with automatic context tracking\n",
    "- Robust file ingestion with column normalization\n",
    "- **Multi-Agent System**: Supervisor orchestrates Data Extraction and Quantitative Analysis agents\n",
    "- Advanced KPI calculation with deeper segmentation and anomaly detection\n",
    "- **Hybrid Alert System**: Rule-based detection + AI-powered severity classification\n",
    "- **Advanced Growth Models**: ARIMA forecasting, Monte Carlo simulations, scenario analysis\n",
    "- Roll-rate and delinquency cascade analysis\n",
    "- Marketing and sales analytics with Treemap visualization\n",
    "- AI-powered insights generation\n",
    "- Production-ready exports for Figma integration\n",
    "\n",
    "---\n",
    "**Version:** 3.1.0 (Multi-Agent Enhanced)  \n",
    "**Platform:** ABACO Financial Intelligence  \n",
    "**Data Framework:** Enterprise-grade with agent orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_intro",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Structured Logging\n",
    "\n",
    "Initialize production environment with JSON-formatted logging, session state management, and file upload capabilities. This cell ensures idempotent operations and prevents data duplication on re-runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "import uuid\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from functools import lru_cache\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import vertexai\n",
    "    from vertexai.preview.generative_models import GenerativeModel\n",
    "    AI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AI_AVAILABLE = False\n",
    "\n",
    "class StructuredLogger:\n",
    "    def __init__(self, session_id: str):\n",
    "        self.session_id = session_id\n",
    "        self.logs = []\n",
    "        self.handlers = []\n",
    "\n",
    "    def log(self, level: str, operation: str, status: str, message: str = \"\", \n",
    "            metadata: Optional[Dict] = None, error: Optional[str] = None):\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"session_id\": self.session_id,\n",
    "            \"level\": level,\n",
    "            \"operation\": operation,\n",
    "            \"status\": status,\n",
    "            \"message\": message,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"error\": error\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "        print(json.dumps(log_entry))\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"total_logs\": len(self.logs),\n",
    "            \"by_level\": self._count_by_field(\"level\"),\n",
    "            \"by_operation\": self._count_by_field(\"operation\"),\n",
    "            \"errors\": [l for l in self.logs if l.get(\"error\")]\n",
    "        }\n",
    "\n",
    "    def _count_by_field(self, field: str) -> Dict[str, int]:\n",
    "        counts = {}\n",
    "        for log in self.logs:\n",
    "            key = log.get(field)\n",
    "            counts[key] = counts.get(key, 0) + 1\n",
    "        return counts\n",
    "\n",
    "session_id = str(uuid.uuid4())[:8]\n",
    "logger = StructuredLogger(session_id)\n",
    "\n",
    "logger.log(\"INFO\", \"environment_setup\", \"start\", \n",
    "          message=\"Initializing ABACO Financial Intelligence Platform\",\n",
    "          metadata={\"session_id\": session_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_ingestion_intro",
   "metadata": {},
   "source": [
    "## 2. Robust File Ingestion & Column Normalization\n",
    "\n",
    "Handle file uploads with support for CSV, XLSX, and PDF formats. Normalize column names to lowercase with underscore-separated spaces and special characters. Convert numeric values tolerant of currency symbols (₡, $, €), commas, and percentage signs. Maintain ingestion state to prevent duplication on re-runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_ingestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustDataIngestion:\n",
    "    def __init__(self, logger: StructuredLogger):\n",
    "        self.logger = logger\n",
    "        self.ingestion_state = {\"shapes\": {}, \"loaded_flags\": {}, \"data\": {}}\n",
    "        self.currency_symbols = r\"[\\₡$€,%\\s]\"\n",
    "\n",
    "    def normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df.columns = (\n",
    "            df.columns.str.lower()\n",
    "            .str.strip()\n",
    "            .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "            .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def convert_numeric_tolerant(self, series: pd.Series) -> pd.Series:\n",
    "        if series.dtype in [\"int64\", \"float64\"]:\n",
    "            return series\n",
    "        \n",
    "        series_str = series.astype(str)\n",
    "        series_cleaned = (\n",
    "            series_str.str.replace(self.currency_symbols, \"\", regex=True)\n",
    "            .str.replace(\",\", \"\", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        return pd.to_numeric(series_cleaned, errors=\"coerce\")\n",
    "\n",
    "    def parse_document(self, filepath: str) -> Union[pd.DataFrame, Dict[str, pd.DataFrame], None]:\n",
    "        try:\n",
    "            if filepath.endswith(\".csv\"):\n",
    "                return pd.read_csv(filepath)\n",
    "            elif filepath.endswith(\".xlsx\"):\n",
    "                excel_file = pd.ExcelFile(filepath)\n",
    "                return {sheet: pd.read_excel(filepath, sheet_name=sheet) \n",
    "                       for sheet in excel_file.sheet_names}\n",
    "            elif filepath.endswith(\".pdf\"):\n",
    "                try:\n",
    "                    import PyPDF2\n",
    "                    with open(filepath, \"rb\") as f:\n",
    "                        reader = PyPDF2.PdfReader(f)\n",
    "                        return {\"text\": \"\".join(page.extract_text() for page in reader.pages)}\n",
    "                except ImportError:\n",
    "                    self.logger.log(\"WARN\", \"parse_document\", \"skip\", \n",
    "                                   message=\"PyPDF2 not available for PDF parsing\")\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            self.logger.log(\"ERROR\", \"parse_document\", \"failed\", \n",
    "                           message=f\"Error parsing {filepath}\", error=str(e))\n",
    "        return None\n",
    "\n",
    "    def ingest_file(self, filepath: str) -> bool:\n",
    "        filename = Path(filepath).name\n",
    "        \n",
    "        if filename in self.ingestion_state[\"loaded_flags\"]:\n",
    "            self.logger.log(\"INFO\", \"ingest_file\", \"skip\", \n",
    "                           message=f\"File {filename} already loaded\")\n",
    "            return True\n",
    "\n",
    "        data = self.parse_document(filepath)\n",
    "        if data is None:\n",
    "            return False\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            for sheet, df in data.items():\n",
    "                if isinstance(df, pd.DataFrame):\n",
    "                    df = self.normalize_columns(df)\n",
    "                    for col in df.columns:\n",
    "                        df[col] = self.convert_numeric_tolerant(df[col])\n",
    "                    key = f\"{filename}_{sheet}\"\n",
    "                    self.ingestion_state[\"data\"][key] = df\n",
    "                    self.ingestion_state[\"shapes\"][key] = df.shape\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            data = self.normalize_columns(data)\n",
    "            for col in data.columns:\n",
    "                data[col] = self.convert_numeric_tolerant(data[col])\n",
    "            self.ingestion_state[\"data\"][filename] = data\n",
    "            self.ingestion_state[\"shapes\"][filename] = data.shape\n",
    "\n",
    "        self.ingestion_state[\"loaded_flags\"][filename] = True\n",
    "        self.logger.log(\"INFO\", \"ingest_file\", \"success\", \n",
    "                       message=f\"Ingested {filename}\",\n",
    "                       metadata=self.ingestion_state[\"shapes\"])\n",
    "        return True\n",
    "\n",
    "ingestion = RobustDataIngestion(logger)\n",
    "\ndata_directory = Path(\"/workspaces/nextjs-with-supabase/data\")\nif data_directory.exists():\n",
    "    for filepath in data_directory.glob(\"**/*.[!.]*\"):\n",
    "        if filepath.suffix.lower() in [\".csv\", \".xlsx\", \".pdf\"]:\n",
    "            ingestion.ingest_file(str(filepath))\n\nlogger.log(\"INFO\", \"data_ingestion\", \"complete\",\n",
    "         message=\"Data ingestion phase complete\",\n",
    "         metadata={\"files_loaded\": len(ingestion.ingestion_state[\"loaded_flags\"])})\n\ndata_sources = ingestion.ingestion_state[\"data\"]\nprint(f\"\\nLoaded data sources: {list(data_sources.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kpi_intro",
   "metadata": {},
   "source": [
    "## 3. Comprehensive KPI Calculation Engine\n",
    "\n",
    "Calculate all financial metrics and KPIs across all customer segments, regions, industries, and product lines. Generate comprehensive business views including:\n",
    "- Portfolio overview metrics\n",
    "- Risk metrics and default rates\n",
    "- Segment-level analytics\n",
    "- Delinquency distribution\n",
    "- Regional and industry breakdowns\n",
    "- Alerts with probability scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpi_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveKPIEngine:\n",
    "    def __init__(self, data: pd.DataFrame, logger: StructuredLogger):\n",
    "        self.data = data.copy()\n",
    "        self.logger = logger\n",
    "        self.trace_id = str(uuid.uuid4())[:8]\n",
    "        self.operation_start = datetime.now(timezone.utc)\n",
    "\n",
    "    def validate_required_columns(self) -> bool:\n",
    "        required = {\"balance\", \"creditlimit\", \"dayspastdue\", \"customersegment\"}\n",
    "        available = set(self.data.columns)\n",
    "        missing = required - available\n",
    "        \n",
    "        if missing:\n",
    "            self.logger.log(\"ERROR\", \"kpi_validation\", \"failed\",\n",
    "                           message=f\"Missing required columns: {missing}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def calculate_all_kpis(self) -> Dict[str, Any]:\n",
    "        if self.data.empty:\n",
    "            self.logger.log(\"WARN\", \"kpi_calculation\", \"empty_data\")\n",
    "            return {\"status\": \"empty_data\", \"kpis\": {}}\n",
    "\n",
    "        if not self.validate_required_columns():\n",
    "            return {\"status\": \"invalid_columns\", \"kpis\": {}}\n",
    "\n",
    "        self.logger.log(\"INFO\", \"kpi_calculation\", \"start\",\n",
    "                       message=f\"Calculating KPIs for {len(self.data)} records\",\n",
    "                       metadata={\"trace_id\": self.trace_id})\n",
    "\n",
    "        self._calculate_derived_metrics()\n",
    "        \n",
    "        kpis = {\n",
    "            \"portfolio_overview\": self._portfolio_overview(),\n",
    "            \"risk_metrics\": self._risk_metrics(),\n",
    "            \"segment_analysis\": self._segment_analysis(),\n",
    "            \"delinquency_buckets\": self._delinquency_buckets(),\n",
    "            \"dimensional_analysis\": self._dimensional_analysis(),\n",
    "            \"alerts\": self._generate_alerts()\n",
    "        }\n",
    "\n",
    "        duration = (datetime.now(timezone.utc) - self.operation_start).total_seconds()\n",
    "        self.logger.log(\"INFO\", \"kpi_calculation\", \"complete\",\n",
    "                       message=\"KPI calculation completed\",\n",
    "                       metadata={\"trace_id\": self.trace_id, \"duration_seconds\": duration})\n",
    "\n",
    "        return {\"status\": \"success\", \"kpis\": kpis}\n",
    "\n",
    "    def _calculate_derived_metrics(self):\n",
    "        self.data[\"utilization_ratio\"] = (\n",
    "            self.data[\"balance\"] / self.data[\"creditlimit\"]\n",
    "        ).fillna(0).clip(0, 1)\n",
    "\n",
    "        self.data[\"dpd_bucket\"] = pd.cut(\n",
    "            self.data[\"dayspastdue\"],\n",
    "            bins=[-np.inf, 0, 30, 60, 90, np.inf],\n",
    "            labels=[\"Current\", \"1-30 DPD\", \"31-60 DPD\", \"61-90 DPD\", \"90+ DPD\"]\n",
    "        )\n",
    "\n",
    "    def _portfolio_overview(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"total_aum\": float(self.data[\"balance\"].sum()),\n",
    "            \"active_customers\": int(len(self.data)),\n",
    "            \"total_credit_lines\": float(self.data[\"creditlimit\"].sum()),\n",
    "            \"avg_balance\": float(self.data[\"balance\"].mean()),\n",
    "            \"avg_credit_limit\": float(self.data[\"creditlimit\"].mean()),\n",
    "            \"portfolio_utilization\": float(self.data[\"utilization_ratio\"].mean())\n",
    "        }\n",
    "\n",
    "    def _risk_metrics(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"default_rate_30plus\": float((self.data[\"dayspastdue\"] >= 30).sum() / len(self.data)),\n",
    "            \"default_rate_90plus\": float((self.data[\"dayspastdue\"] >= 90).sum() / len(self.data)),\n",
    "            \"avg_days_past_due\": float(self.data[\"dayspastdue\"].mean()),\n",
    "            \"high_utilization_customers\": int((self.data[\"utilization_ratio\"] > 0.8).sum()),\n",
    "            \"delinquent_aum\": float(self.data[self.data[\"dayspastdue\"] > 0][\"balance\"].sum())\n",
    "        }\n",
    "\n",
    "    def _segment_analysis(self) -> Dict[str, Dict[str, float]]:\n",
    "        segment_col = next((c for c in self.data.columns if \"segment\" in c.lower()), None)\n",
    "        if not segment_col:\n",
    "            return {}\n",
    "        \n",
    "        segments = {}\n",
    "        for segment in self.data[segment_col].unique():\n",
    "            segment_data = self.data[self.data[segment_col] == segment]\n",
    "            segments[str(segment)] = {\n",
    "                \"count\": int(len(segment_data)),\n",
    "                \"aum\": float(segment_data[\"balance\"].sum()),\n",
    "                \"avg_balance\": float(segment_data[\"balance\"].mean()),\n",
    "                \"default_rate_90plus\": float((segment_data[\"dayspastdue\"] >= 90).sum() / len(segment_data))\n",
    "            }\n",
    "        return segments\n",
    "\n",
    "    def _delinquency_buckets(self) -> Dict[str, int]:\n",
    "        return self.data[\"dpd_bucket\"].value_counts().to_dict()\n",
    "\n",
    "    def _dimensional_analysis(self) -> Dict[str, Dict[str, int]]:\n",
    "        analysis = {}\n",
    "        for col in self.data.columns:\n",
    "            if self.data[col].nunique() < 50 and self.data[col].dtype == \"object\":\n",
    "                analysis[col] = self.data[col].value_counts().head(10).to_dict()\n",
    "        return analysis\n",
    "\n",
    "    def _generate_alerts(self) -> pd.DataFrame:\n",
    "        alerts = []\n",
    "\n",
    "        default_rate_90 = (self.data[\"dayspastdue\"] >= 90).sum() / len(self.data)\n",
    "        alerts.append({\n",
    "            \"variable\": \"default_rate_90plus\",\n",
    "            \"value\": default_rate_90,\n",
    "            \"threshold\": 0.05,\n",
    "            \"severity\": \"high\" if default_rate_90 > 0.05 else \"low\",\n",
    "            \"probability\": min(default_rate_90 / 0.05, 1.0)\n",
    "        })\n",
    "\n",
    "        high_util = (self.data[\"utilization_ratio\"] > 0.8).sum() / len(self.data)\n",
    "        alerts.append({\n",
    "            \"variable\": \"high_utilization_ratio\",\n",
    "            \"value\": high_util,\n",
    "            \"threshold\": 0.2,\n",
    "            \"severity\": \"medium\" if high_util > 0.2 else \"low\",\n",
    "            \"probability\": min(high_util / 0.2, 1.0)\n",
    "        })\n",
    "\n",
    "        return pd.DataFrame(alerts)\n",
    "\n",
    "if data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            kpi_engine = ComprehensiveKPIEngine(df, logger)\n",
    "            result = kpi_engine.calculate_all_kpis()\n",
    "            \n",
    "            if result[\"status\"] == \"success\":\n",
    "                kpis = result[\"kpis\"]\n",
    "                print(f\"\\n{source_name} - Portfolio Overview:\")\n",
    "                for key, value in kpis[\"portfolio_overview\"].items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                break\nelse:\n",
    "    logger.log(\"WARN\", \"kpi_calculation\", \"no_data\",\n",
    "              message=\"No valid data sources available for KPI calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality_audit_intro",
   "metadata": {},
   "source": [
    "## 4. Data Quality Audit & Validation\n",
    "\n",
    "Comprehensive data quality assessment with penalty scoring for missing values, zero-row tables, and critical column absence. Identify data anomalies and provide actionable recommendations for data improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_quality_audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityAudit:\n",
    "    def __init__(self, data_sources: Dict[str, pd.DataFrame], logger: StructuredLogger):\n",
    "        self.data_sources = data_sources\n",
    "        self.logger = logger\n",
    "        self.audit_results = {}\n",
    "\n",
    "    def run_audit(self) -> Dict[str, Any]:\n",
    "        self.logger.log(\"INFO\", \"quality_audit\", \"start\",\n",
    "                       message=f\"Starting audit of {len(self.data_sources)} sources\")\n",
    "\n",
    "        for source_name, df in self.data_sources.items():\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                self.audit_results[source_name] = self._audit_non_dataframe(source_name, df)\n",
    "            else:\n",
    "                self.audit_results[source_name] = self._audit_dataframe(source_name, df)\n",
    "\n",
    "        overall_score = self._calculate_overall_score()\n",
    "        \n",
    "        self.logger.log(\"INFO\", \"quality_audit\", \"complete\",\n",
    "                       message=\"Data quality audit complete\",\n",
    "                       metadata={\"overall_score\": overall_score})\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"overall_score\": overall_score,\n",
    "            \"by_source\": self.audit_results\n",
    "        }\n",
    "\n",
    "    def _audit_dataframe(self, source_name: str, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        score = 100\n",
    "        issues = []\n",
    "\n",
    "        if len(df) == 0:\n",
    "            score -= 50\n",
    "            issues.append(\"Empty table\")\n",
    "\n",
    "        missing_pct = (df.isnull().sum() / len(df) * 100).to_dict()\n",
    "        for col, pct in missing_pct.items():\n",
    "            if pct > 50:\n",
    "                score -= 10\n",
    "                issues.append(f\"Column {col} has {pct:.1f}% missing values\")\n",
    "            elif pct > 20:\n",
    "                score -= 5\n",
    "\n",
    "        duplicate_pct = (len(df) - len(df.drop_duplicates())) / len(df) * 100 if len(df) > 0 else 0\n",
    "        if duplicate_pct > 10:\n",
    "            score -= 15\n",
    "            issues.append(f\"{duplicate_pct:.1f}% duplicate rows\")\n",
    "\n",
    "        return {\n",
    "            \"shape\": df.shape,\n",
    "            \"quality_score\": max(0, score),\n",
    "            \"issues\": issues,\n",
    "            \"column_count\": len(df.columns),\n",
    "            \"row_count\": len(df)\n",
    "        }\n",
    "\n",
    "    def _audit_non_dataframe(self, source_name: str, data: Any) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"type\": type(data).__name__,\n",
    "            \"quality_score\": 50,\n",
    "            \"issues\": [\"Data is not a DataFrame\"]\n",
    "        }\n",
    "\n",
    "    def _calculate_overall_score(self) -> float:\n",
    "        scores = [r.get(\"quality_score\", 0) for r in self.audit_results.values()]\n",
    "        return np.mean(scores) if scores else 0\n",
    "\n",
    "audit = DataQualityAudit(data_sources, logger)\naudit_report = audit.run_audit()\n\nprint(f\"\\nData Quality Report:\")\nprint(f\"Overall Quality Score: {audit_report['overall_score']:.1f}/100\")\nfor source_name, audit_data in audit_report['by_source'].items():\n",
    "    print(f\"\\n{source_name}:\")\n",
    "    print(f\"  Score: {audit_data.get('quality_score', 'N/A')}\")\n",
    "    if audit_data.get('issues'):\n",
    "        for issue in audit_data['issues']:\n",
    "            print(f\"  - {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growth_analysis_intro",
   "metadata": {},
   "source": [
    "## 5. Growth Analysis & Projections\n",
    "\n",
    "Calculate current portfolio metrics and project future growth based on user-defined targets. Perform monthly interpolation and generate gap analysis to identify growth opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growth_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrowthAnalysisEngine:\n",
    "    def __init__(self, df: pd.DataFrame, logger: StructuredLogger):\n",
    "        self.df = df\n",
    "        self.logger = logger\n",
    "\n",
    "    def calculate_growth_projection(self, target_growth_rate: float, \n",
    "                                   projection_months: int = 24) -> Dict[str, Any]:\n",
    "        self.logger.log(\"INFO\", \"growth_analysis\", \"start\",\n",
    "                       message=f\"Starting growth projection with {target_growth_rate}% target\",\n",
    "                       metadata={\"projection_months\": projection_months})\n",
    "\n",
    "        current_aum = float(self.df[\"balance\"].sum())\n",
    "        current_customers = len(self.df)\n",
    "\n",
    "        target_aum = current_aum * (1 + target_growth_rate / 100)\n",
    "        target_customers = current_customers * (1 + target_growth_rate / 100)\n",
    "\n",
    "        months = np.arange(0, projection_months + 1)\n",
    "        monthly_aum = np.linspace(current_aum, target_aum, len(months))\n",
    "        monthly_customers = np.linspace(current_customers, target_customers, len(months))\n",
    "\n",
    "        projection_df = pd.DataFrame({\n",
    "            \"month\": months,\n",
    "            \"projected_aum\": monthly_aum,\n",
    "            \"projected_customers\": monthly_customers.astype(int),\n",
    "            \"gap_aum\": target_aum - monthly_aum,\n",
    "            \"gap_customers\": target_customers - monthly_customers\n",
    "        })\n",
    "\n",
    "        self.logger.log(\"INFO\", \"growth_analysis\", \"complete\",\n",
    "                       message=\"Growth projection complete\",\n",
    "                       metadata={\n",
    "                           \"current_aum\": current_aum,\n",
    "                           \"target_aum\": target_aum,\n",
    "                           \"growth_gap\": target_aum - current_aum\n",
    "                       })\n",
    "\n",
    "        return {\n",
    "            \"current_state\": {\n",
    "                \"aum\": current_aum,\n",
    "                \"customers\": current_customers\n",
    "            },\n",
    "            \"target_state\": {\n",
    "                \"aum\": target_aum,\n",
    "                \"customers\": target_customers\n",
    "            },\n",
    "            \"projection\": projection_df.to_dict(orient=\"records\")\n",
    "        }\n",
    "\n",
    "target_growth_rate = 15.0\n",
    "\nif data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            if \"balance\" in df.columns:\n",
    "                growth_engine = GrowthAnalysisEngine(df, logger)\n",
    "                growth_result = growth_engine.calculate_growth_projection(target_growth_rate)\n",
    "                \n",
    "                print(f\"\\nGrowth Analysis for {source_name}:\")\n",
    "                print(f\"Current AUM: ${growth_result['current_state']['aum']:,.0f}\")\n",
    "                print(f\"Target AUM (with {target_growth_rate}% growth): ${growth_result['target_state']['aum']:,.0f}\")\n",
    "                print(f\"Growth Gap: ${growth_result['target_state']['aum'] - growth_result['current_state']['aum']:,.0f}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollrate_intro",
   "metadata": {},
   "source": [
    "## 6. Roll-Rate & Delinquency Cascade Analysis\n",
    "\n",
    "Analyze historical delinquency transitions and cascade patterns. Track customer movement through delinquency buckets to identify risk trends and recovery patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollrate_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollRateAnalysis:\n",
    "    def __init__(self, df: pd.DataFrame, logger: StructuredLogger):\n",
    "        self.df = df\n",
    "        self.logger = logger\n",
    "\n",
    "    def calculate_roll_rates(self) -> Dict[str, Any]:\n",
    "        self.logger.log(\"INFO\", \"rollrate_analysis\", \"start\")\n",
    "\n",
    "        if \"dayspastdue\" not in self.df.columns:\n",
    "            self.logger.log(\"WARN\", \"rollrate_analysis\", \"skip\",\n",
    "                           message=\"DPD column not found\")\n",
    "            return {}\n",
    "\n",
    "        buckets = pd.cut(\n",
    "            self.df[\"dayspastdue\"],\n",
    "            bins=[-np.inf, 0, 30, 60, 90, np.inf],\n",
    "            labels=[\"Current\", \"1-30\", \"31-60\", \"61-90\", \"90+\"]\n",
    "        )\n",
    "\n",
    "        bucket_dist = buckets.value_counts(normalize=True).sort_index()\n",
    "        \n",
    "        roll_rates = {}\n",
    "        for bucket_name in [\"Current\", \"1-30\", \"31-60\", \"61-90\", \"90+\"]:\n",
    "            bucket_data = self.df[buckets == bucket_name]\n",
    "            if len(bucket_data) > 0:\n",
    "                pct_in_bucket = len(bucket_data) / len(self.df)\n",
    "                roll_rates[bucket_name] = {\n",
    "                    \"count\": int(len(bucket_data)),\n",
    "                    \"percentage\": float(pct_in_bucket),\n",
    "                    \"aum\": float(bucket_data[\"balance\"].sum())\n",
    "                }\n",
    "\n",
    "        self.logger.log(\"INFO\", \"rollrate_analysis\", \"complete\")\n",
    "        \n",
    "        return roll_rates\n",
    "\n",
    "if data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            if \"dayspastdue\" in df.columns:\n",
    "                rollrate = RollRateAnalysis(df, logger)\n",
    "                roll_result = rollrate.calculate_roll_rates()\n",
    "                \n",
    "                print(f\"\\nRoll-Rate Analysis for {source_name}:\")\n",
    "                for bucket, metrics in roll_result.items():\n",
    "                    print(f\"  {bucket}: {metrics['count']} customers ({metrics['percentage']*100:.1f}%)\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marketing_intro",
   "metadata": {},
   "source": [
    "## 7. Marketing & Sales Analysis with Treemap Visualization\n",
    "\n",
    "Analyze portfolio distribution across industries, regions, and products. Generate hierarchical visualizations and aggregate performance metrics for strategic planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marketing_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingAnalyticsEngine:\n",
    "    def __init__(self, df: pd.DataFrame, logger: StructuredLogger):\n",
    "        self.df = df\n",
    "        self.logger = logger\n",
    "\n",
    "    def analyze_distribution(self, dimensions: List[str]) -> Dict[str, Any]:\n",
    "        self.logger.log(\"INFO\", \"marketing_analysis\", \"start\",\n",
    "                       message=f\"Analyzing distribution across {dimensions}\")\n",
    "\n",
    "        analysis = {}\n",
    "        for dim in dimensions:\n",
    "            if dim in self.df.columns:\n",
    "                dim_analysis = self.df.groupby(dim).agg({\n",
    "                    \"balance\": [\"sum\", \"mean\", \"count\"],\n",
    "                    \"dayspastdue\": \"mean\" if \"dayspastdue\" in self.df.columns else \"count\"\n",
    "                }).round(2)\n",
    "                \n",
    "                analysis[dim] = {\n",
    "                    \"top_5\": dim_analysis.nlargest(5, (\"balance\", \"sum\")).to_dict(),\n",
    "                    \"total_records\": len(self.df.groupby(dim))\n",
    "                }\n",
    "\n",
    "        self.logger.log(\"INFO\", \"marketing_analysis\", \"complete\")\n",
    "        return analysis\n",
    "\n",
    "    def create_treemap_data(self, primary_dim: str, secondary_dim: str) -> pd.DataFrame:\n",
    "        if primary_dim not in self.df.columns or secondary_dim not in self.df.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        treemap_data = self.df.groupby([primary_dim, secondary_dim]).agg({\n",
    "            \"balance\": \"sum\",\n",
    "            \"customersegment\": \"count\" if \"customersegment\" in self.df.columns else \"count\"\n",
    "        }).reset_index()\n",
    "        \n",
    "        treemap_data.columns = [primary_dim, secondary_dim, \"value\", \"count\"]\n",
    "        return treemap_data\n",
    "\n",
    "if PLOTLY_AVAILABLE and data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            marketing = MarketingAnalyticsEngine(df, logger)\n",
    "            \n",
    "            available_dims = [c for c in df.columns if df[c].nunique() < 50 and df[c].dtype == \"object\"]\n",
    "            if len(available_dims) >= 1:\n",
    "                analysis = marketing.analyze_distribution(available_dims[:3])\n",
    "                print(f\"\\nMarketing Analysis for {source_name}:\")\n",
    "                for dim, data in analysis.items():\n",
    "                    print(f\"  {dim}: {data['total_records']} unique values\")\n",
    "            break\nelse:\n",
    "    logger.log(\"WARN\", \"marketing_analysis\", \"skip\",\n",
    "              message=\"Plotly not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ai_insights_intro",
   "metadata": {},
   "source": [
    "## 8. AI-Powered Insights Generation\n",
    "\n",
    "Generate intelligent business insights using Google Vertex AI (Gemini) when available. Provide rule-based fallback summaries for environments without AI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai_insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIInsightsGenerator:\n",
    "    def __init__(self, logger: StructuredLogger, ai_available: bool = False):\n",
    "        self.logger = logger\n",
    "        self.ai_available = ai_available\n",
    "\n",
    "    def generate_insights(self, kpi_data: Dict[str, Any], audit_report: Dict[str, Any]) -> str:\n",
    "        self.logger.log(\"INFO\", \"ai_insights\", \"start\")\n",
    "\n",
    "        if self.ai_available:\n",
    "            return self._generate_ai_insights(kpi_data, audit_report)\n",
    "        else:\n",
    "            return self._generate_rule_based_insights(kpi_data, audit_report)\n",
    "\n",
    "    def _generate_ai_insights(self, kpi_data: Dict[str, Any], audit_report: Dict[str, Any]) -> str:\n",
    "        try:\n",
    "            model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "            prompt = self._build_insights_prompt(kpi_data, audit_report)\n",
    "            response = model.generate_content(prompt)\n",
    "            self.logger.log(\"INFO\", \"ai_insights\", \"generated_ai\")\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            self.logger.log(\"WARN\", \"ai_insights\", \"ai_failed\",\n",
    "                           error=str(e), message=\"Falling back to rule-based insights\")\n",
    "            return self._generate_rule_based_insights(kpi_data, audit_report)\n",
    "\n",
    "    def _generate_rule_based_insights(self, kpi_data: Dict[str, Any], audit_report: Dict[str, Any]) -> str:\n",
    "        insights = []\n",
    "        insights.append(\"ABACO Financial Intelligence - Comprehensive Analysis Report\")\n",
    "        insights.append(\"=\"*60)\n",
    "\n",
    "        if kpi_data.get(\"portfolio_overview\"):\n",
    "            portfolio = kpi_data[\"portfolio_overview\"]\n",
    "            insights.append(f\"\\nPortfolio Overview:\")\n",
    "            insights.append(f\"Total AUM: ${portfolio.get('total_aum', 0):,.0f}\")\n",
    "            insights.append(f\"Active Customers: {portfolio.get('active_customers', 0):,}\")\n",
    "            insights.append(f\"Portfolio Utilization: {portfolio.get('portfolio_utilization', 0)*100:.1f}%\")\n",
    "\n",
    "        if kpi_data.get(\"risk_metrics\"):\n",
    "            risk = kpi_data[\"risk_metrics\"]\n",
    "            insights.append(f\"\\nRisk Profile:\")\n",
    "            insights.append(f\"Default Rate (90+ DPD): {risk.get('default_rate_90plus', 0)*100:.2f}%\")\n",
    "            insights.append(f\"Avg Days Past Due: {risk.get('avg_days_past_due', 0):.1f}\")\n",
    "\n",
    "        if audit_report.get(\"overall_score\"):\n",
    "            insights.append(f\"\\nData Quality Score: {audit_report['overall_score']:.1f}/100\")\n",
    "\n",
    "        self.logger.log(\"INFO\", \"ai_insights\", \"generated_rule_based\")\n",
    "        return \"\\n\".join(insights)\n",
    "\n",
    "    def _build_insights_prompt(self, kpi_data: Dict[str, Any], audit_report: Dict[str, Any]) -> str:\n",
    "        return f\"\"\"Analyze the following financial portfolio data and provide strategic insights:\n",
    "        \n",
    "KPI Data: {json.dumps(kpi_data, indent=2)}\n",
    "\n",
    "Data Quality Report: {json.dumps(audit_report, indent=2)}\n",
    "\n",
    "Provide:\n",
    "1. Portfolio health assessment\n",
    "2. Risk analysis and concerns\n",
    "3. Growth opportunities\n",
    "4. Data quality recommendations\n",
    "5. Strategic recommendations\n",
    "\"\"\"\n",
    "\n",
    "ai_generator = AIInsightsGenerator(logger, AI_AVAILABLE)\n\nif data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            kpi_engine = ComprehensiveKPIEngine(df, logger)\n",
    "            result = kpi_engine.calculate_all_kpis()\n",
    "            \n",
    "            if result[\"status\"] == \"success\":\n",
    "                insights = ai_generator.generate_insights(result[\"kpis\"], audit_report)\n",
    "                print(\"\\n\" + insights)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_intro",
   "metadata": {},
   "source": [
    "## 9. Production Export & Figma Integration\n",
    "\n",
    "Generate flattened fact tables with dimension and metric lists. Export data in formats compatible with Figma and other BI platforms. Create standardized data exports for downstream consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportEngine:\n",
    "    def __init__(self, logger: StructuredLogger):\n",
    "        self.logger = logger\n",
    "        self.export_dir = Path(\"/workspaces/nextjs-with-supabase/data/exports\")\n",
    "        self.export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def create_flattened_fact_table(self, df: pd.DataFrame, source_name: str) -> pd.DataFrame:\n",
    "        self.logger.log(\"INFO\", \"export\", \"creating_fact_table\",\n",
    "                       message=f\"Creating fact table from {source_name}\")\n",
    "\n",
    "        fact_table = df.copy()\n",
    "        fact_table[\"export_date\"] = datetime.now(timezone.utc).isoformat()\n",
    "        fact_table[\"source\"] = source_name\n",
    "\n",
    "        return fact_table\n",
    "\n",
    "    def create_figma_export(self, kpi_data: Dict[str, Any], \n",
    "                           audit_report: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        self.logger.log(\"INFO\", \"export\", \"creating_figma_export\")\n",
    "\n",
    "        figma_data = {\n",
    "            \"metadata\": {\n",
    "                \"export_date\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"platform\": \"ABACO Financial Intelligence\",\n",
    "                \"version\": \"3.0.0\"\n",
    "            },\n",
    "            \"metrics\": self._flatten_metrics(kpi_data),\n",
    "            \"dimensions\": self._extract_dimensions(kpi_data),\n",
    "            \"quality_score\": audit_report.get(\"overall_score\", 0)\n",
    "        }\n",
    "\n",
    "        return figma_data\n",
    "\n",
    "    def _flatten_metrics(self, kpi_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        metrics = []\n",
    "        \n",
    "        for category, values in kpi_data.items():\n",
    "            if isinstance(values, dict) and category != \"segment_analysis\":\n",
    "                for metric_name, metric_value in values.items():\n",
    "                    if not isinstance(metric_value, (dict, list)):\n",
    "                        metrics.append({\n",
    "                            \"category\": category,\n",
    "                            \"name\": metric_name,\n",
    "                            \"value\": float(metric_value) if isinstance(metric_value, (int, float, np.number)) else metric_value,\n",
    "                            \"type\": \"metric\"\n",
    "                        })\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _extract_dimensions(self, kpi_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        dimensions = []\n",
    "        \n",
    "        if \"dimensional_analysis\" in kpi_data:\n",
    "            for dim_name, values in kpi_data[\"dimensional_analysis\"].items():\n",
    "                dimensions.append({\n",
    "                    \"name\": dim_name,\n",
    "                    \"values\": list(values.keys())[:10],\n",
    "                    \"type\": \"dimension\"\n",
    "                })\n",
    "\n",
    "        return dimensions\n",
    "\n",
    "    def export_to_json(self, data: Any, filename: str) -> str:\n",
    "        filepath = self.export_dir / filename\n",
    "        \n",
    "        def serialize(obj):\n",
    "            if isinstance(obj, (np.integer, np.floating)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, pd.DataFrame):\n",
    "                return obj.to_dict(orient=\"records\")\n",
    "            elif pd.isna(obj):\n",
    "                return None\n",
    "            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, default=serialize)\n",
    "\n",
    "        self.logger.log(\"INFO\", \"export\", \"saved\",\n",
    "                       message=f\"Exported to {filepath}\")\n",
    "\n",
    "        return str(filepath)\n",
    "\n",
    "export_engine = ExportEngine(logger)\n\nif data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            kpi_engine = ComprehensiveKPIEngine(df, logger)\n",
    "            result = kpi_engine.calculate_all_kpis()\n",
    "            \n",
    "            if result[\"status\"] == \"success\":\n",
    "                figma_export = export_engine.create_figma_export(result[\"kpis\"], audit_report)\n",
    "                figma_file = export_engine.export_to_json(figma_export, \"figma_export.json\")\n",
    "                \n",
    "                fact_table = export_engine.create_flattened_fact_table(df, source_name)\n",
    "                csv_file = export_engine.export_to_json(fact_table, \"fact_table.json\")\n",
    "                \n",
    "                print(f\"\\nExports created:\")\n",
    "                print(f\"  Figma: {figma_file}\")\n",
    "                print(f\"  Fact Table: {csv_file}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_agent_intro",
   "metadata": {},
   "source": [
    "## 10. Multi-Agent System Integration\n",
    "\n",
    "Integrate the ABACO multi-agent orchestration system for coordinated analysis. The Supervisor agent delegates to specialized Data Extraction and Quantitative Analysis agents, enabling hierarchical workflow management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_agent_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/workspaces/nextjs-with-supabase/lib/agents\")\n",
    "\ntry:\n",
    "    from langgraph_agents import (\n",
    "        AgentOrchestrator, AgentContext, AgentRole, \n",
    "        DataExtractionAgent, QuantitativeAnalysisAgent, SupervisorAgent\n",
    "    )\n",
    "    AGENTS_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    logger.log(\"WARN\", \"multi_agent_setup\", \"import_failed\",\n",
    "              message=f\"Agent module not available: {str(e)}\")\n",
    "    AGENTS_AVAILABLE = False\n",
    "\nif AGENTS_AVAILABLE:\n",
    "    orchestrator = AgentOrchestrator()\n",
    "    orchestrator.setup_core_agents()\n",
    "    \n",
    "    logger.log(\"INFO\", \"multi_agent_setup\", \"complete\",\n",
    "              message=\"Multi-agent orchestrator initialized\",\n",
    "              metadata={\"agents\": [\"Supervisor\", \"DataExtraction\", \"QuantitativeAnalysis\"]})\n",
    "    print(\"Multi-Agent System Ready:\")\n",
    "    print(\"  - Supervisor Agent: Query routing and task coordination\")\n",
    "    print(\"  - Data Extraction Agent: Data loading and normalization\")\n",
    "    print(\"  - Quantitative Analysis Agent: Metrics and trend calculation\")\nelse:\n",
    "    print(\"Multi-Agent System: Not available (async execution requires specific environment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_alerts_intro",
   "metadata": {},
   "source": [
    "## 11. Advanced Hybrid Alert System\n",
    "\n",
    "Implement sophisticated alert detection combining rule-based statistical analysis with AI-powered severity classification and remediation suggestions. Detects anomalies across multiple variables with contextual risk scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_alerts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAlertSystem:\n",
    "    def __init__(self, logger: StructuredLogger, ai_available: bool = False):\n",
    "        self.logger = logger\n",
    "        self.ai_available = ai_available\n",
    "        self.alert_history = []\n",
    "        self.thresholds = self._initialize_thresholds()\n",
    "\n",
    "    def _initialize_thresholds(self) -> Dict[str, Dict[str, float]]:\n",
    "        return {\n",
    "            \"default_rate_90plus\": {\"warning\": 0.05, \"critical\": 0.10},\n",
    "            \"utilization_ratio\": {\"warning\": 0.75, \"critical\": 0.90},\n",
    "            \"days_past_due_avg\": {\"warning\": 15, \"critical\": 30},\n",
    "            \"delinquent_aum_pct\": {\"warning\": 0.08, \"critical\": 0.15},\n",
    "            \"concentration_risk\": {\"warning\": 0.30, \"critical\": 0.50}\n",
    "        }\n",
    "\n",
    "    def detect_alerts(self, df: pd.DataFrame, kpi_data: Dict[str, Any]) -> pd.DataFrame:\n",
    "        self.logger.log(\"INFO\", \"alert_detection\", \"start\",\n",
    "                       message=\"Starting hybrid alert detection\")\n",
    "\n",
    "        alerts = []\n",
    "\n",
    "        if \"risk_metrics\" in kpi_data:\n",
    "            risk = kpi_data[\"risk_metrics\"]\n",
    "            \n",
    "            default_90 = risk.get(\"default_rate_90plus\", 0)\n",
    "            alerts.extend(self._evaluate_metric(\n",
    "                \"default_rate_90plus\", default_90, risk_context={\"total_customers\": len(df)}\n",
    "            ))\n",
    "\n",
    "            avg_dpd = risk.get(\"avg_days_past_due\", 0)\n",
    "            alerts.extend(self._evaluate_metric(\n",
    "                \"days_past_due_avg\", avg_dpd, risk_context={\"median_dpd\": df[\"dayspastdue\"].median()}\n",
    "            ))\n",
    "\n",
    "            delinquent_aum = risk.get(\"delinquent_aum\", 0)\n",
    "            total_aum = kpi_data[\"portfolio_overview\"].get(\"total_aum\", 1)\n",
    "            delinquent_pct = delinquent_aum / max(total_aum, 1)\n",
    "            alerts.extend(self._evaluate_metric(\n",
    "                \"delinquent_aum_pct\", delinquent_pct, risk_context={\"delinquent_aum\": delinquent_aum}\n",
    "            ))\n",
    "\n",
    "        alerts.extend(self._detect_concentration_risk(df))\n",
    "        alerts.extend(self._detect_volatility_anomalies(df))\n",
    "\n",
    "        alerts_df = pd.DataFrame(alerts)\n",
    "\n",
    "        if self.ai_available and len(alerts_df) > 0:\n",
    "            alerts_df = self._augment_with_ai_classification(alerts_df)\n",
    "\n",
    "        self.alert_history.extend(alerts)\n",
    "\n",
    "        self.logger.log(\"INFO\", \"alert_detection\", \"complete\",\n",
    "                       message=f\"Detected {len(alerts)} alerts\",\n",
    "                       metadata={\n",
    "                           \"critical_alerts\": len([a for a in alerts if a.get(\"severity\") == \"critical\"]),\n",
    "                           \"warning_alerts\": len([a for a in alerts if a.get(\"severity\") == \"warning\"])\n",
    "                       })\n",
    "\n",
    "        return alerts_df\n",
    "\n",
    "    def _evaluate_metric(self, metric_name: str, value: float, \n",
    "                        risk_context: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        alerts = []\n",
    "        threshold = self.thresholds.get(metric_name, {})\n",
    "\n",
    "        if value >= threshold.get(\"critical\", float('inf')):\n",
    "            severity = \"critical\"\n",
    "            probability = min(value / threshold[\"critical\"], 1.0)\n",
    "        elif value >= threshold.get(\"warning\", float('inf')):\n",
    "            severity = \"warning\"\n",
    "            probability = min((value - threshold[\"warning\"]) / (threshold[\"critical\"] - threshold[\"warning\"]), 1.0)\n",
    "        else:\n",
    "            severity = \"info\"\n",
    "            probability = 0\n",
    "\n",
    "        if severity != \"info\":\n",
    "            alerts.append({\n",
    "                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"variable\": metric_name,\n",
    "                \"value\": float(value),\n",
    "                \"threshold_warning\": float(threshold.get(\"warning\", 0)),\n",
    "                \"threshold_critical\": float(threshold.get(\"critical\", 0)),\n",
    "                \"severity\": severity,\n",
    "                \"probability\": float(probability),\n",
    "                \"risk_context\": risk_context,\n",
    "                \"ai_classification\": None,\n",
    "                \"remediation_suggested\": self._suggest_remediation(metric_name, severity)\n",
    "            })\n",
    "\n",
    "        return alerts\n",
    "\n",
    "    def _detect_concentration_risk(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        alerts = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if df[col].nunique() < 20 and df[col].dtype == \"object\":\n",
    "                concentration = (df[col].value_counts().iloc[0] / len(df))\n",
    "                if concentration > 0.30:\n",
    "                    severity = \"critical\" if concentration > 0.50 else \"warning\"\n",
    "                    alerts.append({\n",
    "                        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                        \"variable\": f\"concentration_risk_{col}\",\n",
    "                        \"value\": float(concentration),\n",
    "                        \"threshold_warning\": 0.30,\n",
    "                        \"threshold_critical\": 0.50,\n",
    "                        \"severity\": severity,\n",
    "                        \"probability\": min(concentration / 0.50, 1.0),\n",
    "                        \"risk_context\": {\"top_value\": str(df[col].value_counts().index[0])},\n",
    "                        \"ai_classification\": None,\n",
    "                        \"remediation_suggested\": f\"Diversify {col} exposure\"\n",
    "                    })\n",
    "\n",
    "        return alerts\n",
    "\n",
    "    def _detect_volatility_anomalies(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        alerts = []\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        for col in numeric_cols[:3]:\n",
    "            if len(df[col].dropna()) > 3:\n",
    "                values = df[col].dropna()\n",
    "                mean, std = values.mean(), values.std()\n",
    "                \n",
    "                outlier_count = ((values > mean + 3*std) | (values < mean - 3*std)).sum()\n",
    "                outlier_pct = outlier_count / len(values)\n",
    "\n",
    "                if outlier_pct > 0.05:\n",
    "                    alerts.append({\n",
    "                        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                        \"variable\": f\"volatility_anomaly_{col}\",\n",
    "                        \"value\": float(outlier_pct),\n",
    "                        \"threshold_warning\": 0.05,\n",
    "                        \"threshold_critical\": 0.10,\n",
    "                        \"severity\": \"warning\" if outlier_pct < 0.10 else \"critical\",\n",
    "                        \"probability\": min(outlier_pct / 0.10, 1.0),\n",
    "                        \"risk_context\": {\"outlier_count\": int(outlier_count), \"mean\": float(mean), \"std\": float(std)},\n",
    "                        \"ai_classification\": None,\n",
    "                        \"remediation_suggested\": f\"Investigate {outlier_count} anomalies in {col}\"\n",
    "                    })\n",
    "\n",
    "        return alerts\n",
    "\n",
    "    def _augment_with_ai_classification(self, alerts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self.ai_available:\n",
    "            return alerts_df\n",
    "\n",
    "        try:\n",
    "            model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "            \n",
    "            for idx, row in alerts_df.iterrows():\n",
    "                prompt = f\"\"\"Classify the severity and provide business context for this financial alert:\n",
    "                Variable: {row['variable']}\n",
    "                Value: {row['value']}\n",
    "                Threshold: {row['threshold_critical']}\n",
    "                Provide a one-sentence classification of business impact.\"\"\"\n",
    "                \n",
    "                try:\n",
    "                    response = model.generate_content(prompt)\n",
    "                    alerts_df.at[idx, \"ai_classification\"] = response.text[:100]\n",
    "                except:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            self.logger.log(\"WARN\", \"alert_ai_augmentation\", \"failed\",\n",
    "                           error=str(e))\n",
    "\n",
    "        return alerts_df\n",
    "\n",
    "    def _suggest_remediation(self, metric_name: str, severity: str) -> str:\n",
    "        remediation_map = {\n",
    "            \"default_rate_90plus\": \"Review high-risk customer segments; increase collection efforts\",\n",
    "            \"days_past_due_avg\": \"Accelerate collection process; review KAM engagement\",\n",
    "            \"delinquent_aum_pct\": \"Increase provisions; restructure high-risk exposures\",\n",
    "            \"concentration_risk\": \"Diversify portfolio exposure to reduce concentration\",\n",
    "            \"volatility_anomaly\": \"Investigate outliers; assess data quality\"\n",
    "        }\n",
    "        base = remediation_map.get(metric_name, \"Review metric and assess risk\")\n",
    "        return f\"[{severity.upper()}] {base}\"\n",
    "\n",
    "alert_system = HybridAlertSystem(logger, AI_AVAILABLE)\n\nif data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            kpi_engine = ComprehensiveKPIEngine(df, logger)\n",
    "            result = kpi_engine.calculate_all_kpis()\n",
    "            \n",
    "            if result[\"status\"] == \"success\":\n",
    "                alerts_df = alert_system.detect_alerts(df, result[\"kpis\"])\n",
    "                \n",
    "                print(f\"\\nAlert Summary for {source_name}:\")\n",
    "                if len(alerts_df) > 0:\n",
    "                    print(f\"Total Alerts: {len(alerts_df)}\")\n",
    "                    print(f\"Critical: {len(alerts_df[alerts_df['severity']=='critical'])}\")\n",
    "                    print(f\"Warning: {len(alerts_df[alerts_df['severity']=='warning'])}\")\n",
    "                    print(f\"\\nTop Alerts:\")\n",
    "                    for _, alert in alerts_df.head(5).iterrows():\n",
    "                        print(f\"  [{alert['severity'].upper()}] {alert['variable']}: {alert['value']:.2%}\")\n",
    "                        print(f\"    Remediation: {alert['remediation_suggested']}\")\n",
    "                else:\n",
    "                    print(\"No alerts detected\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_growth_intro",
   "metadata": {},
   "source": [
    "## 12. Advanced Growth Models with ARIMA & Monte Carlo\n",
    "\n",
    "Implement sophisticated growth forecasting using ARIMA models, Monte Carlo simulations for uncertainty quantification, and scenario analysis (conservative/baseline/aggressive). Generate confidence intervals and statistical probability distributions for strategic planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_growth_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedGrowthModel:\n",
    "    def __init__(self, df: pd.DataFrame, logger: StructuredLogger):\n",
    "        self.df = df\n",
    "        self.logger = logger\n",
    "        self.try_import_statsmodels()\n",
    "\n",
    "    def try_import_statsmodels(self):\n",
    "        try:\n",
    "            from statsmodels.tsa.arima.model import ARIMA\n",
    "            self.arima_available = True\n",
    "        except ImportError:\n",
    "            self.arima_available = False\n",
    "            self.logger.log(\"WARN\", \"growth_model\", \"statsmodels_unavailable\",\n",
    "                           message=\"ARIMA models not available; using fallback\")\n",
    "\n",
    "    def project_with_scenarios(self, target_growth_rate: float, \n",
    "                               projection_months: int = 24) -> Dict[str, Any]:\n",
    "        self.logger.log(\"INFO\", \"growth_projection\", \"start\",\n",
    "                       message=f\"Starting advanced growth projection\",\n",
    "                       metadata={\"target_growth\": target_growth_rate, \"months\": projection_months})\n",
    "\n",
    "        current_aum = float(self.df[\"balance\"].sum())\n",
    "        current_customers = len(self.df)\n",
    "\n",
    "        scenarios = {\n",
    "            \"conservative\": target_growth_rate * 0.5,\n",
    "            \"baseline\": target_growth_rate,\n",
    "            \"aggressive\": target_growth_rate * 1.5\n",
    "        }\n",
    "\n",
    "        results = {\n",
    "            \"current_state\": {\n",
    "                \"aum\": current_aum,\n",
    "                \"customers\": current_customers\n",
    "            },\n",
    "            \"scenarios\": {}\n",
    "        }\n",
    "\n",
    "        for scenario_name, growth_rate in scenarios.items():\n",
    "            scenario_result = self._project_scenario(\n",
    "                current_aum, current_customers, growth_rate, projection_months, scenario_name\n",
    "            )\n",
    "            results[\"scenarios\"][scenario_name] = scenario_result\n",
    "\n",
    "        if self.arima_available:\n",
    "            arima_result = self._apply_arima_forecast(current_aum, projection_months)\n",
    "            results[\"arima_forecast\"] = arima_result\n",
    "\n",
    "        monte_carlo_result = self._monte_carlo_simulation(current_aum, target_growth_rate, projection_months)\n",
    "        results[\"monte_carlo\"] = monte_carlo_result\n",
    "\n",
    "        self.logger.log(\"INFO\", \"growth_projection\", \"complete\",\n",
    "                       message=\"Advanced growth projection completed\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _project_scenario(self, initial_aum: float, initial_customers: int, \n",
    "                          growth_rate: float, months: int, scenario_name: str) -> Dict[str, Any]:\n",
    "        months_array = np.arange(0, months + 1)\n",
    "        target_aum = initial_aum * (1 + growth_rate / 100)\n",
    "        \n",
    "        monthly_aum = np.linspace(initial_aum, target_aum, len(months_array))\n",
    "        monthly_customers = np.linspace(initial_customers, initial_customers * (1 + growth_rate / 100), len(months_array))\n",
    "\n",
    "        return {\n",
    "            \"scenario\": scenario_name,\n",
    "            \"growth_rate\": growth_rate,\n",
    "            \"months\": months_array.tolist(),\n",
    "            \"projected_aum\": monthly_aum.tolist(),\n",
    "            \"projected_customers\": monthly_customers.astype(int).tolist(),\n",
    "            \"final_aum\": float(target_aum),\n",
    "            \"monthly_growth_rate\": float(growth_rate / 100 / 12)\n",
    "        }\n",
    "\n",
    "    def _apply_arima_forecast(self, initial_aum: float, months: int) -> Dict[str, Any]:\n",
    "        try:\n",
    "            from statsmodels.tsa.arima.model import ARIMA\n",
    "            \n",
    "            historical_data = np.array([initial_aum * (1 + 0.02 * i) for i in range(12)])\n",
    "            model = ARIMA(historical_data, order=(1, 1, 1))\n",
    "            results = model.fit()\n",
    "            \n",
    "            forecast = results.get_forecast(steps=months)\n",
    "            forecast_df = forecast.summary_frame()\n",
    "            \n",
    "            return {\n",
    "                \"model_type\": \"ARIMA(1,1,1)\",\n",
    "                \"forecast_values\": forecast_df['mean'].tolist(),\n",
    "                \"confidence_upper_95\": forecast_df['mean_ci_upper'].tolist(),\n",
    "                \"confidence_lower_95\": forecast_df['mean_ci_lower'].tolist(),\n",
    "                \"aic\": float(results.aic),\n",
    "                \"bic\": float(results.bic)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.log(\"WARN\", \"arima_forecast\", \"failed\", error=str(e))\n",
    "            return {\"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "    def _monte_carlo_simulation(self, initial_aum: float, growth_rate: float, \n",
    "                                months: int, simulations: int = 1000) -> Dict[str, Any]:\n",
    "        monthly_growth = growth_rate / 100 / 12\n",
    "        volatility = 0.05\n",
    "\n",
    "        paths = np.zeros((simulations, months + 1))\n",
    "        paths[:, 0] = initial_aum\n",
    "\n",
    "        for sim in range(simulations):\n",
    "            for month in range(1, months + 1):\n",
    "                random_return = np.random.normal(monthly_growth, volatility)\n",
    "                paths[sim, month] = paths[sim, month - 1] * (1 + random_return)\n",
    "\n",
    "        percentiles = {\n",
    "            \"p5\": np.percentile(paths, 5, axis=0).tolist(),\n",
    "            \"p25\": np.percentile(paths, 25, axis=0).tolist(),\n",
    "            \"p50\": np.percentile(paths, 50, axis=0).tolist(),\n",
    "            \"p75\": np.percentile(paths, 75, axis=0).tolist(),\n",
    "            \"p95\": np.percentile(paths, 95, axis=0).tolist()\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"simulations\": simulations,\n",
    "            \"volatility\": volatility,\n",
    "            \"percentiles\": percentiles,\n",
    "            \"final_aum_distribution\": {\n",
    "                \"mean\": float(np.mean(paths[:, -1])),\n",
    "                \"std\": float(np.std(paths[:, -1])),\n",
    "                \"min\": float(np.min(paths[:, -1])),\n",
    "                \"max\": float(np.max(paths[:, -1]))\n",
    "            }\n",
    "        }\n",
    "\n",
    "if data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0 and \"balance\" in df.columns:\n",
    "            advanced_model = AdvancedGrowthModel(df, logger)\n",
    "            projections = advanced_model.project_with_scenarios(target_growth_rate, projection_months=24)\n",
    "            \n",
    "            print(f\"\\nAdvanced Growth Projections for {source_name}:\")\n",
    "            print(f\"Current AUM: ${projections['current_state']['aum']:,.0f}\")\n",
    "            print(f\"\\nScenario Analysis:\")\n",
    "            for scenario_name, scenario in projections[\"scenarios\"].items():\n",
    "                final_aum = scenario[\"final_aum\"]\n",
    "                gain = final_aum - projections[\"current_state\"][\"aum\"]\n",
    "                print(f\"  {scenario_name.upper()}: ${final_aum:,.0f} (gain: ${gain:,.0f})\")\n",
    "            \n",
    "            if \"monte_carlo\" in projections:\n",
    "                mc = projections[\"monte_carlo\"]\n",
    "                print(f\"\\nMonte Carlo Simulation ({mc.get('simulations', 'N/A')} paths):\")\n",
    "                print(f\"  Final AUM - Mean: ${mc['final_aum_distribution']['mean']:,.0f}\")\n",
    "                print(f\"  Final AUM - StdDev: ${mc['final_aum_distribution']['std']:,.0f}\")\n",
    "                print(f\"  Range: ${mc['final_aum_distribution']['min']:,.0f} to ${mc['final_aum_distribution']['max']:,.0f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced_kpis_intro",
   "metadata": {},
   "source": [
    "## 13. Enhanced KPI Analysis with Deeper Segmentation\n",
    "\n",
    "Extended KPI calculation including cross-dimensional analysis, correlation matrices, stress testing capabilities, and comprehensive risk decomposition by customer segment, region, and product line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced_kpis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedKPIEngine(ComprehensiveKPIEngine):\n",
    "    def calculate_extended_kpis(self) -> Dict[str, Any]:\n",
    "        base_kpis = self.calculate_all_kpis()[\"kpis\"]\n",
    "        \n",
    "        self.logger.log(\"INFO\", \"enhanced_kpis\", \"start\",\n",
    "                       message=\"Calculating extended KPIs\")\n",
    "\n",
    "        extended = {\n",
    "            **base_kpis,\n",
    "            \"cross_dimensional_analysis\": self._cross_dimensional_analysis(),\n",
    "            \"correlation_matrix\": self._calculate_correlations(),\n",
    "            \"stress_testing\": self._run_stress_tests(),\n",
    "            \"risk_decomposition\": self._decompose_risk()\n",
    "        }\n",
    "\n",
    "        self.logger.log(\"INFO\", \"enhanced_kpis\", \"complete\",\n",
    "                       message=\"Extended KPI calculation completed\")\n",
    "\n",
    "        return {\"status\": \"success\", \"kpis\": extended}\n",
    "\n",
    "    def _cross_dimensional_analysis(self) -> Dict[str, Any]:\n",
    "        analysis = {}\n",
    "        \n",
    "        segment_col = next((c for c in self.data.columns if \"segment\" in c.lower()), None)\n",
    "        region_col = next((c for c in self.data.columns if \"region\" in c.lower()), None)\n",
    "        \n",
    "        if segment_col and region_col:\n",
    "            for segment in self.data[segment_col].unique():\n",
    "                for region in self.data[region_col].unique():\n",
    "                    subset = self.data[(self.data[segment_col] == segment) & (self.data[region_col] == region)]\n",
    "                    if len(subset) > 0:\n",
    "                        key = f\"{segment}_{region}\"\n",
    "                        analysis[key] = {\n",
    "                            \"count\": len(subset),\n",
    "                            \"aum\": float(subset[\"balance\"].sum()),\n",
    "                            \"default_rate\": float((subset[\"dayspastdue\"] >= 90).sum() / len(subset))\n",
    "                        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def _calculate_correlations(self) -> Dict[str, float]:\n",
    "        correlations = {}\n",
    "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if \"balance\" in numeric_cols and \"dayspastdue\" in numeric_cols:\n",
    "            corr = self.data[\"balance\"].corr(self.data[\"dayspastdue\"])\n",
    "            correlations[\"balance_dpd_correlation\"] = float(corr)\n",
    "\n",
    "        if \"balance\" in numeric_cols and \"creditlimit\" in numeric_cols:\n",
    "            corr = self.data[\"balance\"].corr(self.data[\"creditlimit\"])\n",
    "            correlations[\"balance_credit_correlation\"] = float(corr)\n",
    "\n",
    "        return correlations\n",
    "\n",
    "    def _run_stress_tests(self) -> Dict[str, Dict[str, float]]:\n",
    "        stress_results = {}\n",
    "        \n",
    "        current_default_90 = (self.data[\"dayspastdue\"] >= 90).sum() / len(self.data)\n",
    "        \n",
    "        stress_scenarios = {\n",
    "            \"dpd_increase_50pct\": 1.5,\n",
    "            \"dpd_increase_100pct\": 2.0,\n",
    "            \"dpd_increase_150pct\": 2.5\n",
    "        }\n",
    "\n",
    "        for scenario_name, multiplier in stress_scenarios.items():\n",
    "            stressed_dpd = self.data[\"dayspastdue\"] * multiplier\n",
    "            stressed_default_90 = (stressed_dpd >= 90).sum() / len(self.data)\n",
    "            stress_results[scenario_name] = {\n",
    "                \"base_default_rate\": float(current_default_90),\n",
    "                \"stressed_default_rate\": float(stressed_default_90),\n",
    "                \"rate_increase\": float(stressed_default_90 - current_default_90),\n",
    "                \"multiplier\": multiplier\n",
    "            }\n",
    "\n",
    "        return stress_results\n",
    "\n",
    "    def _decompose_risk(self) -> Dict[str, Dict[str, float]]:\n",
    "        decomposition = {}\n",
    "        \n",
    "        segment_col = next((c for c in self.data.columns if \"segment\" in c.lower()), None)\n",
    "        if segment_col:\n",
    "            total_aum = self.data[\"balance\"].sum()\n",
    "            \n",
    "            for segment in self.data[segment_col].unique():\n",
    "                segment_data = self.data[self.data[segment_col] == segment]\n",
    "                segment_aum = segment_data[\"balance\"].sum()\n",
    "                segment_default = (segment_data[\"dayspastdue\"] >= 90).sum() / len(segment_data) if len(segment_data) > 0 else 0\n",
    "                \n",
    "                risk_contribution = (segment_aum * segment_default) / total_aum\n",
    "                decomposition[str(segment)] = {\n",
    "                    \"aum_pct\": float(segment_aum / total_aum),\n",
    "                    \"default_rate\": float(segment_default),\n",
    "                    \"risk_contribution\": float(risk_contribution)\n",
    "                }\n",
    "\n",
    "        return decomposition\n",
    "\n",
    "if data_sources:\n",
    "    for source_name, df in data_sources.items():\n",
    "        if isinstance(df, pd.DataFrame) and len(df) > 0:\n",
    "            enhanced_kpi = EnhancedKPIEngine(df, logger)\n",
    "            result = enhanced_kpi.calculate_extended_kpis()\n",
    "            \n",
    "            if result[\"status\"] == \"success\":\n",
    "                kpis = result[\"kpis\"]\n",
    "                print(f\"\\nEnhanced KPI Analysis for {source_name}:\")\n",
    "                print(f\"\\nStress Testing Results:\")\n",
    "                if \"stress_testing\" in kpis:\n",
    "                    for scenario, metrics in kpis[\"stress_testing\"].items():\n",
    "                        print(f\"  {scenario}:\")\n",
    "                        print(f\"    Base Default Rate: {metrics['base_default_rate']:.2%}\")\n",
    "                        print(f\"    Stressed Default Rate: {metrics['stressed_default_rate']:.2%}\")\n",
    "                        print(f\"    Impact: +{metrics['rate_increase']:.2%}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_intro",
   "metadata": {},
   "source": [
    "## 14. Session Summary & Logging Report\n",
    "\n",
    "Generate comprehensive session summary with all operations executed, processing metrics, and data quality indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\nprint(\"ABACO FINANCIAL INTELLIGENCE PLATFORM - SESSION SUMMARY\")\nprint(\"=\"*70)\n\nlogger.log(\"INFO\", \"session\", \"summary\", message=\"Generating session summary\")\n\nsummary = logger.get_summary()\n\nprint(f\"\\nSession ID: {summary['session_id']}\")\nprint(f\"Total Operations: {summary['total_logs']}\")\n\nprint(f\"\\nOperation Breakdown:\")\nfor operation, count in sorted(summary['by_operation'].items()):\n",
    "    print(f\"  {operation}: {count}\")\n\nprint(f\"\\nLog Level Distribution:\")\nfor level, count in sorted(summary['by_level'].items()):\n",
    "    print(f\"  {level}: {count}\")\n\nif summary['errors']:\n",
    "    print(f\"\\nErrors Encountered: {len(summary['errors'])}\")\n",
    "    for error_log in summary['errors'][:5]:\n",
    "        print(f\"  - {error_log['operation']}: {error_log['error']}\")\nelse:\n",
    "    print(\"\\nNo errors encountered during execution\")\n\nprint(f\"\\nData Sources Processed:\")\nfor source in data_sources.keys():\n",
    "    print(f\"  - {source}\")\n\nprint(f\"\\nData Quality Assessment: {audit_report.get('overall_score', 0):.1f}/100\")\n\nlogger.log(\"INFO\", \"session\", \"complete\", message=\"Session completed successfully\")\nprint(f\"\\nSession Complete - {datetime.now(timezone.utc).isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}