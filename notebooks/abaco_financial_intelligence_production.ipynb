{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df0f22a",
   "metadata": {},
   "source": [
    "# Abaco Financial Intelligence Platform\n",
    "## Production AI-Powered Financial Analysis\n",
    "\n",
    "**Comprehensive portfolio analysis, risk assessment, and intelligent insights for financial institutions.**\n",
    "\n",
    "This notebook demonstrates the production implementation of:\n",
    "- **AI Toolkit Best Practices** for agent development and tracing\n",
    "- **Azure Cosmos DB** with hierarchical partition keys for optimal performance\n",
    "- **Supabase** for real-time data and authentication\n",
    "- **Advanced Financial Analytics** with KPI calculation and risk assessment\n",
    "\n",
    "---\n",
    "**Platform:** Abaco Financial Intelligence  \n",
    "**Version:** 2.0.0  \n",
    "**AI Toolkit:** Integrated with comprehensive tracing  \n",
    "**Database:** Azure Cosmos DB + Supabase PostgreSQL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Environment Setup - Abaco Financial Intelligence Platform\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "import uuid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure production paths\n",
    "WORKSPACE_PATH = Path(\"/workspaces/nextjs-with-supabase\")\n",
    "DATA_PATH = WORKSPACE_PATH / \"data\"\n",
    "LOGS_PATH = DATA_PATH / \"logs\"\n",
    "REPORTS_PATH = DATA_PATH / \"reports\"\n",
    "\n",
    "# Ensure production directories exist\n",
    "for path in [DATA_PATH, LOGS_PATH, REPORTS_PATH]:\n",
    "    path.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure production logging with AI Toolkit tracing\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(LOGS_PATH / 'abaco_financial_intelligence.log', mode='a')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"AbacoFinancialIntelligence\")\n",
    "\n",
    "# Production environment validation\n",
    "required_env_vars = [\n",
    "    'NEXT_PUBLIC_SUPABASE_URL',\n",
    "    'COSMOS_DB_ENDPOINT', \n",
    "    'COSMOS_DB_KEY'\n",
    "]\n",
    "env_status = {var: bool(os.getenv(var)) for var in required_env_vars}\n",
    "\n",
    "print(\"ðŸ¦ Abaco Financial Intelligence Platform - Production Environment\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ðŸ“ Workspace: {WORKSPACE_PATH}\")\n",
    "print(f\"ðŸ’¾ Data Directory: {DATA_PATH}\")\n",
    "print(f\"ðŸ“‹ Reports Directory: {REPORTS_PATH}\")\n",
    "print(f\"ðŸ” Environment Variables: {env_status}\")\n",
    "print(f\"ðŸ• Session Start: {datetime.now(timezone.utc).isoformat()}\")\n",
    "print(f\"ðŸ¤– AI Toolkit: Tracing Enabled\")\n",
    "\n",
    "logger.info(\"Abaco Financial Intelligence Platform session initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Financial Data Generator - Abaco Standards\n",
    "class AbacoFinancialDataGenerator:\n",
    "    \"\"\"Production-grade financial data generator following Abaco standards\"\"\"\n",
    "    \n",
    "    def __init__(self, tenant_id: str = \"abaco_financial\"):\n",
    "        self.tenant_id = tenant_id\n",
    "        self.operation_id = str(uuid.uuid4())\n",
    "        self.rng = np.random.default_rng()  # Initialize numpy random generator\n",
    "        \n",
    "    def generate_production_portfolio(self, \n",
    "                                    enterprise_count: int = 25,\n",
    "                                    corporate_count: int = 75, \n",
    "                                    sme_count: int = 200,\n",
    "                                    retail_count: int = 500) -> pd.DataFrame:\n",
    "        \"\"\"Generate realistic financial portfolio following Abaco business model\"\"\"\n",
    "        \n",
    "        total_customers = enterprise_count + corporate_count + sme_count + retail_count\n",
    "        logger.info(f\"Generating Abaco production portfolio: {total_customers} customers\")\n",
    "        \n",
    "        # Abaco industry focus areas\n",
    "        industries = {\n",
    "            'ENTERPRISE': ['TECHNOLOGY', 'HEALTHCARE', 'ENERGY', 'MANUFACTURING'],\n",
    "            'CORPORATE': ['FINANCE', 'RETAIL', 'CONSTRUCTION', 'TRANSPORTATION'],\n",
    "            'SME': ['PROFESSIONAL_SERVICES', 'HOSPITALITY', 'AGRICULTURE', 'REAL_ESTATE'],\n",
    "            'RETAIL': ['CONSUMER_SERVICES', 'EDUCATION', 'PERSONAL_CARE', 'LOCAL_BUSINESS']\n",
    "        }\n",
    "        \n",
    "        # Abaco regional presence\n",
    "        regions = ['NORTH_AMERICA', 'EUROPE', 'ASIA_PACIFIC', 'LATIN_AMERICA', 'MIDDLE_EAST']\n",
    "        \n",
    "        # Abaco product codes\n",
    "        products = {\n",
    "            'ENTERPRISE': ['CORP_CREDIT', 'TRADE_FINANCE', 'STRUCTURED_FINANCE'],\n",
    "            'CORPORATE': ['BUSINESS_LOAN', 'REVOLVING_CREDIT', 'EQUIPMENT_FINANCE'],\n",
    "            'SME': ['SME_LOAN', 'WORKING_CAPITAL', 'MERCHANT_FINANCE'],\n",
    "            'RETAIL': ['PERSONAL_LOAN', 'CREDIT_CARD', 'CONSUMER_FINANCE']\n",
    "        }\n",
    "        \n",
    "        portfolio_data = []\n",
    "        customer_id_counter = 100000\n",
    "        \n",
    "        # Generate segments with realistic financial profiles\n",
    "        segments_config = {\n",
    "            'ENTERPRISE': {\n",
    "                'count': enterprise_count,\n",
    "                'balance_range': (2000000, 50000000),\n",
    "                'credit_multiplier': (1.5, 3.0),\n",
    "                'apr_range': (0.04, 0.08),\n",
    "                'risk_distribution': {'A': 0.6, 'B': 0.3, 'C': 0.08, 'D': 0.02}\n",
    "            },\n",
    "            'CORPORATE': {\n",
    "                'count': corporate_count,\n",
    "                'balance_range': (500000, 5000000),\n",
    "                'credit_multiplier': (1.3, 2.5),\n",
    "                'apr_range': (0.06, 0.12),\n",
    "                'risk_distribution': {'A': 0.4, 'B': 0.4, 'C': 0.15, 'D': 0.05}\n",
    "            },\n",
    "            'SME': {\n",
    "                'count': sme_count,\n",
    "                'balance_range': (50000, 1000000),\n",
    "                'credit_multiplier': (1.2, 2.0),\n",
    "                'apr_range': (0.08, 0.16),\n",
    "                'risk_distribution': {'A': 0.25, 'B': 0.45, 'C': 0.25, 'D': 0.05}\n",
    "            },\n",
    "            'RETAIL': {\n",
    "                'count': retail_count,\n",
    "                'balance_range': (5000, 100000),\n",
    "                'credit_multiplier': (1.1, 1.8),\n",
    "                'apr_range': (0.12, 0.24),\n",
    "                'risk_distribution': {'A': 0.2, 'B': 0.5, 'C': 0.25, 'D': 0.05}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for segment, config in segments_config.items():\n",
    "            for i in range(config['count']):\n",
    "                customer_id_counter += 1\n",
    "                # Generate realistic balance\n",
    "                balance = self.rng.uniform(*config['balance_range'])\n",
    "                \n",
    "                # Generate credit limit\n",
    "                credit_multiplier = self.rng.uniform(*config['credit_multiplier'])\n",
    "                credit_limit = balance * credit_multiplier\n",
    "                \n",
    "                # Generate risk grade based on distribution\n",
    "                risk_grades = list(config['risk_distribution'].keys())\n",
    "                risk_probabilities = list(config['risk_distribution'].values())\n",
    "                risk_grade = self.rng.choice(risk_grades, p=risk_probabilities)\n",
    "                \n",
    "                # Generate APR based on risk grade and segment\n",
    "                base_apr = self.rng.uniform(*config['apr_range'])\n",
    "                risk_adjustment = {'A': 0.9, 'B': 1.0, 'C': 1.2, 'D': 1.5}[risk_grade]\n",
    "                apr = base_apr * risk_adjustment\n",
    "                \n",
    "                # Generate days past due based on risk grade\n",
    "                dpd_params = {'A': 2, 'B': 8, 'C': 20, 'D': 45}\n",
    "                days_past_due = max(0, int(self.rng.exponential(dpd_params[risk_grade])))\n",
    "                days_past_due = min(days_past_due, 365)  # Cap at 1 year\n",
    "                \n",
    "                # Generate origination date (realistic aging)\n",
    "                days_since_origination = self.rng.integers(30, 1095)  # 1 month to 3 years\n",
    "                origination_date = datetime.now() - timedelta(days=days_since_origination)\n",
    "                \n",
    "                customer_data = {\n",
    "                    'customerId': f'ABACO{customer_id_counter:07d}',\n",
    "                    'customerSegment': segment,\n",
    "                    'balance': round(balance, 2),\n",
    "                    'creditLimit': round(credit_limit, 2),\n",
    "                    'daysPassDue': days_past_due,\n",
    "                    'industry': self.rng.choice(industries[segment]),\n",
    "                    'region': self.rng.choice(regions),\n",
    "                    'kamOwner': f'KAM{(customer_id_counter % 25) + 1:03d}',\n",
    "                    'productCode': self.rng.choice(products[segment]),\n",
    "                    'riskGrade': risk_grade,\n",
    "                    'apr': round(apr, 4),\n",
    "                    'originationDate': origination_date.strftime('%Y-%m-%d'),\n",
    "                    'analysisDate': datetime.now().strftime('%Y-%m-%d')\n",
    "                }\n",
    "                \n",
    "                portfolio_data.append(customer_data)\n",
    "        \n",
    "        df = pd.DataFrame(portfolio_data)\n",
    "        \n",
    "        logger.info(f\"Generated Abaco portfolio: {len(df)} customers across {df['customerSegment'].nunique()} segments\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize the generator and create sample portfolio\n",
    "generator = AbacoFinancialDataGenerator()\n",
    "portfolio_df = generator.generate_production_portfolio()\n",
    "\n",
    "print(f\"âœ… Generated portfolio with {len(portfolio_df)} customers\")\n",
    "print(f\"ðŸ“Š Segment distribution:\")\n",
    "print(portfolio_df['customerSegment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production KPI Engine with AI Toolkit Integration\n",
    "class AbacoKPIEngine:\n",
    "    \"\"\"Production KPI calculation engine with comprehensive tracing\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, tenant_id: str = \"abaco_financial\"):\n",
    "        self.data = data\n",
    "        self.tenant_id = tenant_id\n",
    "        self.trace_id = str(uuid.uuid4())\n",
    "        self.operation_start = datetime.now(timezone.utc)\n",
    "        \n",
    "    def calculate_comprehensive_kpis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate all KPIs with AI Toolkit tracing\"\"\"\n",
    "        \n",
    "        logger.info(f\"Starting comprehensive KPI calculation for {len(self.data)} records\")\n",
    "        \n",
    "        if self.data.empty:\n",
    "            return {'status': 'no_data', 'kpis': {}}\n",
    "        \n",
    "        # Calculate utilization ratio\n",
    "        self.data['utilizationRatio'] = (\n",
    "            self.data['balance'] / self.data['creditLimit']\n",
    "        ).fillna(0).clip(0, 1)\n",
    "        \n",
    "        # Delinquency buckets\n",
    "        def get_dpd_bucket(days):\n",
    "            if days == 0:\n",
    "                return 'Current'\n",
    "            elif days <= 30:\n",
    "                return '1-30 DPD'\n",
    "            elif days <= 60:\n",
    "                return '31-60 DPD'\n",
    "            elif days <= 90:\n",
    "                return '61-90 DPD'\n",
    "            else:\n",
    "                return '90+ DPD'\n",
    "        \n",
    "        self.data['dpdBucket'] = self.data['daysPassDue'].apply(get_dpd_bucket)\n",
    "        \n",
    "        # Core KPIs\n",
    "        kpis = {\n",
    "            'portfolio_overview': {\n",
    "                'total_aum': float(self.data['balance'].sum()),\n",
    "                'active_customers': int(len(self.data)),\n",
    "                'total_credit_lines': float(self.data['creditLimit'].sum()),\n",
    "                'avg_balance': float(self.data['balance'].mean()),\n",
    "                'avg_credit_limit': float(self.data['creditLimit'].mean()),\n",
    "                'portfolio_utilization': float(self.data['utilizationRatio'].mean())\n",
    "            },\n",
    "            'risk_metrics': {\n",
    "                'default_rate_30plus': float((self.data['daysPassDue'] >= 30).sum() / len(self.data)),\n",
    "                'default_rate_90plus': float((self.data['daysPassDue'] >= 90).sum() / len(self.data)),\n",
    "                'avg_days_past_due': float(self.data['daysPassDue'].mean()),\n",
    "                'high_utilization_customers': int((self.data['utilizationRatio'] > 0.8).sum()),\n",
    "                'risk_grade_distribution': self.data['riskGrade'].value_counts().to_dict()\n",
    "            },\n",
    "            'segment_analysis': {},\n",
    "            'delinquency_buckets': self.data['dpdBucket'].value_counts().to_dict(),\n",
    "            'regional_breakdown': self.data['region'].value_counts().to_dict(),\n",
    "            'industry_breakdown': self.data['industry'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        # Segment-level analysis\n",
    "        for segment in self.data['customerSegment'].unique():\n",
    "            segment_data = self.data[self.data['customerSegment'] == segment]\n",
    "            kpis['segment_analysis'][segment] = {\n",
    "                'customer_count': int(len(segment_data)),\n",
    "                'total_aum': float(segment_data['balance'].sum()),\n",
    "                'avg_balance': float(segment_data['balance'].mean()),\n",
    "                'avg_utilization': float(segment_data['utilizationRatio'].mean()),\n",
    "                'default_rate_30plus': float((segment_data['daysPassDue'] >= 30).sum() / len(segment_data)),\n",
    "                'weighted_apr': float((segment_data['apr'] * segment_data['balance']).sum() / segment_data['balance'].sum())\n",
    "            }\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (datetime.now(timezone.utc) - self.operation_start).total_seconds()\n",
    "        \n",
    "        result = {\n",
    "            'status': 'success',\n",
    "            'kpis': kpis,\n",
    "            'metadata': {\n",
    "                'tenant_id': self.tenant_id,\n",
    "                'trace_id': self.trace_id,\n",
    "                'processing_time_seconds': processing_time,\n",
    "                'records_processed': len(self.data),\n",
    "                'generated_at': datetime.now(timezone.utc).isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"KPI calculation completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Calculate KPIs for the generated portfolio\n",
    "kpi_engine = AbacoKPIEngine(portfolio_df)\n",
    "kpi_results = kpi_engine.calculate_comprehensive_kpis()\n",
    "\n",
    "print(\"ðŸŽ¯ Portfolio KPI Analysis Results:\")\n",
    "print(f\"ðŸ“Š Total AUM: ${kpi_results['kpis']['portfolio_overview']['total_aum']:,.2f}\")\n",
    "print(f\"ðŸ‘¥ Active Customers: {kpi_results['kpis']['portfolio_overview']['active_customers']:,}\")\n",
    "print(f\"âš ï¸ 30+ DPD Rate: {kpi_results['kpis']['risk_metrics']['default_rate_30plus']:.2%}\")\n",
    "print(f\"ðŸ”´ 90+ DPD Rate: {kpi_results['kpis']['risk_metrics']['default_rate_90plus']:.2%}\")\n",
    "print(f\"â±ï¸ Processing Time: {kpi_results['metadata']['processing_time_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Powered Risk Assessment and Alert Generation\n",
    "class AbacoRiskEngine:\n",
    "    \"\"\"AI-powered risk assessment with comprehensive alert generation\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, kpis: Dict[str, Any]):\n",
    "        self.data = data\n",
    "        self.kpis = kpis\n",
    "        self.alerts = []\n",
    "        self.insights = []\n",
    "        \n",
    "    def generate_risk_alerts(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate risk alerts based on portfolio analysis\"\"\"\n",
    "        \n",
    "        # High utilization alerts\n",
    "        high_util_customers = self.data[self.data['utilizationRatio'] > 0.9]\n",
    "        if len(high_util_customers) > 0:\n",
    "            self.alerts.append({\n",
    "                'rule': 'high_utilization_alert',\n",
    "                'severity': 'high',\n",
    "                'count': len(high_util_customers),\n",
    "                'description': f'{len(high_util_customers)} customers with >90% credit utilization',\n",
    "                'recommended_action': 'Review credit limits and risk profiles'\n",
    "            })\n",
    "        \n",
    "        # Delinquency concentration alerts\n",
    "        dpd_90plus_rate = self.kpis['kpis']['risk_metrics']['default_rate_90plus']\n",
    "        if dpd_90plus_rate > 0.05:  # 5% threshold\n",
    "            self.alerts.append({\n",
    "                'rule': 'high_delinquency_rate',\n",
    "                'severity': 'critical',\n",
    "                'value': dpd_90plus_rate,\n",
    "                'description': f'Portfolio 90+ DPD rate of {dpd_90plus_rate:.2%} exceeds 5% threshold',\n",
    "                'recommended_action': 'Immediate portfolio review and collection strategy'\n",
    "            })\n",
    "        \n",
    "        # Segment concentration risk\n",
    "        for segment, metrics in self.kpis['kpis']['segment_analysis'].items():\n",
    "            aum_concentration = metrics['total_aum'] / self.kpis['kpis']['portfolio_overview']['total_aum']\n",
    "            if aum_concentration > 0.4:  # 40% concentration threshold\n",
    "                self.alerts.append({\n",
    "                    'rule': 'segment_concentration_risk',\n",
    "                    'severity': 'medium',\n",
    "                    'segment': segment,\n",
    "                    'concentration': aum_concentration,\n",
    "                    'description': f'{segment} segment represents {aum_concentration:.1%} of total AUM',\n",
    "                    'recommended_action': 'Consider diversification strategies'\n",
    "                })\n",
    "        \n",
    "        # Risk grade distribution alerts\n",
    "        risk_dist = self.kpis['kpis']['risk_metrics']['risk_grade_distribution']\n",
    "        high_risk_pct = (risk_dist.get('C', 0) + risk_dist.get('D', 0)) / sum(risk_dist.values())\n",
    "        if high_risk_pct > 0.3:  # 30% threshold\n",
    "            self.alerts.append({\n",
    "                'rule': 'high_risk_concentration',\n",
    "                'severity': 'high',\n",
    "                'value': high_risk_pct,\n",
    "                'description': f'High-risk customers (C&D grades) represent {high_risk_pct:.1%} of portfolio',\n",
    "                'recommended_action': 'Review underwriting standards and risk pricing'\n",
    "            })\n",
    "        \n",
    "        return self.alerts\n",
    "    \n",
    "    def generate_ai_insights(self) -> List[str]:\n",
    "        \"\"\"Generate AI-powered portfolio insights\"\"\"\n",
    "        \n",
    "        portfolio_kpis = self.kpis['kpis']['portfolio_overview']\n",
    "        risk_kpis = self.kpis['kpis']['risk_metrics']\n",
    "        \n",
    "        # Portfolio health insight\n",
    "        avg_utilization = portfolio_kpis['portfolio_utilization']\n",
    "        if avg_utilization < 0.3:\n",
    "            self.insights.append(\"Portfolio shows conservative utilization patterns, indicating potential for growth strategies.\")\n",
    "        elif avg_utilization > 0.7:\n",
    "            self.insights.append(\"High portfolio utilization suggests strong customer engagement but requires careful risk monitoring.\")\n",
    "        \n",
    "        # Risk trend insight\n",
    "        dpd_30plus = risk_kpis['default_rate_30plus']\n",
    "        if dpd_30plus < 0.02:\n",
    "            self.insights.append(\"Excellent portfolio quality with very low delinquency rates.\")\n",
    "        elif dpd_30plus > 0.1:\n",
    "            self.insights.append(\"Elevated delinquency rates require immediate attention and enhanced collection efforts.\")\n",
    "        \n",
    "        # Segment performance insight\n",
    "        best_segment = min(self.kpis['kpis']['segment_analysis'].items(), \n",
    "                          key=lambda x: x[1]['default_rate_30plus'])\n",
    "        worst_segment = max(self.kpis['kpis']['segment_analysis'].items(), \n",
    "                           key=lambda x: x[1]['default_rate_30plus'])\n",
    "        \n",
    "        self.insights.append(f\"{best_segment[0]} segment shows the strongest performance with {best_segment[1]['default_rate_30plus']:.2%} delinquency rate.\")\n",
    "        self.insights.append(f\"{worst_segment[0]} segment requires attention with {worst_segment[1]['default_rate_30plus']:.2%} delinquency rate.\")\n",
    "        \n",
    "        # APR optimization insight\n",
    "        segment_aprs = [(seg, data['weighted_apr']) for seg, data in self.kpis['kpis']['segment_analysis'].items()]\n",
    "        segment_aprs.sort(key=lambda x: x[1])\n",
    "        \n",
    "        if len(segment_aprs) > 1:\n",
    "            lowest_apr_segment = segment_aprs[0][0]\n",
    "            highest_apr_segment = segment_aprs[-1][0]\n",
    "            self.insights.append(f\"APR optimization opportunity: {lowest_apr_segment} segment has the lowest rates while {highest_apr_segment} has the highest, suggesting pricing review opportunities.\")\n",
    "        \n",
    "        return self.insights\n",
    "\n",
    "# Generate risk assessment and insights\n",
    "risk_engine = AbacoRiskEngine(portfolio_df, kpi_results)\n",
    "alerts = risk_engine.generate_risk_alerts()\n",
    "insights = risk_engine.generate_ai_insights()\n",
    "\n",
    "print(\"ðŸš¨ Risk Alerts Generated:\")\n",
    "for i, alert in enumerate(alerts, 1):\n",
    "    print(f\"{i}. [{alert['severity'].upper()}] {alert['description']}\")\n",
    "    print(f\"   Action: {alert['recommended_action']}\")\n",
    "\n",
    "print(\"\\nðŸ§  AI-Powered Insights:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results with Azure Cosmos DB Integration\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Prepare data for export with Azure Cosmos DB HPK structure\n",
    "def prepare_cosmos_export(kpis: Dict[str, Any], alerts: List[Dict], insights: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Prepare data for Azure Cosmos DB with Hierarchical Partition Keys\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now()\n",
    "    analysis_date = timestamp.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Create hierarchical partition key structure\n",
    "    cosmos_document = {\n",
    "        'id': f\"portfolio_analysis_{int(timestamp.timestamp())}\",\n",
    "        'partitionKey': f\"abaco_financial/PORTFOLIO/{analysis_date}\",  # HPK format\n",
    "        'tenantId': 'abaco_financial',\n",
    "        'customerSegment': 'PORTFOLIO',\n",
    "        'analysisDate': analysis_date,\n",
    "        'documentType': 'portfolio_analysis',\n",
    "        'createdAt': timestamp.isoformat(),\n",
    "        'updatedAt': timestamp.isoformat(),\n",
    "        'ttl': 365 * 24 * 60 * 60,  # 1 year TTL\n",
    "        \n",
    "        # Portfolio analysis data\n",
    "        'portfolioAnalysis': {\n",
    "            'kpis': kpis['kpis'],\n",
    "            'alerts': alerts,\n",
    "            'insights': insights,\n",
    "            'metadata': kpis['metadata']\n",
    "        },\n",
    "        \n",
    "        # Additional metadata for AI Toolkit tracing\n",
    "        'aiToolkitTrace': {\n",
    "            'traceId': kpis['metadata']['trace_id'],\n",
    "            'processingTime': kpis['metadata']['processing_time_seconds'],\n",
    "            'recordsProcessed': kpis['metadata']['records_processed'],\n",
    "            'agentVersion': '2.0.0',\n",
    "            'platform': 'abaco_financial_intelligence'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return cosmos_document\n",
    "\n",
    "# Prepare and save results\n",
    "cosmos_export = prepare_cosmos_export(kpi_results, alerts, insights)\n",
    "\n",
    "# Save to files for review\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save detailed results\n",
    "results_file = REPORTS_PATH / f'abaco_portfolio_analysis_{timestamp_str}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(cosmos_export, f, indent=2, default=str)\n",
    "\n",
    "# Save summary CSV for quick analysis\n",
    "summary_df = pd.DataFrame([{\n",
    "    'analysis_date': cosmos_export['analysisDate'],\n",
    "    'total_aum': cosmos_export['portfolioAnalysis']['kpis']['portfolio_overview']['total_aum'],\n",
    "    'active_customers': cosmos_export['portfolioAnalysis']['kpis']['portfolio_overview']['active_customers'],\n",
    "    'default_rate_30plus': cosmos_export['portfolioAnalysis']['kpis']['risk_metrics']['default_rate_30plus'],\n",
    "    'default_rate_90plus': cosmos_export['portfolioAnalysis']['kpis']['risk_metrics']['default_rate_90plus'],\n",
    "    'portfolio_utilization': cosmos_export['portfolioAnalysis']['kpis']['portfolio_overview']['portfolio_utilization'],\n",
    "    'alerts_count': len(alerts),\n",
    "    'insights_count': len(insights),\n",
    "    'processing_time_seconds': cosmos_export['portfolioAnalysis']['metadata']['processing_time_seconds']\n",
    "}])\n",
    "\n",
    "summary_file = REPORTS_PATH / f'abaco_portfolio_summary_{timestamp_str}.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "print(\"ðŸ“„ Analysis Results Exported:\")\n",
    "print(f\"ðŸ“‹ Detailed Report: {results_file}\")\n",
    "print(f\"ðŸ“Š Summary Report: {summary_file}\")\n",
    "print(f\"ðŸ—‚ï¸ Cosmos DB Document Ready for HPK: {cosmos_export['partitionKey']}\")\n",
    "\n",
    "# Display key metrics summary\n",
    "print(\"\\nðŸ“ˆ Executive Summary:\")\n",
    "print(f\"ðŸ’° Total AUM: ${cosmos_export['portfolioAnalysis']['kpis']['portfolio_overview']['total_aum']:,.2f}\")\n",
    "print(f\"ðŸ‘¥ Active Customers: {cosmos_export['portfolioAnalysis']['kpis']['portfolio_overview']['active_customers']:,}\")\n",
    "print(f\"âš ï¸ Risk Alerts: {len(alerts)}\")\n",
    "print(f\"ðŸ’¡ AI Insights: {len(insights)}\")\n",
    "print(f\"â±ï¸ Processing Time: {cosmos_export['portfolioAnalysis']['metadata']['processing_time_seconds']:.2f}s\")\n",
    "print(f\"ðŸŽ¯ Analysis Status: Production Ready for ABACO Financial Intelligence Platform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabase Integration with AI Toolkit Tracing\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "\n",
    "class AbacoSupabaseIntegration:\n",
    "    \"\"\"Integration with Supabase following AI Toolkit best practices\"\"\"\n",
    "    \n",
    "    def __init__(self, tenant_id: str = \"abaco_financial\"):\n",
    "        self.tenant_id = tenant_id\n",
    "        self.session_id = f\"session_{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "    def create_financial_data_batch(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Prepare financial data for Supabase insertion with AI Toolkit tracing\"\"\"\n",
    "        \n",
    "        trace_start = datetime.now(timezone.utc)\n",
    "        logger.info(f\"Preparing {len(df)} records for Supabase insertion\")\n",
    "        \n",
    "        batch_records = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Map pandas DataFrame columns to Supabase schema\n",
    "            record = {\n",
    "                'customer_id': row['customerId'],\n",
    "                'tenant_id': self.tenant_id,\n",
    "                'balance': float(row['balance']),\n",
    "                'credit_limit': float(row['creditLimit']),\n",
    "                'days_past_due': int(row['daysPassDue']),\n",
    "                'customer_segment': row['customerSegment'],\n",
    "                'industry': row['industry'],\n",
    "                'region': row['region'],\n",
    "                'kam_owner': row['kamOwner'],\n",
    "                'product_code': row['productCode'],\n",
    "                'risk_grade': row['riskGrade'],\n",
    "                'apr': float(row['apr']),\n",
    "                'origination_date': row['originationDate'],\n",
    "                'analysis_date': row['analysisDate']\n",
    "            }\n",
    "            batch_records.append(record)\n",
    "        \n",
    "        processing_time = (datetime.now(timezone.utc) - trace_start).total_seconds()\n",
    "        \n",
    "        # AI Toolkit tracing for batch preparation\n",
    "        trace_data = {\n",
    "            'operation': 'supabase_batch_preparation',\n",
    "            'tenant_id': self.tenant_id,\n",
    "            'records_prepared': len(batch_records),\n",
    "            'processing_time_seconds': processing_time,\n",
    "            'trace_id': f\"supabase_prep_{uuid.uuid4().hex[:8]}\",\n",
    "            'timestamp': datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Supabase batch prepared: {trace_data}\")\n",
    "        \n",
    "        return batch_records\n",
    "    \n",
    "    def create_portfolio_analysis_session(self, analyst_id: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Create portfolio analysis session following Supabase schema\"\"\"\n",
    "        \n",
    "        session_data = {\n",
    "            'session_id': self.session_id,\n",
    "            'analyst_id': analyst_id,\n",
    "            'tenant_id': self.tenant_id,\n",
    "            'customer_segment': 'PORTFOLIO',\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'status': 'processing',\n",
    "            'trace_data': {\n",
    "                'trace_id': self.session_id,\n",
    "                'start_time': datetime.now(timezone.utc).timestamp(),\n",
    "                'agent_version': '2.0.0',\n",
    "                'toolkit_version': 'ai-toolkit-v1',\n",
    "                'session_metadata': {\n",
    "                    'tenant_id': self.tenant_id,\n",
    "                    'customer_segment': 'PORTFOLIO',\n",
    "                    'platform': 'abaco_financial_intelligence'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return session_data\n",
    "    \n",
    "    def create_agent_execution_log(self, operation: str, status: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Create agent execution log following AI Toolkit tracing best practices\"\"\"\n",
    "        \n",
    "        log_entry = {\n",
    "            'session_id': self.session_id,\n",
    "            'agent_type': 'financial_intelligence',\n",
    "            'operation': operation,\n",
    "            'status': status,\n",
    "            'input_data': kwargs.get('input_data', {}),\n",
    "            'output_data': kwargs.get('output_data', {}),\n",
    "            'error_message': kwargs.get('error_message'),\n",
    "            'duration_ms': kwargs.get('duration_ms', 0),\n",
    "            'tokens_used': kwargs.get('tokens_used', 0),\n",
    "            'cost_usd': kwargs.get('cost_usd', 0.0),\n",
    "            'trace_id': self.session_id,\n",
    "            'span_id': f\"span_{uuid.uuid4().hex[:8]}\"\n",
    "        }\n",
    "        \n",
    "        return log_entry\n",
    "\n",
    "# Initialize Supabase integration\n",
    "supabase_integration = AbacoSupabaseIntegration()\n",
    "\n",
    "# Create portfolio analysis session\n",
    "session_data = supabase_integration.create_portfolio_analysis_session()\n",
    "print(\"ðŸ“Š Portfolio Analysis Session Created:\")\n",
    "print(f\"   Session ID: {session_data['session_id']}\")\n",
    "print(f\"   Tenant ID: {session_data['tenant_id']}\")\n",
    "print(f\"   Status: {session_data['status']}\")\n",
    "\n",
    "# Prepare financial data for Supabase\n",
    "financial_records = supabase_integration.create_financial_data_batch(portfolio_df)\n",
    "print(f\"\\nðŸ’¾ Prepared {len(financial_records)} financial records for Supabase\")\n",
    "\n",
    "# Create agent execution logs for the analysis\n",
    "agent_logs = []\n",
    "\n",
    "# Log data generation\n",
    "data_gen_log = supabase_integration.create_agent_execution_log(\n",
    "    operation='portfolio_data_generation',\n",
    "    status='completed',\n",
    "    input_data={'record_count': len(portfolio_df)},\n",
    "    output_data={'customers_generated': len(portfolio_df)},\n",
    "    duration_ms=1000,  # Placeholder\n",
    "    tokens_used=0\n",
    ")\n",
    "agent_logs.append(data_gen_log)\n",
    "\n",
    "# Log KPI calculation\n",
    "kpi_calc_log = supabase_integration.create_agent_execution_log(\n",
    "    operation='kpi_calculation',\n",
    "    status='completed',\n",
    "    input_data={'portfolio_size': len(portfolio_df)},\n",
    "    output_data=kpi_results['kpis']['portfolio_overview'],\n",
    "    duration_ms=int(kpi_results['metadata']['processing_time_seconds'] * 1000),\n",
    "    tokens_used=0\n",
    ")\n",
    "agent_logs.append(kpi_calc_log)\n",
    "\n",
    "# Log risk assessment\n",
    "risk_assessment_log = supabase_integration.create_agent_execution_log(\n",
    "    operation='risk_assessment',\n",
    "    status='completed',\n",
    "    input_data={'kpi_data': 'portfolio_analysis'},\n",
    "    output_data={'alerts_generated': len(alerts), 'insights_generated': len(insights)},\n",
    "    duration_ms=500,  # Placeholder\n",
    "    tokens_used=0\n",
    ")\n",
    "agent_logs.append(risk_assessment_log)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Created {len(agent_logs)} agent execution logs\")\n",
    "\n",
    "# Export SQL insertion scripts for Supabase\n",
    "sql_statements = []\n",
    "\n",
    "# Financial data insertions (batch of 5 samples for demonstration)\n",
    "sample_records = financial_records[:5]\n",
    "for record in sample_records:\n",
    "    sql = f\"\"\"\n",
    "INSERT INTO financial_data (\n",
    "    customer_id, tenant_id, balance, credit_limit, days_past_due, \n",
    "    customer_segment, industry, region, kam_owner, product_code, \n",
    "    risk_grade, apr, origination_date, analysis_date\n",
    ") VALUES (\n",
    "    '{record['customer_id']}', '{record['tenant_id']}', {record['balance']}, \n",
    "    {record['credit_limit']}, {record['days_past_due']}, '{record['customer_segment']}', \n",
    "    '{record['industry']}', '{record['region']}', '{record['kam_owner']}', \n",
    "    '{record['product_code']}', '{record['risk_grade']}', {record['apr']}, \n",
    "    '{record['origination_date']}', '{record['analysis_date']}'\n",
    ");\"\"\"\n",
    "    sql_statements.append(sql)\n",
    "\n",
    "# Portfolio analysis session insertion\n",
    "session_sql = f\"\"\"\n",
    "INSERT INTO portfolio_analysis (\n",
    "    session_id, tenant_id, customer_segment, analysis_date, status, trace_data\n",
    ") VALUES (\n",
    "    '{session_data['session_id']}', '{session_data['tenant_id']}', \n",
    "    '{session_data['customer_segment']}', '{session_data['analysis_date']}', \n",
    "    '{session_data['status']}', '{json.dumps(session_data['trace_data'])}'::jsonb\n",
    ");\"\"\"\n",
    "sql_statements.append(session_sql)\n",
    "\n",
    "# Save SQL statements for Supabase execution\n",
    "sql_file = REPORTS_PATH / f'supabase_insertion_statements_{timestamp_str}.sql'\n",
    "with open(sql_file, 'w') as f:\n",
    "    f.write(\"-- ABACO Financial Intelligence Platform - Supabase Data Insertion\\n\")\n",
    "    f.write(\"-- Generated with AI Toolkit tracing integration\\n\")\n",
    "    f.write(\"-- Execute these statements in your Supabase SQL Editor\\n\\n\")\n",
    "    for sql in sql_statements:\n",
    "        f.write(sql + \"\\n\")\n",
    "\n",
    "print(f\"\\nðŸ“„ SQL Statements Generated: {sql_file}\")\n",
    "print(\"ðŸ”— Ready for Supabase integration with AI Toolkit tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd1235",
   "metadata": {},
   "source": [
    "## Azure Cosmos DB Integration with Hierarchical Partition Keys\n",
    "\n",
    "This section demonstrates how to integrate the financial intelligence analysis with Azure Cosmos DB using Hierarchical Partition Keys (HPK) for optimal performance and scalability.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Overcome 20GB limit** of single logical partitions\n",
    "- **Improve query flexibility** with targeted multi-partition queries  \n",
    "- **Enhanced performance** for financial data access patterns\n",
    "- **AI Toolkit tracing** for comprehensive observability\n",
    "\n",
    "### HPK Structure:\n",
    "`{tenantId}/{customerSegment}/{analysisDate}`\n",
    "\n",
    "Examples:\n",
    "- `abaco_financial/ENTERPRISE/2024-01-15`\n",
    "- `abaco_financial/PORTFOLIO/2024-01-15`\n",
    "- `abaco_financial/SME/2024-01-15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24335466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cosmos DB Integration with AI Toolkit Best Practices\n",
    "class AbacoCosmosDBClient:\n",
    "    \"\"\"Azure Cosmos DB client optimized for financial intelligence with HPK\"\"\"\n",
    "    \n",
    "    def __init__(self, tenant_id: str = \"abaco_financial\"):\n",
    "        self.tenant_id = tenant_id\n",
    "        self.operation_metrics = {\n",
    "            'total_operations': 0,\n",
    "            'successful_operations': 0,\n",
    "            'failed_operations': 0,\n",
    "            'total_ru_consumed': 0.0,\n",
    "            'average_latency': 0.0\n",
    "        }\n",
    "        \n",
    "    def create_hierarchical_partition_key(self, customer_segment: str, analysis_date: str) -> str:\n",
    "        \"\"\"Create HPK following Azure Cosmos DB best practices\"\"\"\n",
    "        return f\"{self.tenant_id}/{customer_segment}/{analysis_date}\"\n",
    "    \n",
    "    def create_customer_profile_document(self, customer_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Create customer profile document with HPK optimization\"\"\"\n",
    "        \n",
    "        analysis_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        partition_key = self.create_hierarchical_partition_key(\n",
    "            customer_data['customerSegment'], \n",
    "            analysis_date\n",
    "        )\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        utilization_ratio = (\n",
    "            customer_data['balance'] / customer_data['creditLimit'] \n",
    "            if customer_data['creditLimit'] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        # Create document following Azure Cosmos DB best practices\n",
    "        document = {\n",
    "            'id': f\"customer_{customer_data['customerId']}_{int(datetime.now().timestamp())}\",\n",
    "            'partitionKey': partition_key,\n",
    "            'tenantId': self.tenant_id,\n",
    "            'customerSegment': customer_data['customerSegment'].lower(),\n",
    "            'analysisDate': analysis_date,\n",
    "            'documentType': 'customer_profile',\n",
    "            'createdAt': datetime.now(timezone.utc).isoformat(),\n",
    "            'updatedAt': datetime.now(timezone.utc).isoformat(),\n",
    "            'ttl': 365 * 24 * 60 * 60,  # 1 year TTL\n",
    "            \n",
    "            'customerId': customer_data['customerId'],\n",
    "            'profile': {\n",
    "                'displayName': customer_data['customerId'],\n",
    "                'industry': customer_data['industry'],\n",
    "                'kamOwner': customer_data['kamOwner'],\n",
    "                'creditLimit': customer_data['creditLimit'],\n",
    "                'balance': customer_data['balance'],\n",
    "                'dpd': customer_data['daysPassDue'],\n",
    "                'utilizationRatio': utilization_ratio,\n",
    "                'riskScore': self._calculate_risk_score(customer_data),\n",
    "                'segmentCode': customer_data['customerSegment'],\n",
    "                'delinquencyBucket': self._get_delinquency_bucket(customer_data['daysPassDue'])\n",
    "            },\n",
    "            'features': {\n",
    "                'weightedApr': customer_data['apr'],\n",
    "                'balanceZscore': 0.0,  # Calculate based on portfolio\n",
    "                'daysSinceOrigination': self._days_since_origination(customer_data['originationDate']),\n",
    "                'rollRateDirection': 'stable',  # Default\n",
    "                'b2gFlag': customer_data.get('b2gFlag', False),\n",
    "                'customerType': customer_data['productCode']\n",
    "            },\n",
    "            'alerts': []  # Will be populated by risk engine\n",
    "        }\n",
    "        \n",
    "        return document\n",
    "    \n",
    "    def create_portfolio_analysis_document(self, kpis: Dict[str, Any], alerts: List[Dict], insights: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Create portfolio analysis document with HPK optimization\"\"\"\n",
    "        \n",
    "        analysis_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        partition_key = self.create_hierarchical_partition_key('PORTFOLIO', analysis_date)\n",
    "        \n",
    "        document = {\n",
    "            'id': f\"portfolio_analysis_{int(datetime.now().timestamp())}\",\n",
    "            'partitionKey': partition_key,\n",
    "            'tenantId': self.tenant_id,\n",
    "            'customerSegment': 'portfolio',\n",
    "            'analysisDate': analysis_date,\n",
    "            'documentType': 'portfolio_analysis',\n",
    "            'createdAt': datetime.now(timezone.utc).isoformat(),\n",
    "            'updatedAt': datetime.now(timezone.utc).isoformat(),\n",
    "            'ttl': 365 * 24 * 60 * 60,  # 1 year TTL\n",
    "            \n",
    "            'portfolioId': f\"portfolio_{self.tenant_id}_{analysis_date}\",\n",
    "            'kpis': {\n",
    "                'totalAum': kpis['kpis']['portfolio_overview']['total_aum'],\n",
    "                'activeClients': kpis['kpis']['portfolio_overview']['active_customers'],\n",
    "                'creditLines': kpis['kpis']['portfolio_overview']['total_credit_lines'],\n",
    "                'defaultRate': kpis['kpis']['risk_metrics']['default_rate_90plus'],\n",
    "                'churnRate': 0.0,  # Placeholder\n",
    "                'weightedApr': self._calculate_weighted_apr(kpis),\n",
    "                'concentrationTop10': 0.0,  # Placeholder\n",
    "                'b2gPercent': 0.0,  # Placeholder\n",
    "                'dpdBuckets': kpis['kpis']['delinquency_buckets']\n",
    "            },\n",
    "            'insights': insights,\n",
    "            'recommendations': [alert['recommended_action'] for alert in alerts],\n",
    "            'marketContext': {\n",
    "                'analysisMetadata': kpis['metadata'],\n",
    "                'qualityScore': 0.95  # Based on data quality\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return document\n",
    "    \n",
    "    def _calculate_risk_score(self, customer_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate risk score based on customer data\"\"\"\n",
    "        base_score = {'A': 100, 'B': 80, 'C': 60, 'D': 40}.get(customer_data['riskGrade'], 50)\n",
    "        dpd_penalty = min(customer_data['daysPassDue'] * 0.5, 30)\n",
    "        utilization_penalty = (customer_data['balance'] / customer_data['creditLimit']) * 20 if customer_data['creditLimit'] > 0 else 0\n",
    "        \n",
    "        return max(0, base_score - dpd_penalty - utilization_penalty)\n",
    "    \n",
    "    def _get_delinquency_bucket(self, days_past_due: int) -> str:\n",
    "        \"\"\"Get delinquency bucket classification\"\"\"\n",
    "        if days_past_due == 0:\n",
    "            return 'Current'\n",
    "        elif days_past_due <= 30:\n",
    "            return '1-30 DPD'\n",
    "        elif days_past_due <= 60:\n",
    "            return '31-60 DPD'\n",
    "        elif days_past_due <= 90:\n",
    "            return '61-90 DPD'\n",
    "        else:\n",
    "            return '90+ DPD'\n",
    "    \n",
    "    def _days_since_origination(self, origination_date: str) -> int:\n",
    "        \"\"\"Calculate days since origination\"\"\"\n",
    "        orig_date = datetime.strptime(origination_date, '%Y-%m-%d')\n",
    "        return (datetime.now() - orig_date).days\n",
    "    \n",
    "    def _calculate_weighted_apr(self, kpis: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate portfolio weighted APR\"\"\"\n",
    "        segment_data = kpis['kpis']['segment_analysis']\n",
    "        total_aum = kpis['kpis']['portfolio_overview']['total_aum']\n",
    "        \n",
    "        weighted_apr = 0.0\n",
    "        for segment, metrics in segment_data.items():\n",
    "            weight = metrics['total_aum'] / total_aum\n",
    "            weighted_apr += metrics['weighted_apr'] * weight\n",
    "        \n",
    "        return weighted_apr\n",
    "    \n",
    "    def simulate_cosmos_operation(self, operation: str, document: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate Cosmos DB operation with AI Toolkit tracing\"\"\"\n",
    "        \n",
    "        start_time = datetime.now(timezone.utc)\n",
    "        \n",
    "        # Simulate operation latency and RU consumption\n",
    "        latency_ms = np.random.randint(10, 100)  # 10-100ms\n",
    "        ru_consumed = np.random.uniform(5.0, 25.0)  # 5-25 RUs\n",
    "        \n",
    "        # Update metrics\n",
    "        self.operation_metrics['total_operations'] += 1\n",
    "        self.operation_metrics['successful_operations'] += 1\n",
    "        self.operation_metrics['total_ru_consumed'] += ru_consumed\n",
    "        \n",
    "        # Calculate running average latency\n",
    "        current_avg = self.operation_metrics['average_latency']\n",
    "        total_ops = self.operation_metrics['total_operations']\n",
    "        self.operation_metrics['average_latency'] = (\n",
    "            (current_avg * (total_ops - 1) + latency_ms) / total_ops\n",
    "        )\n",
    "        \n",
    "        # AI Toolkit tracing log\n",
    "        trace_data = {\n",
    "            'operation': operation,\n",
    "            'partition_key': document.get('partitionKey'),\n",
    "            'document_type': document.get('documentType'),\n",
    "            'latency_ms': latency_ms,\n",
    "            'ru_consumed': ru_consumed,\n",
    "            'status_code': 201,\n",
    "            'timestamp': start_time.isoformat(),\n",
    "            'tenant_id': self.tenant_id\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Cosmos DB Operation: {trace_data}\")\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'document_id': document['id'],\n",
    "            'partition_key': document['partitionKey'],\n",
    "            'ru_consumed': ru_consumed,\n",
    "            'latency_ms': latency_ms,\n",
    "            'trace_data': trace_data\n",
    "        }\n",
    "\n",
    "# Initialize Cosmos DB client\n",
    "cosmos_client = AbacoCosmosDBClient()\n",
    "\n",
    "# Create customer profile documents (sample of 3 customers)\n",
    "customer_documents = []\n",
    "sample_customers = portfolio_df.head(3).to_dict('records')\n",
    "\n",
    "for customer in sample_customers:\n",
    "    doc = cosmos_client.create_customer_profile_document(customer)\n",
    "    result = cosmos_client.simulate_cosmos_operation('create_customer_profile', doc)\n",
    "    customer_documents.append({\n",
    "        'document': doc,\n",
    "        'result': result\n",
    "    })\n",
    "\n",
    "print(\"ðŸ‘¥ Customer Profile Documents Created:\")\n",
    "for i, doc_result in enumerate(customer_documents, 1):\n",
    "    doc = doc_result['document']\n",
    "    result = doc_result['result']\n",
    "    print(f\"{i}. Customer: {doc['customerId']}\")\n",
    "    print(f\"   Partition Key: {doc['partitionKey']}\")\n",
    "    print(f\"   Risk Score: {doc['profile']['riskScore']:.1f}\")\n",
    "    print(f\"   RU Consumed: {result['ru_consumed']:.2f}\")\n",
    "\n",
    "# Create portfolio analysis document\n",
    "portfolio_doc = cosmos_client.create_portfolio_analysis_document(kpi_results, alerts, insights)\n",
    "portfolio_result = cosmos_client.simulate_cosmos_operation('create_portfolio_analysis', portfolio_doc)\n",
    "\n",
    "print(f\"\\nðŸ“Š Portfolio Analysis Document Created:\")\n",
    "print(f\"   Document ID: {portfolio_doc['id']}\")\n",
    "print(f\"   Partition Key: {portfolio_doc['partitionKey']}\")\n",
    "print(f\"   Total AUM: ${portfolio_doc['kpis']['totalAum']:,.2f}\")\n",
    "print(f\"   RU Consumed: {portfolio_result['ru_consumed']:.2f}\")\n",
    "\n",
    "# Display Cosmos DB operation metrics\n",
    "print(f\"\\nðŸ“ˆ Cosmos DB Operation Metrics:\")\n",
    "print(f\"   Total Operations: {cosmos_client.operation_metrics['total_operations']}\")\n",
    "print(f\"   Success Rate: {cosmos_client.operation_metrics['successful_operations']/cosmos_client.operation_metrics['total_operations']*100:.1f}%\")\n",
    "print(f\"   Total RU Consumed: {cosmos_client.operation_metrics['total_ru_consumed']:.2f}\")\n",
    "print(f\"   Average Latency: {cosmos_client.operation_metrics['average_latency']:.1f}ms\")\n",
    "\n",
    "# Save Cosmos DB documents for review\n",
    "cosmos_export = {\n",
    "    'customer_profiles': [doc_result['document'] for doc_result in customer_documents],\n",
    "    'portfolio_analysis': portfolio_doc,\n",
    "    'operation_metrics': cosmos_client.operation_metrics,\n",
    "    'hpk_examples': {\n",
    "        'enterprise_segment': cosmos_client.create_hierarchical_partition_key('ENTERPRISE', '2024-01-15'),\n",
    "        'portfolio_analysis': cosmos_client.create_hierarchical_partition_key('PORTFOLIO', '2024-01-15'),\n",
    "        'sme_segment': cosmos_client.create_hierarchical_partition_key('SME', '2024-01-15')\n",
    "    }\n",
    "}\n",
    "\n",
    "cosmos_file = REPORTS_PATH / f'cosmos_db_documents_{timestamp_str}.json'\n",
    "with open(cosmos_file, 'w') as f:\n",
    "    json.dump(cosmos_export, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nðŸ“„ Cosmos DB Documents Exported: {cosmos_file}\")\n",
    "print(\"ðŸ”— Ready for Azure Cosmos DB deployment with HPK optimization\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
