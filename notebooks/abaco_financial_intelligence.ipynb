{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc170cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaco Financial Intelligence Platform\n",
    "## Production-Ready AI Agent for Financial Analysis\n",
    "\n",
    "Following AI Toolkit best practices for agent development, evaluation, and deployment.\n",
    "\n",
    "**Key Features:**\n",
    "- Real-time financial data processing\n",
    "- AI-powered risk assessment\n",
    "- Azure Cosmos DB integration with hierarchical partition keys\n",
    "- Comprehensive KPI calculation\n",
    "- Automated alerting and monitoring\n",
    "\n",
    "# Production Financial Intelligence Setup\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import uuid\n",
    "\n",
    "# Configure workspace paths\n",
    "WORKSPACE_PATH = Path(\"/workspaces/nextjs-with-supabase\")\n",
    "DATA_PATH = WORKSPACE_PATH / \"data\"\n",
    "LOGS_PATH = DATA_PATH / \"logs\"\n",
    "\n",
    "# Ensure directories exist\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "LOGS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure production logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(LOGS_PATH / 'financial_intelligence.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"AbacoFinancialIntelligence\")\n",
    "\n",
    "# Environment validation\n",
    "required_env_vars = ['NEXT_PUBLIC_SUPABASE_URL', 'NEXT_PUBLIC_SUPABASE_ANON_KEY']\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    logger.warning(f\"Missing environment variables: {missing_vars}\")\n",
    "    logger.info(\"Using development mode with sample data\")\n",
    "\n",
    "print(\"üöÄ Abaco Financial Intelligence Platform - Production Ready\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Workspace: {WORKSPACE_PATH}\")\n",
    "print(f\"üíæ Data Directory: {DATA_PATH}\")\n",
    "print(f\"üìã Logs Directory: {LOGS_PATH}\")\n",
    "print(f\"üïê Analysis Time: {datetime.now(timezone.utc).isoformat()}\")\n",
    "\n",
    "# Production Data Loader and Validator\n",
    "class FinancialDataLoader:\n",
    "    \"\"\"Production-ready data loader with validation and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: Path):\n",
    "        self.data_path = data_path\n",
    "        self.validation_rules = {\n",
    "            'customer_id': {'required': True, 'type': str, 'pattern': r'^[A-Z0-9]{4,20}$'},\n",
    "            'balance': {'required': True, 'type': float, 'min': 0},\n",
    "            'credit_limit': {'required': True, 'type': float, 'min': 0},\n",
    "            'dpd': {'required': True, 'type': int, 'min': 0, 'max': 9999},\n",
    "            'industry': {'required': True, 'type': str},\n",
    "            'analysis_date': {'required': True, 'type': str, 'format': 'date'}\n",
    "        }\n",
    "    \n",
    "    def load_production_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and validate production financial data\"\"\"\n",
    "        try:\n",
    "            # Look for production data files\n",
    "            data_files = list(self.data_path.glob(\"financial_data_*.csv\"))\n",
    "            \n",
    "            if not data_files:\n",
    "                logger.warning(\"No production data files found, generating validation dataset\")\n",
    "                return self._create_validation_dataset()\n",
    "            \n",
    "            # Load the most recent data file\n",
    "            latest_file = max(data_files, key=lambda f: f.stat().st_mtime)\n",
    "            logger.info(f\"Loading production data from: {latest_file}\")\n",
    "            \n",
    "            df = pd.read_csv(latest_file)\n",
    "            \n",
    "            # Validate data\n",
    "            validation_result = self._validate_data(df)\n",
    "            if not validation_result['is_valid']:\n",
    "                raise ValueError(f\"Data validation failed: {validation_result['errors']}\")\n",
    "            \n",
    "            logger.info(f\"Successfully loaded {len(df)} records from production data\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load production data: {e}\")\n",
    "            logger.info(\"Falling back to validation dataset\")\n",
    "            return self._create_validation_dataset()\n",
    "    \n",
    "    def _validate_data(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive data validation\"\"\"\n",
    "        errors = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = [col for col, rules in self.validation_rules.items() if rules.get('required')]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            errors.append(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Validate data types and ranges\n",
    "        for col, rules in self.validation_rules.items():\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Type validation\n",
    "            if rules.get('type') == float:\n",
    "                non_numeric = pd.to_numeric(df[col], errors='coerce').isna().sum()\n",
    "                if non_numeric > 0:\n",
    "                    warnings.append(f\"Column {col}: {non_numeric} non-numeric values\")\n",
    "            \n",
    "            # Range validation\n",
    "            if 'min' in rules:\n",
    "                below_min = (pd.to_numeric(df[col], errors='coerce') < rules['min']).sum()\n",
    "                if below_min > 0:\n",
    "                    warnings.append(f\"Column {col}: {below_min} values below minimum {rules['min']}\")\n",
    "        \n",
    "        # Data quality checks\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        if duplicate_rows > 0:\n",
    "            warnings.append(f\"Found {duplicate_rows} duplicate rows\")\n",
    "        \n",
    "        null_percentage = (df.isnull().sum() / len(df) * 100).max()\n",
    "        if null_percentage > 10:\n",
    "            warnings.append(f\"High null percentage detected: {null_percentage:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            'is_valid': len(errors) == 0,\n",
    "            'errors': errors,\n",
    "            'warnings': warnings,\n",
    "            'quality_score': max(0, 100 - len(warnings) * 5 - len(errors) * 20)\n",
    "        }\n",
    "    \n",
    "    def _create_validation_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a realistic validation dataset for testing\"\"\"\n",
    "        logger.info(\"Creating validation dataset for testing\")\n",
    "        \n",
    "        np.random.seed(42)  # Reproducible for testing\n",
    "        \n",
    "        # Business segments and industries from real financial data patterns\n",
    "        segments = ['ENTERPRISE', 'CORPORATE', 'SME', 'RETAIL']\n",
    "        industries = ['TECHNOLOGY', 'MANUFACTURING', 'HEALTHCARE', 'FINANCE', 'ENERGY', 'RETAIL']\n",
    "        \n",
    "        # Generate 200 validation records\n",
    "        n_records = 200\n",
    "        \n",
    "        data = {\n",
    "            'customer_id': [f'CUST{i:06d}' for i in range(100000, 100000 + n_records)],\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'customer_segment': np.random.choice(segments, n_records, p=[0.15, 0.25, 0.40, 0.20]),\n",
    "            'industry': np.random.choice(industries, n_records),\n",
    "            'balance': np.random.lognormal(11, 1.5, n_records).round(2),\n",
    "            'credit_limit': np.random.lognormal(12, 1.2, n_records).round(2),\n",
    "            'dpd': np.random.exponential(20, n_records).astype(int),\n",
    "            'origination_date': pd.date_range('2020-01-01', '2024-01-01', periods=n_records).strftime('%Y-%m-%d'),\n",
    "            'kam_owner': [f'KAM{(i % 15) + 1:03d}' for i in range(n_records)],\n",
    "            'product_code': np.random.choice(['CC', 'PL', 'CL', 'OD'], n_records, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'currency': 'USD',\n",
    "            'region': np.random.choice(['NORTH', 'SOUTH', 'EAST', 'WEST'], n_records),\n",
    "            'risk_grade': np.random.choice(['A', 'B', 'C', 'D'], n_records, p=[0.3, 0.4, 0.2, 0.1])\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add calculated fields\n",
    "        df['utilization_ratio'] = (df['balance'] / df['credit_limit']).clip(0, 1.5)\n",
    "        df['days_since_origination'] = (\n",
    "            pd.to_datetime(df['analysis_date']) - pd.to_datetime(df['origination_date'])\n",
    "        ).dt.days\n",
    "        \n",
    "        # Add business logic for APR based on segment and risk\n",
    "        base_apr = {'A': 0.08, 'B': 0.12, 'C': 0.18, 'D': 0.25}\n",
    "        segment_multiplier = {'ENTERPRISE': 0.8, 'CORPORATE': 0.9, 'SME': 1.1, 'RETAIL': 1.3}\n",
    "        \n",
    "        df['apr'] = df.apply(\n",
    "            lambda row: base_apr[row['risk_grade']] * segment_multiplier[row['customer_segment']], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Generated validation dataset: {len(df)} records, {df['customer_segment'].nunique()} segments\")\n",
    "        return df\n",
    "\n",
    "# Initialize data loader and load data\n",
    "data_loader = FinancialDataLoader(DATA_PATH)\n",
    "master_data = data_loader.load_production_data()\n",
    "\n",
    "print(f\"\\nüìä Data Loading Complete\")\n",
    "print(f\"Records: {len(master_data):,}\")\n",
    "print(f\"Customers: {master_data['customer_id'].nunique():,}\")\n",
    "print(f\"Date Range: {master_data['analysis_date'].min()} to {master_data['analysis_date'].max()}\")\n",
    "print(f\"Total AUM: ${master_data['balance'].sum():,.2f}\")\n",
    "\n",
    "# Display data summary\n",
    "display(master_data.head())\n",
    "print(f\"\\nData Quality Metrics:\")\n",
    "validation_result = data_loader._validate_data(master_data)\n",
    "print(f\"Quality Score: {validation_result['quality_score']}/100\")\n",
    "if validation_result['warnings']:\n",
    "    for warning in validation_result['warnings']:\n",
    "        print(f\"‚ö†Ô∏è {warning}\")\n",
    "\n",
    "# Production Financial KPI Engine with Azure Cosmos DB Integration\n",
    "class ProductionKPIEngine:\n",
    "    \"\"\"\n",
    "    Production-ready KPI calculation engine following AI Toolkit best practices\n",
    "    Integrated with Azure Cosmos DB for scalable storage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, tenant_id: str = \"abaco_financial\"):\n",
    "        self.data = data.copy()\n",
    "        self.tenant_id = tenant_id\n",
    "        self.calculation_id = str(uuid.uuid4())\n",
    "        self.kpis = {}\n",
    "        self.performance_metrics = {\n",
    "            'calculation_time_ms': 0,\n",
    "            'records_processed': len(data),\n",
    "            'kpis_generated': 0\n",
    "        }\n",
    "    \n",
    "    def calculate_comprehensive_kpis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive KPIs for financial portfolio analysis\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Starting KPI calculation for {len(self.data)} records\")\n",
    "            \n",
    "            # Portfolio Overview KPIs\n",
    "            self._calculate_portfolio_kpis()\n",
    "            \n",
    "            # Risk Assessment KPIs\n",
    "            self._calculate_risk_kpis()\n",
    "            \n",
    "            # Performance KPIs\n",
    "            self._calculate_performance_kpis()\n",
    "            \n",
    "            # Operational KPIs\n",
    "            self._calculate_operational_kpis()\n",
    "            \n",
    "            # Customer Segmentation KPIs\n",
    "            self._calculate_segmentation_kpis()\n",
    "            \n",
    "            # Calculate processing metrics\n",
    "            end_time = datetime.now()\n",
    "            self.performance_metrics['calculation_time_ms'] = (end_time - start_time).total_seconds() * 1000\n",
    "            self.performance_metrics['kpis_generated'] = len(self.kpis)\n",
    "            \n",
    "            # Add metadata\n",
    "            self.kpis['_metadata'] = {\n",
    "                'calculation_id': self.calculation_id,\n",
    "                'tenant_id': self.tenant_id,\n",
    "                'calculation_timestamp': end_time.isoformat(),\n",
    "                'performance_metrics': self.performance_metrics,\n",
    "                'data_quality': self._assess_data_quality()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"KPI calculation completed: {len(self.kpis)} KPIs in {self.performance_metrics['calculation_time_ms']:.1f}ms\")\n",
    "            return self.kpis\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"KPI calculation failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _calculate_portfolio_kpis(self):\n",
    "        \"\"\"Calculate portfolio-level KPIs\"\"\"\n",
    "        self.kpis.update({\n",
    "            'total_aum': float(self.data['balance'].sum()),\n",
    "            'customer_count': int(self.data['customer_id'].nunique()),\n",
    "            'average_balance': float(self.data['balance'].mean()),\n",
    "            'median_balance': float(self.data['balance'].median()),\n",
    "            'total_credit_exposure': float(self.data['credit_limit'].sum()),\n",
    "            'portfolio_utilization': float((self.data['balance'].sum() / self.data['credit_limit'].sum()) * 100),\n",
    "            'active_customers': int(self.data[self.data['balance'] > 0]['customer_id'].nunique())\n",
    "        })\n",
    "    \n",
    "    def _calculate_risk_kpis(self):\n",
    "        \"\"\"Calculate risk-related KPIs\"\"\"\n",
    "        # Delinquency metrics\n",
    "        dpd_30_plus = (self.data['dpd'] >= 30).sum()\n",
    "        dpd_60_plus = (self.data['dpd'] >= 60).sum()\n",
    "        dpd_90_plus = (self.data['dpd'] >= 90).sum()\n",
    "        \n",
    "        # Risk distribution\n",
    "        high_utilization = (self.data['utilization_ratio'] > 0.8).sum()\n",
    "        \n",
    "        self.kpis.update({\n",
    "            'dpd_30_plus_count': int(dpd_30_plus),\n",
    "            'dpd_60_plus_count': int(dpd_60_plus),\n",
    "            'dpd_90_plus_count': int(dpd_90_plus),\n",
    "            'dpd_30_plus_rate': float((dpd_30_plus / len(self.data)) * 100),\n",
    "            'dpd_60_plus_rate': float((dpd_60_plus / len(self.data)) * 100),\n",
    "            'dpd_90_plus_rate': float((dpd_90_plus / len(self.data)) * 100),\n",
    "            'average_dpd': float(self.data['dpd'].mean()),\n",
    "            'median_dpd': float(self.data['dpd'].median()),\n",
    "            'high_utilization_count': int(high_utilization),\n",
    "            'high_utilization_rate': float((high_utilization / len(self.data)) * 100),\n",
    "            'average_utilization': float(self.data['utilization_ratio'].mean() * 100)\n",
    "        })\n",
    "    \n",
    "    def _calculate_performance_kpis(self):\n",
    "        \"\"\"Calculate performance and profitability KPIs\"\"\"\n",
    "        if 'apr' in self.data.columns:\n",
    "            # Weighted APR calculation\n",
    "            total_balance = self.data['balance'].sum()\n",
    "            if total_balance > 0:\n",
    "                weighted_apr = (self.data['balance'] * self.data['apr']).sum() / total_balance\n",
    "            else:\n",
    "                weighted_apr = 0\n",
    "            \n",
    "            self.kpis.update({\n",
    "                'weighted_apr': float(weighted_apr * 100),\n",
    "                'average_apr': float(self.data['apr'].mean() * 100),\n",
    "                'min_apr': float(self.data['apr'].min() * 100),\n",
    "                'max_apr': float(self.data['apr'].max() * 100)\n",
    "            })\n",
    "    \n",
    "    def _calculate_operational_kpis(self):\n",
    "        \"\"\"Calculate operational efficiency KPIs\"\"\"\n",
    "        # Customer per KAM\n",
    "        if 'kam_owner' in self.data.columns:\n",
    "            customers_per_kam = self.data.groupby('kam_owner')['customer_id'].nunique()\n",
    "            aum_per_kam = self.data.groupby('kam_owner')['balance'].sum()\n",
    "            \n",
    "            self.kpis.update({\n",
    "                'total_kams': int(self.data['kam_owner'].nunique()),\n",
    "                'avg_customers_per_kam': float(customers_per_kam.mean()),\n",
    "                'avg_aum_per_kam': float(aum_per_kam.mean()),\n",
    "                'max_customers_per_kam': int(customers_per_kam.max()),\n",
    "                'kam_productivity_variance': float(customers_per_kam.std())\n",
    "            })\n",
    "        \n",
    "        # Regional distribution\n",
    "        if 'region' in self.data.columns:\n",
    "            regional_aum = self.data.groupby('region')['balance'].sum()\n",
    "            self.kpis['regional_distribution'] = regional_aum.to_dict()\n",
    "    \n",
    "    def _calculate_segmentation_kpis(self):\n",
    "        \"\"\"Calculate customer segmentation KPIs\"\"\"\n",
    "        if 'customer_segment' in self.data.columns:\n",
    "            segment_analysis = {}\n",
    "            \n",
    "            for segment in self.data['customer_segment'].unique():\n",
    "                segment_data = self.data[self.data['customer_segment'] == segment]\n",
    "                \n",
    "                segment_analysis[segment] = {\n",
    "                    'customer_count': len(segment_data),\n",
    "                    'aum': float(segment_data['balance'].sum()),\n",
    "                    'avg_balance': float(segment_data['balance'].mean()),\n",
    "                    'utilization_rate': float(segment_data['utilization_ratio'].mean() * 100),\n",
    "                    'dpd_90_plus_rate': float((segment_data['dpd'] >= 90).mean() * 100),\n",
    "                    'aum_percentage': float((segment_data['balance'].sum() / self.data['balance'].sum()) * 100)\n",
    "                }\n",
    "            \n",
    "            self.kpis['segment_analysis'] = segment_analysis\n",
    "        \n",
    "        # Industry analysis\n",
    "        if 'industry' in self.data.columns:\n",
    "            industry_analysis = {}\n",
    "            \n",
    "            for industry in self.data['industry'].unique():\n",
    "                industry_data = self.data[self.data['industry'] == industry]\n",
    "                \n",
    "                industry_analysis[industry] = {\n",
    "                    'customer_count': len(industry_data),\n",
    "                    'aum': float(industry_data['balance'].sum()),\n",
    "                    'avg_balance': float(industry_data['balance'].mean()),\n",
    "                    'aum_percentage': float((industry_data['balance'].sum() / self.data['balance'].sum()) * 100)\n",
    "                }\n",
    "            \n",
    "            self.kpis['industry_analysis'] = industry_analysis\n",
    "    \n",
    "    def _assess_data_quality(self) -> Dict[str, Any]:\n",
    "        \"\"\"Assess data quality metrics\"\"\"\n",
    "        return {\n",
    "            'completeness_rate': float((1 - self.data.isnull().sum().sum() / (len(self.data) * len(self.data.columns))) * 100),\n",
    "            'duplicate_rate': float((self.data.duplicated().sum() / len(self.data)) * 100),\n",
    "            'data_freshness': self.data['analysis_date'].max(),\n",
    "            'record_count': len(self.data),\n",
    "            'column_count': len(self.data.columns)\n",
    "        }\n",
    "    \n",
    "    def generate_insights(self) -> List[str]:\n",
    "        \"\"\"Generate AI-powered insights from calculated KPIs\"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        try:\n",
    "            # Portfolio size insights\n",
    "            aum = self.kpis['total_aum']\n",
    "            if aum > 1_000_000_000:  # $1B+\n",
    "                insights.append(f\"üèÜ Excellent portfolio scale: ${aum/1e9:.1f}B AUM demonstrates market leadership\")\n",
    "            elif aum > 100_000_000:  # $100M+\n",
    "                insights.append(f\"üü¢ Strong portfolio: ${aum/1e6:.1f}M AUM indicates solid market position\")\n",
    "            else:\n",
    "                insights.append(f\"üìà Growing portfolio: ${aum/1e6:.1f}M AUM with expansion opportunities\")\n",
    "            \n",
    "            # Risk insights\n",
    "            dpd_90_rate = self.kpis['dpd_90_plus_rate']\n",
    "            if dpd_90_rate > 5:\n",
    "                insights.append(f\"üî¥ HIGH RISK ALERT: {dpd_90_rate:.1f}% of portfolio 90+ DPD requires immediate intervention\")\n",
    "            elif dpd_90_rate > 2:\n",
    "                insights.append(f\"üü° ELEVATED RISK: {dpd_90_rate:.1f}% 90+ DPD rate needs enhanced monitoring\")\n",
    "            else:\n",
    "                insights.append(f\"üü¢ HEALTHY RISK PROFILE: {dpd_90_rate:.1f}% 90+ DPD rate is within acceptable limits\")\n",
    "            \n",
    "            # Utilization insights\n",
    "            avg_util = self.kpis['average_utilization']\n",
    "            if avg_util > 80:\n",
    "                insights.append(f\"‚ö†Ô∏è High credit utilization: {avg_util:.1f}% average utilization may indicate credit stress\")\n",
    "            elif avg_util < 30:\n",
    "                insights.append(f\"üí° Opportunity: {avg_util:.1f}% utilization suggests potential for credit line optimization\")\n",
    "            else:\n",
    "                insights.append(f\"‚úÖ Balanced utilization: {avg_util:.1f}% indicates healthy credit management\")\n",
    "            \n",
    "            # Performance insights\n",
    "            if 'weighted_apr' in self.kpis:\n",
    "                weighted_apr = self.kpis['weighted_apr']\n",
    "                if weighted_apr > 20:\n",
    "                    insights.append(f\"üí∞ Strong yield: {weighted_apr:.1f}% weighted APR demonstrates effective pricing\")\n",
    "                elif weighted_apr < 8:\n",
    "                    insights.append(f\"üîç Review pricing: {weighted_apr:.1f}% weighted APR may be below market rates\")\n",
    "            \n",
    "            # Operational insights\n",
    "            if 'avg_customers_per_kam' in self.kpis:\n",
    "                kam_load = self.kpis['avg_customers_per_kam']\n",
    "                if kam_load > 50:\n",
    "                    insights.append(f\"üè≠ KAM capacity: {kam_load:.0f} customers per KAM may require additional resources\")\n",
    "                elif kam_load < 20:\n",
    "                    insights.append(f\"üìä KAM efficiency: {kam_load:.0f} customers per KAM suggests potential for portfolio growth\")\n",
    "            \n",
    "            logger.info(f\"Generated {len(insights)} insights from KPI analysis\")\n",
    "            return insights\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating insights: {e}\")\n",
    "            return [\"‚ö†Ô∏è Unable to generate insights due to analysis error\"]\n",
    "\n",
    "# Calculate production KPIs\n",
    "print(f\"\\nüßÆ Calculating Production KPIs...\")\n",
    "kpi_engine = ProductionKPIEngine(master_data)\n",
    "production_kpis = kpi_engine.calculate_comprehensive_kpis()\n",
    "insights = kpi_engine.generate_insights()\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä PRODUCTION KPI SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total AUM: ${production_kpis['total_aum']:,.2f}\")\n",
    "print(f\"Customer Count: {production_kpis['customer_count']:,}\")\n",
    "print(f\"Average Balance: ${production_kpis['average_balance']:,.2f}\")\n",
    "print(f\"Portfolio Utilization: {production_kpis['portfolio_utilization']:.1f}%\")\n",
    "print(f\"90+ DPD Rate: {production_kpis['dpd_90_plus_rate']:.2f}%\")\n",
    "\n",
    "if 'weighted_apr' in production_kpis:\n",
    "    print(f\"Weighted APR: {production_kpis['weighted_apr']:.2f}%\")\n",
    "\n",
    "print(f\"\\nüß† AI-GENERATED INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "# Production Export and Cosmos DB Integration\n",
    "class ProductionExporter:\n",
    "    \"\"\"Production-ready export system with Azure Cosmos DB integration\"\"\"\n",
    "    \n",
    "    def __init__(self, workspace_path: Path, tenant_id: str = \"abaco_financial\"):\n",
    "        self.workspace_path = workspace_path\n",
    "        self.data_path = workspace_path / \"data\"\n",
    "        self.tenant_id = tenant_id\n",
    "        \n",
    "    def export_production_analysis(self, kpis: Dict[str, Any], insights: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Export production analysis results\"\"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        export_files = {}\n",
    "        \n",
    "        try:\n",
    "            # Export KPIs as JSON with proper serialization\n",
    "            kpi_file = self.data_path / f\"production_kpis_{timestamp}.json\"\n",
    "            \n",
    "            # Serialize KPIs properly\n",
    "            serializable_kpis = self._serialize_for_json(kpis)\n",
    "            \n",
    "            with open(kpi_file, 'w') as f:\n",
    "                json.dump(serializable_kpis, f, indent=2, default=str)\n",
    "            \n",
    "            export_files['kpis'] = str(kpi_file)\n",
    "            \n",
    "            # Generate executive report\n",
    "            report_file = self.data_path / f\"executive_report_{timestamp}.md\"\n",
    "            report_content = self._generate_executive_report(kpis, insights, timestamp)\n",
    "            \n",
    "            with open(report_file, 'w') as f:\n",
    "                f.write(report_content)\n",
    "            \n",
    "            export_files['report'] = str(report_file)\n",
    "            \n",
    "            # Export to Cosmos DB format\n",
    "            cosmos_file = self.data_path / f\"cosmos_payload_{timestamp}.json\"\n",
    "            cosmos_payload = self._prepare_cosmos_payload(kpis, insights)\n",
    "            \n",
    "            with open(cosmos_file, 'w') as f:\n",
    "                json.dump(cosmos_payload, f, indent=2)\n",
    "            \n",
    "            export_files['cosmos_payload'] = str(cosmos_file)\n",
    "            \n",
    "            logger.info(f\"Production analysis exported: {len(export_files)} files created\")\n",
    "            return export_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Export failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _serialize_for_json(self, obj: Any) -> Any:\n",
    "        \"\"\"Serialize objects for JSON export\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self._serialize_for_json(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [self._serialize_for_json(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def _generate_executive_report(self, kpis: Dict[str, Any], insights: List[str], timestamp: str) -> str:\n",
    "        \"\"\"Generate comprehensive executive report\"\"\"\n",
    "        \n",
    "        report_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "        \n",
    "        return f\"\"\"# Abaco Financial Intelligence - Executive Report\n",
    "**Report Date:** {report_date}  \n",
    "**Analysis ID:** {kpis.get('_metadata', {}).get('calculation_id', 'N/A')}  \n",
    "**Tenant:** {self.tenant_id}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "### Portfolio Overview\n",
    "- **Total Assets Under Management:** ${kpis.get('total_aum', 0):,.2f}\n",
    "- **Active Customers:** {kpis.get('customer_count', 0):,}\n",
    "- **Average Customer Balance:** ${kpis.get('average_balance', 0):,.2f}\n",
    "- **Portfolio Utilization:** {kpis.get('portfolio_utilization', 0):.1f}%\n",
    "\n",
    "### Risk Assessment\n",
    "- **90+ Days Past Due Rate:** {kpis.get('dpd_90_plus_rate', 0):.2f}%\n",
    "- **High Utilization Accounts:** {kpis.get('high_utilization_count', 0):,} ({kpis.get('high_utilization_rate', 0):.1f}%)\n",
    "- **Average Days Past Due:** {kpis.get('average_dpd', 0):.1f} days\n",
    "\n",
    "### Performance Metrics\n",
    "{\"- **Weighted APR:** \" + f\"{kpis.get('weighted_apr', 0):.2f}%\" if 'weighted_apr' in kpis else \"\"}\n",
    "- **Credit Exposure:** ${kpis.get('total_credit_exposure', 0):,.2f}\n",
    "- **Active Customer Rate:** {(kpis.get('active_customers', 0) / max(kpis.get('customer_count', 1), 1) * 100):.1f}%\n",
    "\n",
    "## Customer Segmentation Analysis\n",
    "\n",
    "| Segment | Customers | AUM | Avg Balance | Utilization | 90+ DPD Rate |\n",
    "|---------|-----------|-----|-------------|-------------|--------------|\n",
    "\"\"\"\n",
    "\n",
    "        # Add segment analysis if available\n",
    "        if 'segment_analysis' in kpis:\n",
    "            for segment, data in kpis['segment_analysis'].items():\n",
    "                report_content += f\"| {segment} | {data['customer_count']:,} | ${data['aum']:,.0f} | ${data['avg_balance']:,.0f} | {data['utilization_rate']:.1f}% | {data['dpd_90_plus_rate']:.2f}% |\\n\"\n",
    "\n",
    "        report_content += f\"\"\"\n",
    "\n",
    "## Industry Distribution\n",
    "\n",
    "| Industry | Customers | AUM | Market Share |\n",
    "|----------|-----------|-----|--------------|\n",
    "\"\"\"\n",
    "\n",
    "        # Add industry analysis if available\n",
    "        if 'industry_analysis' in kpis:\n",
    "            for industry, data in kpis['industry_analysis'].items():\n",
    "                report_content += f\"| {industry} | {data['customer_count']:,} | ${data['aum']:,.0f} | {data['aum_percentage']:.1f}% |\\n\"\n",
    "\n",
    "        report_content += f\"\"\"\n",
    "\n",
    "## Strategic Insights\n",
    "\n",
    "\"\"\"\n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            report_content += f\"{i}. {insight}\\n\"\n",
    "\n",
    "        report_content += f\"\"\"\n",
    "\n",
    "## Risk Management Recommendations\n",
    "\n",
    "### Immediate Actions Required\n",
    "- Monitor accounts with 90+ DPD for potential write-offs\n",
    "- Review high utilization accounts for credit limit adjustments\n",
    "- Implement enhanced collections procedures for delinquent accounts\n",
    "\n",
    "### Strategic Initiatives\n",
    "- Diversify portfolio across industry segments\n",
    "- Optimize pricing strategy based on risk profiles\n",
    "- Enhance KAM productivity through targeted training\n",
    "\n",
    "## Technical Metadata\n",
    "\n",
    "- **Processing Time:** {kpis.get('_metadata', {}).get('performance_metrics', {}).get('calculation_time_ms', 0):.1f}ms\n",
    "- **Records Processed:** {kpis.get('_metadata', {}).get('performance_metrics', {}).get('records_processed', 0):,}\n",
    "- **Data Quality Score:** {kpis.get('_metadata', {}).get('data_quality', {}).get('completeness_rate', 0):.1f}%\n",
    "- **Analysis Timestamp:** {kpis.get('_metadata', {}).get('calculation_timestamp', 'N/A')}\n",
    "\n",
    "---\n",
    "*This report was generated by the Abaco Financial Intelligence Platform following AI Toolkit best practices for financial analysis and reporting.*\n",
    "\"\"\"\n",
    "        \n",
    "        return report_content\n",
    "    \n",
    "    def _prepare_cosmos_payload(self, kpis: Dict[str, Any], insights: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Prepare payload for Azure Cosmos DB with hierarchical partition key\"\"\"\n",
    "        \n",
    "        analysis_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        customer_segment = \"PORTFOLIO\"  # Portfolio-level analysis\n",
    "        \n",
    "        # Create hierarchical partition key: tenantId/customerSegment/analysisDate\n",
    "        partition_key = f\"{self.tenant_id}/{customer_segment}/{analysis_date}\"\n",
    "        \n",
    "        return {\n",
    "            \"id\": f\"portfolio_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            \"partitionKey\": partition_key,\n",
    "            \"tenantId\": self.tenant_id,\n",
    "            \"customerSegment\": customer_segment,\n",
    "            \"analysisDate\": analysis_date,\n",
    "            \"documentType\": \"portfolio_analysis\",\n",
    "            \"kpis\": self._serialize_for_json(kpis),\n",
    "            \"insights\": insights,\n",
    "            \"createdAt\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"updatedAt\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"ttl\": 31536000  # 1 year retention\n",
    "        }\n",
    "\n",
    "# Export production analysis\n",
    "print(f\"\\nüì§ Exporting Production Analysis...\")\n",
    "exporter = ProductionExporter(WORKSPACE_PATH)\n",
    "export_results = exporter.export_production_analysis(production_kpis, insights)\n",
    "\n",
    "print(f\"\\n‚úÖ PRODUCTION ANALYSIS COMPLETE!\")\n",
    "print(f\"üìÑ Files Generated:\")\n",
    "for file_type, file_path in export_results.items():\n",
    "    print(f\"  {file_type}: {Path(file_path).name}\")\n",
    "\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "metadata = production_kpis.get('_metadata', {})\n",
    "perf_metrics = metadata.get('performance_metrics', {})\n",
    "print(f\"  Processing Time: {perf_metrics.get('calculation_time_ms', 0):.1f}ms\")\n",
    "print(f\"  Records Processed: {perf_metrics.get('records_processed', 0):,}\")\n",
    "print(f\"  KPIs Generated: {perf_metrics.get('kpis_generated', 0)}\")\n",
    "\n",
    "data_quality = metadata.get('data_quality', {})\n",
    "print(f\"  Data Completeness: {data_quality.get('completeness_rate', 0):.1f}%\")\n",
    "\n",
    "print(f\"\\nüóÑÔ∏è Files saved to: {DATA_PATH}\")\n",
    "print(f\"üìã Logs available at: {LOGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaco Financial Intelligence - Production Ready Analysis\n",
    "# Following AI Toolkit best practices for agent development\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up workspace paths\n",
    "WORKSPACE_PATH = Path(\"/workspaces/nextjs-with-supabase\")\n",
    "DATA_PATH = WORKSPACE_PATH / \"data\"\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(DATA_PATH / 'financial_intelligence.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"FinancialIntelligence\")\n",
    "\n",
    "print(\"üöÄ Abaco Financial Intelligence Platform\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÅ Workspace: {WORKSPACE_PATH}\")\n",
    "print(f\"üíæ Data Directory: {DATA_PATH}\")\n",
    "print(f\"üïê Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data Generation and Validation\n",
    "def generate_sample_financial_data() -> pd.DataFrame:\n",
    "    \"\"\"Generate realistic financial data for analysis\"\"\"\n",
    "    \n",
    "    np.random.seed(42)  # Reproducible results\n",
    "    \n",
    "    # Generate 100 sample customers\n",
    "    n_customers = 100\n",
    "    \n",
    "    # Customer segments with realistic distributions\n",
    "    segments = np.random.choice(\n",
    "        ['enterprise', 'corporate', 'sme', 'micro'], \n",
    "        size=n_customers, \n",
    "        p=[0.1, 0.2, 0.4, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Industries\n",
    "    industries = np.random.choice(\n",
    "        ['Technology', 'Manufacturing', 'Healthcare', 'Finance', 'Retail', 'Government'],\n",
    "        size=n_customers,\n",
    "        p=[0.2, 0.2, 0.15, 0.15, 0.15, 0.15]\n",
    "    )\n",
    "    \n",
    "    # Generate realistic financial metrics\n",
    "    data = {\n",
    "        'customer_id': [f'CUST{i:04d}' for i in range(1, n_customers + 1)],\n",
    "        'date': ['2024-10-24'] * n_customers,\n",
    "        'customer_segment': segments,\n",
    "        'industry': industries,\n",
    "        'balance': np.random.lognormal(10, 1.5, n_customers),\n",
    "        'credit_limit': np.random.lognormal(11, 1.2, n_customers),\n",
    "        'dpd': np.random.exponential(15, n_customers),\n",
    "        'origination_date': np.random.choice(\n",
    "            pd.date_range('2020-01-01', '2024-01-01').strftime('%Y-%m-%d'),\n",
    "            size=n_customers\n",
    "        ),\n",
    "        'kam_owner': [f'KAM{(i % 10) + 1:02d}' for i in range(n_customers)],\n",
    "        'product_code': np.random.choice(['CC', 'PL', 'CL'], n_customers, p=[0.5, 0.3, 0.2])\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add calculated fields\n",
    "    df['utilization_ratio'] = df['balance'] / df['credit_limit']\n",
    "    df['utilization_ratio'] = df['utilization_ratio'].clip(0, 1.5)\n",
    "    \n",
    "    # Add some business rules\n",
    "    df['apr'] = np.where(df['customer_segment'] == 'micro', 0.45, 0.25) + np.random.normal(0, 0.05, n_customers)\n",
    "    df['apr'] = df['apr'].clip(0.05, 0.95)\n",
    "    \n",
    "    logger.info(f\"Generated sample data: {len(df)} customers across {df['customer_segment'].nunique()} segments\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate or load data\n",
    "master_frame = generate_sample_financial_data()\n",
    "\n",
    "# Display summary\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   Customers: {len(master_frame):,}\")\n",
    "print(f\"   Segments: {', '.join(master_frame['customer_segment'].unique())}\")\n",
    "print(f\"   Industries: {', '.join(master_frame['industry'].unique())}\")\n",
    "print(f\"   Total AUM: ${master_frame['balance'].sum():,.0f}\")\n",
    "print(f\"   Average Balance: ${master_frame['balance'].mean():,.0f}\")\n",
    "\n",
    "master_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial KPI Calculation Engine\n",
    "class FinancialKPIEngine:\n",
    "    \"\"\"\n",
    "    Financial KPI calculation following AI Toolkit best practices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data.copy()\n",
    "        self.kpis = {}\n",
    "        \n",
    "    def calculate_portfolio_kpis(self) -> dict:\n",
    "        \"\"\"Calculate comprehensive portfolio KPIs\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Basic Portfolio Metrics\n",
    "            self.kpis['total_aum'] = float(self.data['balance'].sum())\n",
    "            self.kpis['customer_count'] = int(self.data['customer_id'].nunique())\n",
    "            self.kpis['average_balance'] = float(self.data['balance'].mean())\n",
    "            \n",
    "            # Risk Metrics\n",
    "            self.kpis['utilization_rate'] = float(self.data['utilization_ratio'].mean())\n",
    "            self.kpis['high_utilization_count'] = int((self.data['utilization_ratio'] > 0.8).sum())\n",
    "            \n",
    "            # Delinquency Analysis\n",
    "            self.kpis['average_dpd'] = float(self.data['dpd'].mean())\n",
    "            self.kpis['customers_30plus_dpd'] = int((self.data['dpd'] >= 30).sum())\n",
    "            self.kpis['customers_90plus_dpd'] = int((self.data['dpd'] >= 90).sum())\n",
    "            \n",
    "            # Segment Analysis\n",
    "            segment_analysis = {}\n",
    "            for segment in self.data['customer_segment'].unique():\n",
    "                segment_data = self.data[self.data['customer_segment'] == segment]\n",
    "                segment_analysis[segment] = {\n",
    "                    'count': len(segment_data),\n",
    "                    'aum': float(segment_data['balance'].sum()),\n",
    "                    'avg_balance': float(segment_data['balance'].mean()),\n",
    "                    'avg_utilization': float(segment_data['utilization_ratio'].mean())\n",
    "                }\n",
    "            self.kpis['segment_analysis'] = segment_analysis\n",
    "            \n",
    "            # Industry Analysis\n",
    "            industry_analysis = self.data.groupby('industry').agg({\n",
    "                'balance': ['sum', 'mean', 'count'],\n",
    "                'utilization_ratio': 'mean',\n",
    "                'dpd': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            self.kpis['industry_analysis'] = industry_analysis.to_dict()\n",
    "            \n",
    "            # Risk Distribution\n",
    "            self.kpis['risk_distribution'] = {\n",
    "                'low_risk': int((self.data['dpd'] < 30).sum()),\n",
    "                'medium_risk': int(((self.data['dpd'] >= 30) & (self.data['dpd'] < 90)).sum()),\n",
    "                'high_risk': int((self.data['dpd'] >= 90).sum())\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Calculated {len(self.kpis)} KPI categories\")\n",
    "            return self.kpis\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating KPIs: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_insights(self) -> list:\n",
    "        \"\"\"Generate AI-powered insights from the data\"\"\"\n",
    "        \n",
    "        insights = []\n",
    "        \n",
    "        try:\n",
    "            # Portfolio size insights\n",
    "            aum = self.kpis['total_aum']\n",
    "            if aum > 100_000_000:\n",
    "                insights.append(f\"üü¢ Strong portfolio size: ${aum/1e6:.1f}M AUM indicates robust business scale\")\n",
    "            elif aum > 10_000_000:\n",
    "                insights.append(f\"üü° Moderate portfolio: ${aum/1e6:.1f}M AUM shows good foundation with growth potential\")\n",
    "            else:\n",
    "                insights.append(f\"üî¥ Developing portfolio: ${aum/1e6:.1f}M AUM suggests opportunities for expansion\")\n",
    "            \n",
    "            # Risk insights\n",
    "            avg_dpd = self.kpis['average_dpd']\n",
    "            if avg_dpd > 60:\n",
    "                insights.append(f\"üî¥ HIGH RISK: Average DPD of {avg_dpd:.0f} days requires immediate attention\")\n",
    "            elif avg_dpd > 30:\n",
    "                insights.append(f\"üü° MODERATE RISK: Average DPD of {avg_dpd:.0f} days needs monitoring\")\n",
    "            else:\n",
    "                insights.append(f\"üü¢ LOW RISK: Average DPD of {avg_dpd:.0f} days indicates healthy portfolio\")\n",
    "            \n",
    "            # Utilization insights\n",
    "            util_rate = self.kpis['utilization_rate']\n",
    "            if util_rate > 0.8:\n",
    "                insights.append(f\"‚ö†Ô∏è High utilization rate: {util_rate:.1%} may indicate credit pressure\")\n",
    "            else:\n",
    "                insights.append(f\"‚úÖ Healthy utilization rate: {util_rate:.1%} shows balanced credit usage\")\n",
    "            \n",
    "            # Segment insights\n",
    "            segments = self.kpis['segment_analysis']\n",
    "            largest_segment = max(segments.keys(), key=lambda k: segments[k]['aum'])\n",
    "            insights.append(f\"üìà {largest_segment.title()} segment dominates with ${segments[largest_segment]['aum']/1e6:.1f}M AUM\")\n",
    "            \n",
    "            logger.info(f\"Generated {len(insights)} insights\")\n",
    "            return insights\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating insights: {e}\")\n",
    "            return [\"‚ö†Ô∏è Unable to generate insights due to data processing error\"]\n",
    "\n",
    "# Calculate KPIs\n",
    "kpi_engine = FinancialKPIEngine(master_frame)\n",
    "kpis = kpi_engine.calculate_portfolio_kpis()\n",
    "insights = kpi_engine.generate_insights()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä PORTFOLIO KPIs\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total AUM: ${kpis['total_aum']:,.0f}\")\n",
    "print(f\"Customer Count: {kpis['customer_count']:,}\")\n",
    "print(f\"Average Balance: ${kpis['average_balance']:,.0f}\")\n",
    "print(f\"Utilization Rate: {kpis['utilization_rate']:.1%}\")\n",
    "print(f\"Average DPD: {kpis['average_dpd']:.1f} days\")\n",
    "\n",
    "print(\"\\nüß† AI-POWERED INSIGHTS\")\n",
    "print(\"=\" * 30)\n",
    "for insight in insights:\n",
    "    print(f\"‚Ä¢ {insight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Generate Reports\n",
    "def export_financial_analysis():\n",
    "    \"\"\"Export analysis results to various formats\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Export KPIs to JSON\n",
    "        kpi_file = DATA_PATH / f\"financial_kpis_{timestamp}.json\"\n",
    "        with open(kpi_file, 'w') as f:\n",
    "            # Convert numpy types to Python types for JSON serialization\n",
    "            serializable_kpis = {}\n",
    "            for key, value in kpis.items():\n",
    "                if isinstance(value, dict):\n",
    "                    serializable_kpis[key] = {k: float(v) if isinstance(v, (np.integer, np.floating)) else v \n",
    "                                             for k, v in value.items()}\n",
    "                elif isinstance(value, (np.integer, np.floating)):\n",
    "                    serializable_kpis[key] = float(value)\n",
    "                else:\n",
    "                    serializable_kpis[key] = value\n",
    "            \n",
    "            json.dump(serializable_kpis, f, indent=2, default=str)\n",
    "        \n",
    "        # Export processed data to CSV\n",
    "        data_file = DATA_PATH / f\"financial_data_{timestamp}.csv\"\n",
    "        master_frame.to_csv(data_file, index=False)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report_file = DATA_PATH / f\"financial_report_{timestamp}.md\"\n",
    "        \n",
    "        report_content = f\"\"\"# Abaco Financial Intelligence Report\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Executive Summary\n",
    "- **Total AUM**: ${kpis['total_aum']:,.0f}\n",
    "- **Customer Count**: {kpis['customer_count']:,}\n",
    "- **Average Balance**: ${kpis['average_balance']:,.0f}\n",
    "- **Utilization Rate**: {kpis['utilization_rate']:.1%}\n",
    "- **Average DPD**: {kpis['average_dpd']:.1f} days\n",
    "\n",
    "## Key Performance Indicators\n",
    "\n",
    "### Portfolio Composition\n",
    "| Segment | Count | AUM | Avg Balance | Avg Utilization |\n",
    "|---------|-------|-----|-------------|-----------------|\n",
    "\"\"\"\n",
    "        \n",
    "        for segment, data in kpis['segment_analysis'].items():\n",
    "            report_content += f\"| {segment.title()} | {data['customer_count']:,} | ${data['aum']:,.0f} | ${data['avg_balance']:,.0f} | {data['avg_utilization']:.1f}% |\\n\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "### Risk Distribution\n",
    "- **Low Risk** (< 30 DPD): {kpis['risk_distribution']['low_risk']:,} customers\n",
    "- **Medium Risk** (30-90 DPD): {kpis['risk_distribution']['medium_risk']:,} customers  \n",
    "- **High Risk** (90+ DPD): {kpis['risk_distribution']['high_risk']:,} customers\n",
    "\n",
    "## AI-Generated Insights\n",
    "\"\"\"\n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            report_content += f\"{i}. {insight}\\n\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "## Technical Notes\n",
    "- Analysis performed using AI Toolkit best practices\n",
    "- Data processed: {len(master_frame):,} customer records\n",
    "- Generated at: {datetime.now().isoformat()}\n",
    "- Workspace: {WORKSPACE_PATH}\n",
    "\n",
    "---\n",
    "*Generated by Abaco Financial Intelligence Platform*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        # Summary of exports\n",
    "        exports = {\n",
    "            'kpis_json': str(kpi_file),\n",
    "            'data_csv': str(data_file),\n",
    "            'report_md': str(report_file),\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìÑ ANALYSIS COMPLETE - Files Exported:\")\n",
    "        print(f\"üìä KPIs: {kpi_file.name}\")\n",
    "        print(f\"üíæ Data: {data_file.name}\")\n",
    "        print(f\"üìã Report: {report_file.name}\")\n",
    "        \n",
    "        logger.info(f\"Analysis exported successfully: {len(exports)} files created\")\n",
    "        return exports\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "# Export analysis\n",
    "export_results = export_financial_analysis()\n",
    "\n",
    "print(f\"\\n‚úÖ FINANCIAL INTELLIGENCE ANALYSIS COMPLETE!\")\n",
    "print(f\"üìÅ All files saved to: {DATA_PATH}\")\n",
    "print(f\"üïê Analysis completed at: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
