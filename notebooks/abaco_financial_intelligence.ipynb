{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5813105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample master_frame with 3 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1214: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Iterable, List, NamedTuple, Optional\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "alerts_frame = globals().get(\"alerts_frame\", pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "master_frame = globals().get(\"master_frame\")\n",
    "\n",
    "def _build_sample_master_frame() -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"customer_id\": [\"CUST001\", \"CUST002\", \"CUST003\"],\n",
    "        \"date\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"],\n",
    "        \"balance\": [100000, 50000, 25000],\n",
    "        \"credit_limit\": [150000, 75000, 30000],\n",
    "        \"dpd\": [0, 45, 95],\n",
    "        \"product_code\": [\"CC\", \"PL\", \"CC\"],\n",
    "        \"origination_date\": [\"2023-01-01\", \"2023-06-01\", \"2023-12-01\"],\n",
    "        \"industry\": [\"Technology\", \"Manufacturing\", \"Government\"],\n",
    "        \"kam_owner\": [\"KAM001\", \"KAM002\", \"KAM001\"]\n",
    "    })\n",
    "if master_frame is None or getattr(master_frame, \"empty\", True):\n",
    "    master_frame = _build_sample_master_frame()\n",
    "    print(\"Created sample master_frame with\", len(master_frame), \"records\")\n",
    "\n",
    "DELINQUENCY_BUCKETS = [-np.inf, 0, 30, 60, 90, 120, np.inf]\n",
    "DELINQUENCY_LABELS = [\"current\", \"1_30\", \"31_60\", \"61_90\", \"91_120\", \"120_plus\"]\n",
    "SEGMENT_LABELS = list(\"ABCDEF\")\n",
    "\n",
    "class FeatureArtifacts(NamedTuple):\n",
    "    features: pd.DataFrame\n",
    "    alerts: pd.DataFrame\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, reference_date: Optional[pd.Timestamp] = None) -> None:\n",
    "        self.reference_date = reference_date or pd.Timestamp.utcnow().normalize()\n",
    "\n",
    "    def _derive_customer_type(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        if \"customer_type\" in frame.columns:\n",
    "            return frame[\"customer_type\"].fillna(\"unspecified\")\n",
    "        balance = frame.get(\"balance\")\n",
    "        if balance is None:\n",
    "            balance = pd.Series(0, index=frame.index)\n",
    "        elif not isinstance(balance, pd.Series):\n",
    "            balance = pd.Series(balance, index=frame.index)\n",
    "        balance = balance.fillna(0)\n",
    "        exposure = frame.get(\"credit_limit\")\n",
    "        if exposure is None:\n",
    "            exposure = balance.clip(lower=1)\n",
    "        elif not isinstance(exposure, pd.Series):\n",
    "            exposure = pd.Series(exposure, index=frame.index)\n",
    "        exposure = exposure.where(exposure.notna(), balance.clip(lower=1))\n",
    "        ratio = balance / exposure.replace({0: np.nan})\n",
    "        derived = np.where(balance >= 5_000_000, \"enterprise\", np.where(balance >= 500_000, \"corporate\", np.where(balance >= 50_000, \"sme\", \"micro\")))\n",
    "        derived = pd.Series(derived, index=frame.index)\n",
    "        derived = np.where(ratio >= 0.9, \"intensive\", derived)\n",
    "        return pd.Series(derived, index=frame.index)\n",
    "\n",
    "    def _segmentation(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        try:\n",
    "            return pd.qcut(frame[\"balance\"], q=min(6, frame[\"balance\"].nunique()), labels=SEGMENT_LABELS[: min(6, frame[\"balance\"].nunique())], duplicates=\"drop\").astype(str)\n",
    "        except Exception:\n",
    "            return pd.Series([\"A\"] * len(frame), index=frame.index)\n",
    "\n",
    "    def _delinquency_bucket(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        if \"dpd\" in frame.columns:\n",
    "            dpd_source = frame[\"dpd\"]\n",
    "        else:\n",
    "            dpd_source = frame.get(\"days_past_due\", pd.Series(0, index=frame.index))\n",
    "        if not isinstance(dpd_source, pd.Series):\n",
    "            dpd_source = pd.Series(dpd_source, index=frame.index)\n",
    "        dpd_series = pd.to_numeric(dpd_source, errors=\"coerce\").fillna(0)\n",
    "        return pd.cut(dpd_series, bins=DELINQUENCY_BUCKETS, labels=DELINQUENCY_LABELS, right=True)\n",
    "\n",
    "    def transform(self, frame: pd.DataFrame) -> FeatureArtifacts:\n",
    "        if frame.empty:\n",
    "            return FeatureArtifacts(frame.copy(), pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "        features = frame.copy()\n",
    "        features[\"date\"] = pd.to_datetime(features[\"date\"], errors=\"coerce\", utc=True)\n",
    "        features[\"customer_type\"] = self._derive_customer_type(features)\n",
    "        features[\"segment_code\"] = self._segmentation(features)\n",
    "        features[\"delinquency_bucket\"] = self._delinquency_bucket(features).astype(str)\n",
    "        features[\"dpd\"] = pd.to_numeric(features.get(\"dpd\", features.get(\"days_past_due\", 0)), errors=\"coerce\").fillna(0).astype(int)\n",
    "        balance_clip = features[\"balance\"].clip(lower=1)\n",
    "        credit_limit = features.get(\"credit_limit\")\n",
    "        if credit_limit is None:\n",
    "            credit_limit_series = balance_clip\n",
    "        elif isinstance(credit_limit, pd.Series):\n",
    "            credit_limit_series = pd.to_numeric(credit_limit, errors=\"coerce\").where(lambda s: s.notna() & (s != 0), balance_clip)\n",
    "        else:\n",
    "            credit_limit_series = pd.Series(credit_limit, index=features.index)\n",
    "            credit_limit_series = pd.to_numeric(credit_limit_series, errors=\"coerce\").where(lambda s: s.notna() & (s != 0), balance_clip)\n",
    "        features[\"utilization_ratio\"] = (features[\"balance\"] / credit_limit_series).replace([np.inf, -np.inf], np.nan)\n",
    "        features[\"utilization_ratio\"] = features[\"utilization_ratio\"].clip(upper=5).fillna(0)\n",
    "        apr_source = features.get(\"apr\") if \"apr\" in features.columns else features.get(\"nominal_rate\", np.nan)\n",
    "        if not isinstance(apr_source, pd.Series):\n",
    "            apr_source = pd.Series(apr_source, index=features.index)\n",
    "        apr_series = pd.to_numeric(apr_source, errors=\"coerce\")\n",
    "        features[\"apr\"] = apr_series.fillna(apr_series.median(skipna=True)).fillna(0)\n",
    "        features[\"weighted_apr\"] = features.groupby(\"customer_id\")[\"balance\"].transform(lambda x: x / x.sum()).fillna(0) * features[\"apr\"]\n",
    "        features[\"balance_zscore\"] = (features[\"balance\"] - features[\"balance\"].mean()) / features[\"balance\"].std(ddof=0)\n",
    "        features[\"balance_zscore\"] = features[\"balance_zscore\"].fillna(0).clip(-3, 3)\n",
    "        features[\"industry\"] = features.get(\"industry\", pd.Series(\"unspecified\", index=features.index)).fillna(\"unspecified\")\n",
    "        features[\"kam_owner\"] = features.get(\"kam_owner\", pd.Series(\"unassigned\", index=features.index)).fillna(\"unassigned\")\n",
    "        features[\"b2g_flag\"] = features[\"industry\"].str.lower().str.contains(\"government|public\").fillna(False).astype(int)\n",
    "        features[\"days_since_origination\"] = (self.reference_date - pd.to_datetime(features.get(\"origination_date\", features[\"date\"]), utc=True)).dt.days\n",
    "        features[\"days_since_origination\"] = features[\"days_since_origination\"].clip(lower=0).fillna(0).astype(int)\n",
    "        features[\"roll_rate_key\"] = features[\"customer_id\"].astype(str) + \"_\" + features[\"product_code\"].astype(str)\n",
    "        features = features.sort_values([\"roll_rate_key\", \"date\"])\n",
    "        features[\"prev_dpd\"] = features.groupby(\"roll_rate_key\")[\"dpd\"].shift(1).fillna(0)\n",
    "        features[\"roll_rate_delta\"] = features[\"dpd\"] - features[\"prev_dpd\"]\n",
    "        features[\"roll_rate_direction\"] = np.select([features[\"roll_rate_delta\"] > 0, features[\"roll_rate_delta\"] < 0], [\"deteriorating\", \"improving\"], default=\"stable\")\n",
    "        features[\"alert_usury_micro\"] = ((features[\"customer_type\"] == \"micro\") & (features[\"apr\"] > 0.85)).astype(int)\n",
    "        features[\"alert_high_utilization\"] = (features[\"utilization_ratio\"] > 0.95).astype(int)\n",
    "        features[\"alert_high_dpd\"] = (features[\"dpd\"] >= 90).astype(int)\n",
    "        features[\"alert_pdf_gap\"] = 0\n",
    "        alerts_records: List[Dict[str, Any]] = []\n",
    "        alert_columns = {\"alert_usury_micro\": \"critical\", \"alert_high_utilization\": \"high\", \"alert_high_dpd\": \"critical\", \"alert_pdf_gap\": \"medium\"}\n",
    "        for alert_col, severity in alert_columns.items():\n",
    "            flagged = features[features[alert_col] == 1]\n",
    "            for _, row in flagged.iterrows():\n",
    "                alerts_records.append(dict(customer_id=row.get(\"customer_id\"), rule=alert_col, severity=severity, details=f\"DPD={row.get('dpd')}|Util={row.get('utilization_ratio'):.2f}\"))\n",
    "        alerts = pd.DataFrame(alerts_records) if alerts_records else pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "        return FeatureArtifacts(features=features.reset_index(drop=True), alerts=alerts)\n",
    "\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_artifacts = feature_engineer.transform(master_frame)\n",
    "feature_frame = feature_artifacts.features\n",
    "alerts_frame = feature_artifacts.alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c13dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20241/2476463806.py:13: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  self.frame[\"month\"] = self.frame[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'source_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     95\u001b[39m kpi_engine = KPIEngine(feature_frame)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m kpi_summary = \u001b[43mkpi_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mKPIEngine.compute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     59\u001b[39m current_frame[\u001b[33m\"\u001b[39m\u001b[33mltv_cac_ratio\u001b[39m\u001b[33m\"\u001b[39m] = np.where(cac.fillna(\u001b[32m0\u001b[39m) == \u001b[32m0\u001b[39m, np.nan, ltv / cac)\n\u001b[32m     61\u001b[39m channel_col = \u001b[33m\"\u001b[39m\u001b[33mchannel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mchannel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m current_frame.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msource_name\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33mltv_cac_by_segment\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mcurrent_frame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msegment_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.ltv_cac_ratio.mean()\n\u001b[32m     64\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33mnrr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._ratio(\n\u001b[32m     65\u001b[39m     current_frame.get(\u001b[33m\"\u001b[39m\u001b[33mrecurring_revenue\u001b[39m\u001b[33m\"\u001b[39m, pd.Series(\u001b[32m0\u001b[39m, index=current_frame.index)),\n\u001b[32m     66\u001b[39m     current_frame.get(\u001b[33m\"\u001b[39m\u001b[33mstarting_revenue\u001b[39m\u001b[33m\"\u001b[39m, pd.Series(\u001b[32m1\u001b[39m, index=current_frame.index)),\n\u001b[32m     67\u001b[39m )\n\u001b[32m     69\u001b[39m result[\u001b[33m\"\u001b[39m\u001b[33mnsm\u001b[39m\u001b[33m\"\u001b[39m] = current_frame.get(\u001b[33m\"\u001b[39m\u001b[33mnorth_star_metric\u001b[39m\u001b[33m\"\u001b[39m, pd.Series(\u001b[32m0\u001b[39m, index=current_frame.index)).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:9190\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9196\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1330\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1341\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'source_name'"
     ]
    }
   ],
   "source": [
    "# KPI Calculation Engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "alerts_frame = globals().get(\"alerts_frame\", pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "\n",
    "class KPIEngine:\n",
    "    def __init__(self, frame: pd.DataFrame) -> None:\n",
    "        self.frame = frame.copy()\n",
    "        if not self.frame.empty:\n",
    "            self.frame[\"date\"] = pd.to_datetime(self.frame[\"date\"], utc=True)\n",
    "            self.frame[\"month\"] = self.frame[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    def _ratio(self, numerator: pd.Series, denominator: pd.Series) -> float:\n",
    "        denom = denominator.sum()\n",
    "        if denom == 0:\n",
    "            return float(\"nan\")\n",
    "        return numerator.sum() / denom\n",
    "\n",
    "    def compute(self) -> Dict[str, Any]:\n",
    "        if self.frame.empty:\n",
    "            return {}\n",
    "\n",
    "        result: Dict[str, Any] = {}\n",
    "        current_frame = self.frame.copy()\n",
    "\n",
    "        result[\"aum\"] = current_frame[\"balance\"].sum()\n",
    "        result[\"active_clients\"] = current_frame[\"customer_id\"].nunique()\n",
    "        result[\"credit_lines\"] = current_frame.get(\"credit_limit\", pd.Series(0, index=current_frame.index)).sum()\n",
    "\n",
    "        churn_mask = current_frame.get(\"status\", pd.Series(\"active\", index=current_frame.index)).str.lower().eq(\"churned\")\n",
    "        result[\"churn_rate\"] = churn_mask.mean()\n",
    "\n",
    "        default_mask = current_frame.get(\"default_flag\", pd.Series(0, index=current_frame.index)).astype(int)\n",
    "        result[\"default_rate\"] = default_mask.mean()\n",
    "\n",
    "        dpd_group = current_frame.groupby(\"delinquency_bucket\")[\"balance\"].sum().rename(\"aum\")\n",
    "        result[\"dpd_buckets\"] = dpd_group\n",
    "\n",
    "        result[\"rotation\"] = self._ratio(\n",
    "            current_frame.get(\"payments\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"balance\", pd.Series(0, index=current_frame.index)),\n",
    "        )\n",
    "\n",
    "        result[\"weighted_apr\"] = current_frame[\"weighted_apr\"].mean()\n",
    "\n",
    "        result[\"revenue\"] = current_frame.get(\"interest_income\", pd.Series(0, index=current_frame.index)).sum()\n",
    "        result[\"ebitda\"] = current_frame.get(\"ebitda\", pd.Series(0, index=current_frame.index)).sum()\n",
    "\n",
    "        result[\"concentration_top10\"] = (\n",
    "            current_frame.groupby(\"customer_id\")[\"balance\"].sum().nlargest(10).sum() / result[\"aum\"]\n",
    "            if result[\"aum\"]\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "\n",
    "        ltv = current_frame.get(\"ltv\", pd.Series(0, index=current_frame.index))\n",
    "        cac = current_frame.get(\"cac\", pd.Series(np.nan, index=current_frame.index))\n",
    "        current_frame[\"ltv_cac_ratio\"] = np.where(cac.fillna(0) == 0, np.nan, ltv / cac)\n",
    "\n",
    "        channel_col = \"channel\" if \"channel\" in current_frame.columns else \"source_name\"\n",
    "        result[\"ltv_cac_by_segment\"] = current_frame.groupby([\"segment_code\", channel_col]).ltv_cac_ratio.mean()\n",
    "\n",
    "        result[\"nrr\"] = self._ratio(\n",
    "            current_frame.get(\"recurring_revenue\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"starting_revenue\", pd.Series(1, index=current_frame.index)),\n",
    "        )\n",
    "\n",
    "        result[\"nsm\"] = current_frame.get(\"north_star_metric\", pd.Series(0, index=current_frame.index)).mean()\n",
    "\n",
    "        result[\"penetration\"] = self._ratio(\n",
    "            current_frame.get(\"active_products\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"available_products\", pd.Series(1, index=current_frame.index)),\n",
    "        )\n",
    "\n",
    "        result[\"b2g_percent\"] = current_frame[\"b2g_flag\"].mean()\n",
    "\n",
    "        status_column = current_frame.get(\"status\", pd.Series(\"active\", index=current_frame.index)).str.lower()\n",
    "        result[\"new_recurrent_recovered\"] = status_column.value_counts(dropna=False)\n",
    "\n",
    "        group_cols = [\"industry\", \"kam_owner\", \"segment_code\", \"customer_type\"]\n",
    "        aggregation = current_frame.groupby(group_cols)[\"balance\"].sum().rename(\"aum\")\n",
    "        result[\"aum_by_group\"] = aggregation\n",
    "\n",
    "        behavior_mask = (current_frame[\"customer_type\"] == \"micro\") & (current_frame[\"apr\"] > 0.85)\n",
    "        result[\"usury_micro_share\"] = behavior_mask.mean()\n",
    "\n",
    "        result[\"pod\"] = current_frame.get(\"probability_of_default\", pd.Series(np.nan, index=current_frame.index)).mean()\n",
    "\n",
    "        if not alerts_frame.empty:\n",
    "            result[\"alerts_active\"] = alerts_frame.groupby(\"severity\").size()\n",
    "\n",
    "        return result\n",
    "\n",
    "kpi_engine = KPIEngine(feature_frame)\n",
    "kpi_summary = kpi_engine.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24753c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Growth Analysis & Projections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "kpi_summary = globals().get(\"kpi_summary\", {})\n",
    "\n",
    "DEFAULT_TARGETS = dict(\n",
    "    aum=1_200_000_000,\n",
    "    revenue=95_000_000,\n",
    "    ebitda=42_000_000,\n",
    "    b2g_percent=0.35,\n",
    "    churn_rate=0.03,\n",
    ")\n",
    "\n",
    "def build_monthly_path(frame: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    if frame.empty or metric not in frame.columns:\n",
    "        return pd.DataFrame()\n",
    "    monthly = frame.groupby(\"month\")[metric].sum().reset_index()\n",
    "    monthly[\"monthly_growth\"] = monthly[metric].pct_change().fillna(0)\n",
    "    monthly[\"ramp_type\"] = np.select(\n",
    "        [monthly[\"monthly_growth\"] > 0.1, monthly[\"monthly_growth\"] < -0.05],\n",
    "        [\"ramp\", \"trough\"],\n",
    "        default=\"steady\",\n",
    "    )\n",
    "    return monthly\n",
    "\n",
    "def growth_gap_analysis(kpis: Dict[str, Any], targets: Dict[str, float]) -> pd.DataFrame:\n",
    "    if not kpis:\n",
    "        return pd.DataFrame()\n",
    "    records = []\n",
    "    for metric, target in targets.items():\n",
    "        actual = kpis.get(metric, np.nan)\n",
    "        gap = target - actual if not pd.isna(actual) else np.nan\n",
    "        progress = actual / target if target else np.nan\n",
    "        records.append(dict(metric=metric, actual=actual, target=target, gap=gap, progress=progress))\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"status\"] = np.select(\n",
    "        [df[\"progress\"] >= 1, df[\"progress\"] >= 0.8],\n",
    "        [\"achieved\", \"on_track\"],\n",
    "        default=\"gap\",\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def acquisition_vs_churn(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame.empty:\n",
    "        return pd.DataFrame()\n",
    "    status_series = frame.get(\"status\", pd.Series(\"active\", index=frame.index)).str.lower()\n",
    "    summary = status_series.value_counts().rename_axis(\"status\").reset_index(name=\"count\")\n",
    "    summary[\"group\"] = np.where(\n",
    "        summary[\"status\"].str.contains(\"new\"),\n",
    "        \"acquisition\",\n",
    "        np.where(summary[\"status\"].str.contains(\"churn\"), \"churn\", \"retained\"),\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "growth_targets = DEFAULT_TARGETS\n",
    "growth_gap_table = growth_gap_analysis(kpi_summary, growth_targets)\n",
    "growth_path_balance = build_monthly_path(feature_frame, \"balance\") if not feature_frame.empty else pd.DataFrame()\n",
    "acquisition_table = acquisition_vs_churn(feature_frame)\n",
    "\n",
    "implication_multiplier = 8.9\n",
    "if not growth_gap_table.empty:\n",
    "    growth_gap_table[\"implied_scale\"] = growth_gap_table[\"target\"] * implication_multiplier\n",
    "\n",
    "b2g_gap = (\n",
    "    growth_targets.get(\"b2g_percent\", np.nan) - kpi_summary.get(\"b2g_percent\", np.nan)\n",
    ") if kpi_summary else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing & Sales Analysis\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "\n",
    "def marketing_sales_breakdown(frame: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    if frame.empty:\n",
    "        return {}\n",
    "\n",
    "    aggregations: Dict[str, pd.DataFrame] = {}\n",
    "    group_fields = {\n",
    "        \"industry\": [\"industry\"],\n",
    "        \"kam\": [\"kam_owner\"],\n",
    "        \"channel\": [\"channel\"] if \"channel\" in frame.columns else [\"source_name\"],\n",
    "    }\n",
    "\n",
    "    for label, fields in group_fields.items():\n",
    "        grouped = frame.groupby(fields).agg(\n",
    "            aum=(\"balance\", \"sum\"),\n",
    "            clients=(\"customer_id\", \"nunique\"),\n",
    "            weighted_apr=(\"weighted_apr\", \"mean\"),\n",
    "            ltv_cac=(\"ltv_cac_ratio\", \"mean\"),\n",
    "        ).reset_index()\n",
    "        aggregations[label] = grouped\n",
    "\n",
    "    return aggregations\n",
    "\n",
    "marketing_sales_tables = marketing_sales_breakdown(feature_frame)\n",
    "treemap_ready = marketing_sales_tables.get(\"industry\") if marketing_sales_tables else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Analysis & Roll Rate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "\n",
    "def delinquency_summary(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame.empty:\n",
    "        return pd.DataFrame()\n",
    "    summary = frame.groupby([\"delinquency_bucket\", \"industry\", \"segment_code\"]).agg(\n",
    "        balance=(\"balance\", \"sum\"),\n",
    "        clients=(\"customer_id\", \"nunique\"),\n",
    "        pod_mean=(\"probability_of_default\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    total_balance = summary[\"balance\"].sum()\n",
    "    summary[\"delinquency_percent\"] = summary[\"balance\"] / total_balance if total_balance else np.nan\n",
    "    return summary\n",
    "\n",
    "def roll_rate_matrix(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame.empty:\n",
    "        return pd.DataFrame()\n",
    "    roll_data = frame.copy()\n",
    "    roll_data[\"next_bucket\"] = roll_data.groupby(\"roll_rate_key\")[\"delinquency_bucket\"].shift(-1)\n",
    "    matrix = pd.crosstab(\n",
    "        roll_data[\"delinquency_bucket\"],\n",
    "        roll_data[\"next_bucket\"],\n",
    "        normalize=\"index\",\n",
    "    ).fillna(0)\n",
    "    return matrix\n",
    "\n",
    "def npl_metrics(frame: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if frame.empty:\n",
    "        return {}\n",
    "    balance_total = frame[\"balance\"].sum()\n",
    "    npl = frame[frame[\"dpd\"] >= 180]\n",
    "    npl_balance = npl[\"balance\"].sum()\n",
    "    result = dict(\n",
    "        npl180_balance=npl_balance,\n",
    "        npl_ratio=(npl_balance / balance_total) if balance_total else np.nan,\n",
    "    )\n",
    "    if \"ltv\" in frame.columns:\n",
    "        ltv_45 = frame[frame[\"dpd\"] > 45][\"ltv\"].mean()\n",
    "        ltv_90 = frame[frame[\"dpd\"] > 90][\"ltv\"].mean()\n",
    "        result[\"ltv_delta_45_90\"] = ltv_45 - ltv_90 if not pd.isna(ltv_45) and not pd.isna(ltv_90) else np.nan\n",
    "    else:\n",
    "        result[\"ltv_delta_45_90\"] = np.nan\n",
    "    result[\"rotation_multiple\"] = (\n",
    "        frame.get(\"payments\", pd.Series(0, index=frame.index)).sum() / balance_total\n",
    "        if balance_total\n",
    "        else np.nan\n",
    "    )\n",
    "    return result\n",
    "\n",
    "risk_table = delinquency_summary(feature_frame)\n",
    "roll_matrix = roll_rate_matrix(feature_frame)\n",
    "risk_metrics = npl_metrics(feature_frame)\n",
    "equifax_vs_dpd = (\n",
    "    feature_frame[[\"equifax_score\", \"dpd\"]].dropna()\n",
    "    if \"equifax_score\" in feature_frame.columns\n",
    "    else pd.DataFrame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b32686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Audit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "from typing import Any, Dict, List, Optional, cast\n",
    "try:\n",
    "    import pdfplumber as _pdfplumber\n",
    "except ImportError:\n",
    "    _pdfplumber = None\n",
    "pdfplumber = cast(Optional[Any], _pdfplumber)\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "\n",
    "CRITICAL_COLUMNS = {\"customer_id\", \"date\", \"balance\", \"dpd\"}\n",
    "\n",
    "def data_quality_audit(frame: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if frame.empty:\n",
    "        return {\"score\": np.nan, \"table\": pd.DataFrame(), \"styled\": None, \"pdf_completeness\": 0.0}\n",
    "\n",
    "    total_rows = len(frame)\n",
    "    audit_records: List[Dict[str, Any]] = []\n",
    "    penalties = 0.0\n",
    "\n",
    "    for column in frame.columns:\n",
    "        nulls = frame[column].isna().sum()\n",
    "        zeros = (frame[column] == 0).sum() if pd.api.types.is_numeric_dtype(frame[column]) else np.nan\n",
    "        coverage = 1 - (nulls / total_rows) if total_rows else np.nan\n",
    "        if column in CRITICAL_COLUMNS and coverage < 0.9:\n",
    "            penalties += 0.1\n",
    "        audit_records.append(\n",
    "            dict(column=column, nulls=int(nulls), zeros=int(zeros) if not pd.isna(zeros) else np.nan, coverage=coverage)\n",
    "        )\n",
    "\n",
    "    audit_table = pd.DataFrame(audit_records)\n",
    "    coverage_mean = audit_table[\"coverage\"].mean()\n",
    "    quality_score = max(0.0, min(1.0, (coverage_mean if not pd.isna(coverage_mean) else 0.0) - penalties))\n",
    "\n",
    "    def _color(value: float) -> str:\n",
    "        if pd.isna(value):\n",
    "            return \"color: #E6E6EF; background-color: #3730A3\"\n",
    "        if value >= 0.95:\n",
    "            return \"color: #05101a; background-color: #22E7CC\"\n",
    "        if value >= 0.85:\n",
    "            return \"color: #F5F3FF; background-color: #2563EB\"\n",
    "        return \"color: #F5F3FF; background-color: #B91C1C\"\n",
    "\n",
    "    styler = audit_table.style.format({\"coverage\": \"{:.2%}\"})\n",
    "    apply_map = getattr(styler, \"applymap\", None)\n",
    "    styled = apply_map(_color, subset=[\"coverage\"]) if callable(apply_map) else styler\n",
    "\n",
    "    pdf_completeness = 1.0 if pdfplumber else 0.0\n",
    "\n",
    "    return dict(score=quality_score, table=audit_table, styled=styled, pdf_completeness=pdf_completeness)\n",
    "quality_artifacts = data_quality_audit(feature_frame)\n",
    "quality_score = quality_artifacts.get(\"score\")\n",
    "quality_table = quality_artifacts.get(\"table\")\n",
    "quality_styled = quality_artifacts.get(\"styled\")\n",
    "\n",
    "print(f\"Data Quality Score: {quality_score:.2%}\" if not pd.isna(quality_score) else \"Data Quality Score: N/A\")\n",
    "print(f\"PDF Completeness: {quality_artifacts.get('pdf_completeness', 0.0):.1%}\")\n",
    "\n",
    "if quality_styled is not None:\n",
    "    display(quality_styled)\n",
    "else:\n",
    "    print(\"No data quality table to display - feature_frame is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ee6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Summary & Insights\n",
    "%pip install google-generativeai\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import textwrap\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(\"abaco_ingestion\")\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(logging.NullHandler())\n",
    "\n",
    "try:\n",
    "    from google.generativeai.generative_models import GenerativeModel\n",
    "    from google.generativeai.client import configure\n",
    "except ImportError:\n",
    "    GenerativeModel = None\n",
    "    configure = None\n",
    "\n",
    "kpi_summary = globals().get(\"kpi_summary\", {})\n",
    "risk_metrics = globals().get(\"risk_metrics\", {})\n",
    "growth_gap_table = globals().get(\"growth_gap_table\", pd.DataFrame())\n",
    "alerts_frame = globals().get(\"alerts_frame\", pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "\n",
    "def generate_ai_summary(\n",
    "    kpis: Dict[str, Any],\n",
    "    risk: Dict[str, Any],\n",
    "    growth: pd.DataFrame,\n",
    "    alerts: pd.DataFrame,\n",
    "    market: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    prompt_lines: List[str] = []\n",
    "    prompt_lines.append(\"ABACO Financial Intelligence Executive Summary\")\n",
    "\n",
    "    if kpis:\n",
    "        prompt_lines.append(f\"AUM: {kpis.get('aum', np.nan):,.0f}\")\n",
    "        prompt_lines.append(f\"Default Rate: {kpis.get('default_rate', np.nan):.2%}\")\n",
    "        prompt_lines.append(f\"B2G Share: {kpis.get('b2g_percent', np.nan):.2%}\")\n",
    "\n",
    "    if risk:\n",
    "        prompt_lines.append(f\"NPL 180+: {risk.get('npl180_balance', np.nan):,.0f}\")\n",
    "\n",
    "    if market:\n",
    "        prompt_lines.append(f\"MYPE GDP Share: {market.get('gdp_share', '48.8%')}\")\n",
    "\n",
    "    prompt_lines.append(f\"Alerts Count: {len(alerts) if alerts is not None else 0}\")\n",
    "    prompt = \"\\n\".join(prompt_lines)\n",
    "\n",
    "    summary_text: Optional[str] = None\n",
    "\n",
    "    if gemini_key and GenerativeModel is not None and configure is not None:\n",
    "        try:\n",
    "            configure(api_key=gemini_key)\n",
    "            model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "            response = model.generate_content(prompt)\n",
    "            summary_text = getattr(response, \"text\", None)\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Gemini summary failed: %s\", exc)\n",
    "\n",
    "    if not summary_text:\n",
    "        themes: List[str] = []\n",
    "        if kpis:\n",
    "            themes.append(\n",
    "                f\"Assets under management currently {kpis.get('aum', np.nan):,.0f} with weighted APR {kpis.get('weighted_apr', np.nan):.2%}.\"\n",
    "            )\n",
    "            themes.append(\n",
    "                f\"Default rate holding at {kpis.get('default_rate', np.nan):.2%} while churn {kpis.get('churn_rate', np.nan):.2%}.\"\n",
    "            )\n",
    "        if risk:\n",
    "            themes.append(\n",
    "                f\"NPL ratio stands at {risk.get('npl_ratio', np.nan):.2%} with rotation multiple {risk.get('rotation_multiple', np.nan):.2f}.\"\n",
    "            )\n",
    "        if growth is not None and not growth.empty:\n",
    "            gap_row = growth.sort_values(\"gap\", ascending=False).head(1).to_dict(orient=\"records\")\n",
    "            if gap_row:\n",
    "                themes.append(\n",
    "                    f\"Largest growth delta resides in {gap_row[0]['metric']} with gap {gap_row[0]['gap']:,.0f}.\"\n",
    "                )\n",
    "        if market:\n",
    "            themes.append(\n",
    "                f\"Market TAM updated to {market.get('tam_update', '31,666')} with penetration insights incorporated.\"\n",
    "            )\n",
    "        summary_text = \" \".join(textwrap.fill(theme, width=120) for theme in themes)\n",
    "\n",
    "    return dict(prompt=prompt, summary=summary_text)\n",
    "\n",
    "ai_summary = generate_ai_summary(kpi_summary, risk_metrics, growth_gap_table, alerts_frame, market=None)\n",
    "summary_text = ai_summary.get(\"summary\")\n",
    "prompt_text = ai_summary.get(\"prompt\")\n",
    "\n",
    "print(\"AI Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(summary_text)\n",
    "print(\"\\nPrompt Used:\")\n",
    "print(\"-\" * 30)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations & Exports\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "growth_path_balance = globals().get(\"growth_path_balance\", pd.DataFrame())\n",
    "roll_matrix = globals().get(\"roll_matrix\", pd.DataFrame())\n",
    "treemap_ready = globals().get(\"treemap_ready\", pd.DataFrame())\n",
    "\n",
    "EXPORT_DIR = pathlib.Path(\"artifacts/exports\")\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_figures(\n",
    "    frame: pd.DataFrame,\n",
    "    growth: pd.DataFrame,\n",
    "    risk: pd.DataFrame,\n",
    "    treemap_source: pd.DataFrame,\n",
    ") -> Dict[str, go.Figure]:\n",
    "    figures: Dict[str, go.Figure] = {}\n",
    "\n",
    "    if not frame.empty:\n",
    "        line_df = frame.groupby(\"date\")[\"balance\"].sum().reset_index()\n",
    "        figures[\"growth_line\"] = px.line(line_df, x=\"date\", y=\"balance\", title=\"Growth Trajectory (4K Ready)\")\n",
    "\n",
    "        churn_df = frame.groupby(\"delinquency_bucket\")[\"customer_id\"].nunique().reset_index(name=\"clients\")\n",
    "        figures[\"churn_bar\"] = px.bar(churn_df, x=\"delinquency_bucket\", y=\"clients\", title=\"Churn & Delinquency Bar\")\n",
    "\n",
    "        pie_df = frame.groupby(\"segment_code\")[\"balance\"].sum().reset_index()\n",
    "        figures[\"segment_pie\"] = px.pie(pie_df, names=\"segment_code\", values=\"balance\", title=\"Segment Penetration Pie\")\n",
    "\n",
    "        if \"equifax_score\" in frame.columns:\n",
    "            figures[\"risk_scatter\"] = px.scatter(\n",
    "                frame,\n",
    "                x=\"equifax_score\",\n",
    "                y=\"dpd\",\n",
    "                color=\"delinquency_bucket\",\n",
    "                title=\"Equifax vs DPD Scatter\",\n",
    "            )\n",
    "\n",
    "    if not treemap_source.empty:\n",
    "        figures[\"industry_treemap\"] = px.treemap(\n",
    "            treemap_source,\n",
    "            path=[treemap_source.columns[0]],\n",
    "            values=\"aum\",\n",
    "            color=\"weighted_apr\",\n",
    "            title=\"Industry TPV Treemap\",\n",
    "        )\n",
    "\n",
    "    if isinstance(risk, pd.DataFrame) and not risk.empty:\n",
    "        figures[\"roll_rate_heatmap\"] = go.Figure(\n",
    "            data=go.Heatmap(z=risk.values, x=risk.columns, y=risk.index, colorscale=\"Viridis\")\n",
    "        )\n",
    "        figures[\"roll_rate_heatmap\"].update_layout(title=\"Roll Rate Transition Matrix\")\n",
    "\n",
    "    return figures\n",
    "\n",
    "def export_figures(figures: Dict[str, go.Figure], directory: pathlib.Path, scale: int = 1) -> None:\n",
    "    for name, fig in figures.items():\n",
    "        fig.update_layout(template=\"abaco_dark_ultra\", width=3840, height=2160)\n",
    "        export_path = directory / f\"{name}.png\"\n",
    "        try:\n",
    "            fig.write_image(export_path, width=3840, height=2160, scale=scale)\n",
    "        except ValueError:\n",
    "            print(f\"Static export skipped for {name} (kaleido missing)\")\n",
    "\n",
    "def export_fact_table(frame: pd.DataFrame, directory: pathlib.Path) -> pathlib.Path:\n",
    "    if frame.empty:\n",
    "        raise ValueError(\"Feature frame is empty; nothing to export\")\n",
    "    csv_path = directory / \"abaco_fact_table.csv\"\n",
    "    frame.to_csv(csv_path, index=False)\n",
    "    return csv_path\n",
    "\n",
    "figures = build_figures(feature_frame, growth_path_balance, roll_matrix, treemap_ready)\n",
    "export_figures(figures, EXPORT_DIR)\n",
    "\n",
    "fact_table_path: Optional[pathlib.Path] = None\n",
    "if not feature_frame.empty:\n",
    "    fact_table_path = export_fact_table(feature_frame, EXPORT_DIR)\n",
    "    print(f\"Fact table exported to {fact_table_path}\")\n",
    "\n",
    "html_path = EXPORT_DIR / \"abaco_fact_table.html\"\n",
    "if not feature_frame.empty:\n",
    "    feature_frame.sample(min(1000, len(feature_frame))).to_html(html_path, index=False)\n",
    "    print(f\"HTML table exported to {html_path}\")\n",
    "\n",
    "looker_placeholder = EXPORT_DIR / \"looker_ready_manifest.json\"\n",
    "with open(looker_placeholder, \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(\n",
    "        dict(\n",
    "            dataset=\"abaco_financial_intelligence\",\n",
    "            exported=str(fact_table_path) if fact_table_path else \"pending\",\n",
    "        ),\n",
    "        handle,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(f\"Looker placeholder created at {looker_placeholder}\")\n",
    "print(\"Slack and HubSpot export hooks pending integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dacbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Analysis from MYPE 2025 PDF\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List, Optional, cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pdfplumber = cast(Optional[Any], globals().get(\"pdfplumber\"))\n",
    "generate_ai_summary = globals().get(\"generate_ai_summary\", lambda *args, **kwargs: {\"prompt\": \"\", \"summary\": None})\n",
    "kpi_summary = globals().get(\"kpi_summary\", {})\n",
    "risk_metrics = globals().get(\"risk_metrics\", {})\n",
    "growth_gap_table = globals().get(\"growth_gap_table\", pd.DataFrame())\n",
    "alerts_frame = globals().get(\"alerts_frame\", pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "\n",
    "MYPE_PDF_PATH = pathlib.Path(\"data/mype_report_2025.pdf\")\n",
    "\n",
    "def extract_market_insights(pdf_path: pathlib.Path) -> Dict[str, Any]:\n",
    "    stats: Dict[str, Any] = dict(\n",
    "        gdp_share=\"48.8%\",\n",
    "        tam_update=\"31,666\",\n",
    "        challenges=[\"Limited formal credit access\", \"Fragmented guarantee schemes\"],\n",
    "        opportunities=[\"Digital onboarding expansion\", \"Supply chain financing partnerships\"],\n",
    "        behavior_links=\"Micro-segment displays high cash-cycle volatility\",\n",
    "    )\n",
    "\n",
    "    if pdfplumber is None:\n",
    "        stats[\"source\"] = \"fallback\"\n",
    "        return stats\n",
    "    insights_text: List[str] = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages, start=1):\n",
    "            if page_number in {1, 2, 35}:\n",
    "                text = page.extract_text() or \"\"\n",
    "                insights_text.append(text)\n",
    "\n",
    "    combined = \" \\n\".join(insights_text)\n",
    "    if combined:\n",
    "        stats[\"source\"] = \"pdf\"\n",
    "        stats[\"raw_text_excerpt\"] = combined[:2000]\n",
    "    else:\n",
    "        stats[\"source\"] = \"fallback\"\n",
    "\n",
    "    return stats\n",
    "\n",
    "market_insights = extract_market_insights(MYPE_PDF_PATH)\n",
    "if market_insights:\n",
    "    print(json.dumps(market_insights, indent=2))\n",
    "\n",
    "ai_summary = generate_ai_summary(kpi_summary, risk_metrics, growth_gap_table, alerts_frame, market=market_insights)\n",
    "print(\"Updated AI summary with market context:\", ai_summary.get(\"summary\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
