{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55d8d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Feature Engineering\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NamedTuple\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m DELINQUENCY_BUCKETS = [-\u001b[43mnp\u001b[49m.inf, \u001b[32m0\u001b[39m, \u001b[32m30\u001b[39m, \u001b[32m60\u001b[39m, \u001b[32m90\u001b[39m, \u001b[32m120\u001b[39m, np.inf]\n\u001b[32m      5\u001b[39m DELINQUENCY_LABELS = [\u001b[33m\"\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1_30\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m31_60\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m61_90\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m91_120\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m120_plus\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m SEGMENT_LABELS = \u001b[38;5;28mlist\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mABCDEF\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "\n",
    "DELINQUENCY_BUCKETS = [-np.inf, 0, 30, 60, 90, 120, np.inf]\n",
    "DELINQUENCY_LABELS = [\"current\", \"1_30\", \"31_60\", \"61_90\", \"91_120\", \"120_plus\"]\n",
    "SEGMENT_LABELS = list(\"ABCDEF\")\n",
    "\n",
    "class FeatureArtifacts(NamedTuple):\n",
    "    features: pd.DataFrame\n",
    "    alerts: pd.DataFrame\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, reference_date: Optional[pd.Timestamp] = None) -> None:\n",
    "        self.reference_date = reference_date or pd.Timestamp.utcnow().normalize()\n",
    "\n",
    "    def _derive_customer_type(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        if \"customer_type\" in frame.columns:\n",
    "            return frame[\"customer_type\"].fillna(\"unspecified\")\n",
    "        balance = frame.get(\"balance\", pd.Series(np.nan, index=frame.index)).fillna(0)\n",
    "        exposure = frame.get(\"credit_limit\", pd.Series(np.nan, index=frame.index)).fillna(balance.clip(lower=1))\n",
    "        ratio = balance / exposure.replace({0: np.nan})\n",
    "        derived = np.where(balance >= 5_000_000, \"enterprise\", np.where(balance >= 500_000, \"corporate\", np.where(balance >= 50_000, \"sme\", \"micro\")))\n",
    "        derived = pd.Series(derived, index=frame.index)\n",
    "        derived = np.where(ratio >= 0.9, \"intensive\", derived)\n",
    "        return pd.Series(derived, index=frame.index)\n",
    "\n",
    "    def _segmentation(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        try:\n",
    "            return pd.qcut(frame[\"balance\"], q=min(6, frame[\"balance\"].nunique()), labels=SEGMENT_LABELS[: min(6, frame[\"balance\"].nunique())], duplicates=\"drop\").astype(str)\n",
    "        except Exception:\n",
    "            return pd.Series([\"A\"] * len(frame), index=frame.index)\n",
    "\n",
    "    def _delinquency_bucket(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        dpd = frame.get(\"dpd\") or frame.get(\"days_past_due\")\n",
    "        if dpd is None:\n",
    "            dpd = pd.Series(0, index=frame.index)\n",
    "        dpd = pd.to_numeric(dpd, errors=\"coerce\").fillna(0)\n",
    "        return pd.cut(dpd, bins=DELINQUENCY_BUCKETS, labels=DELINQUENCY_LABELS, right=True)\n",
    "\n",
    "    def transform(self, frame: pd.DataFrame) -> FeatureArtifacts:\n",
    "        if frame.empty:\n",
    "            return FeatureArtifacts(frame.copy(), pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "        features = frame.copy()\n",
    "        features[\"date\"] = pd.to_datetime(features[\"date\"], errors=\"coerce\", utc=True)\n",
    "        features[\"customer_type\"] = self._derive_customer_type(features)\n",
    "        features[\"segment_code\"] = self._segmentation(features)\n",
    "        features[\"delinquency_bucket\"] = self._delinquency_bucket(features).astype(str)\n",
    "        features[\"dpd\"] = pd.to_numeric(features.get(\"dpd\", features.get(\"days_past_due\", 0)), errors=\"coerce\").fillna(0).astype(int)\n",
    "        features[\"dpd_30_flag\"] = (features[\"dpd\"] >= 30).astype(int)\n",
    "        features[\"dpd_60_flag\"] = (features[\"dpd\"] >= 60).astype(int)\n",
    "        features[\"dpd_90_flag\"] = (features[\"dpd\"] >= 90).astype(int)\n",
    "        features[\"utilization_ratio\"] = features[\"balance\"] / features.get(\"credit_limit\", features[\"balance\"].clip(lower=1))\n",
    "        features[\"utilization_ratio\"] = features[\"utilization_ratio\"].clip(upper=5).fillna(0)\n",
    "        features[\"apr\"] = pd.to_numeric(features.get(\"apr\", features.get(\"nominal_rate\", np.nan)), errors=\"coerce\")\n",
    "        features[\"apr\"] = features[\"apr\"].fillna(features[\"apr\"].median(skipna=True))\n",
    "        features[\"weighted_apr\"] = features.groupby(\"customer_id\")[\"balance\"].transform(lambda x: x / x.sum()).fillna(0) * features[\"apr\"]\n",
    "        features[\"balance_zscore\"] = (features[\"balance\"] - features[\"balance\"].mean()) / features[\"balance\"].std(ddof=0)\n",
    "        features[\"balance_zscore\"] = features[\"balance_zscore\"].fillna(0).clip(-3, 3)\n",
    "        features[\"industry\"] = features.get(\"industry\", pd.Series(\"unspecified\", index=features.index)).fillna(\"unspecified\")\n",
    "        features[\"kam_owner\"] = features.get(\"kam_owner\", pd.Series(\"unassigned\", index=features.index)).fillna(\"unassigned\")\n",
    "        features[\"b2g_flag\"] = features[\"industry\"].str.lower().str.contains(\"government|public\").fillna(False).astype(int)\n",
    "        features[\"days_since_origination\"] = (self.reference_date - pd.to_datetime(features.get(\"origination_date\", features[\"date\"]), utc=True)).dt.days\n",
    "        features[\"days_since_origination\"] = features[\"days_since_origination\"].clip(lower=0).fillna(0).astype(int)\n",
    "        features[\"roll_rate_key\"] = features[\"customer_id\"].astype(str) + \"_\" + features[\"product_code\"].astype(str)\n",
    "        features = features.sort_values([\"roll_rate_key\", \"date\"])\n",
    "        features[\"prev_dpd\"] = features.groupby(\"roll_rate_key\")[\"dpd\"].shift(1).fillna(0)\n",
    "        features[\"roll_rate_delta\"] = features[\"dpd\"] - features[\"prev_dpd\"]\n",
    "        features[\"roll_rate_direction\"] = np.select([features[\"roll_rate_delta\"] > 0, features[\"roll_rate_delta\"] < 0], [\"deteriorating\", \"improving\"], default=\"stable\")\n",
    "        features[\"alert_usury_micro\"] = ((features[\"customer_type\"] == \"micro\") & (features[\"apr\"] > 0.85)).astype(int)\n",
    "        features[\"alert_high_utilization\"] = (features[\"utilization_ratio\"] > 0.95).astype(int)\n",
    "        features[\"alert_high_dpd\"] = (features[\"dpd\"] >= 90).astype(int)\n",
    "        features[\"alert_pdf_gap\"] = 0\n",
    "        alerts_records: List[Dict[str, Any]] = []\n",
    "        alert_columns = {\"alert_usury_micro\": \"critical\", \"alert_high_utilization\": \"high\", \"alert_high_dpd\": \"critical\", \"alert_pdf_gap\": \"medium\"}\n",
    "        for alert_col, severity in alert_columns.items():\n",
    "            flagged = features[features[alert_col] == 1]\n",
    "            for _, row in flagged.iterrows():\n",
    "                alerts_records.append(dict(customer_id=row.get(\"customer_id\"), rule=alert_col, severity=severity, details=f\"DPD={row.get('dpd')}|Util={row.get('utilization_ratio'):.2f}\"))\n",
    "        alerts = pd.DataFrame(alerts_records) if alerts_records else pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "        return FeatureArtifacts(features=features.reset_index(drop=True), alerts=alerts)\n",
    "\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_artifacts = feature_engineer.transform(master_frame)\n",
    "feature_frame = feature_artifacts.features\n",
    "alerts_frame = feature_artifacts.alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Iterable, List, NamedTuple, Optional\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    alerts_frame\n",
    "except NameError:\n",
    "    alerts_frame = pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "\n",
    "try:\n",
    "    master_frame\n",
    "except NameError:\n",
    "    master_frame = pd.DataFrame()\n",
    "\n",
    "DELINQUENCY_BUCKETS = [-np.inf, 0, 30, 60, 90, 120, np.inf]\n",
    "DELINQUENCY_LABELS = [\"current\", \"1_30\", \"31_60\", \"61_90\", \"91_120\", \"120_plus\"]\n",
    "SEGMENT_LABELS = list(\"ABCDEF\")\n",
    "\n",
    "class FeatureArtifacts(NamedTuple):\n",
    "    features: pd.DataFrame\n",
    "    alerts: pd.DataFrame\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, reference_date: Optional[pd.Timestamp] = None) -> None:\n",
    "        self.reference_date = reference_date or pd.Timestamp.utcnow().normalize()\n",
    "\n",
    "    def _derive_customer_type(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        if \"customer_type\" in frame.columns:\n",
    "            return frame[\"customer_type\"].fillna(\"unspecified\")\n",
    "        balance = frame.get(\"balance\", pd.Series(np.nan, index=frame.index)).fillna(0)\n",
    "        exposure = frame.get(\"credit_limit\", pd.Series(np.nan, index=frame.index)).fillna(balance.clip(lower=1))\n",
    "        ratio = balance / exposure.replace({0: np.nan})\n",
    "        derived = np.where(balance >= 5_000_000, \"enterprise\", np.where(balance >= 500_000, \"corporate\", np.where(balance >= 50_000, \"sme\", \"micro\")))\n",
    "        derived = pd.Series(derived, index=frame.index)\n",
    "        derived = np.where(ratio >= 0.9, \"intensive\", derived)\n",
    "        return pd.Series(derived, index=frame.index)\n",
    "\n",
    "    def _segmentation(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        try:\n",
    "            return pd.qcut(frame[\"balance\"], q=min(6, frame[\"balance\"].nunique()), labels=SEGMENT_LABELS[: min(6, frame[\"balance\"].nunique())], duplicates=\"drop\").astype(str)\n",
    "        except Exception:\n",
    "            return pd.Series([\"A\"] * len(frame), index=frame.index)\n",
    "\n",
    "    def _delinquency_bucket(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        dpd_series = frame.get(\"dpd\") if \"dpd\" in frame.columns else frame.get(\"days_past_due\")\n",
    "        if dpd_series is None:\n",
    "            dpd_series = pd.Series(0, index=frame.index)\n",
    "        dpd_series = pd.to_numeric(dpd_series, errors=\"coerce\").fillna(0)\n",
    "        return pd.cut(dpd_series, bins=DELINQUENCY_BUCKETS, labels=DELINQUENCY_LABELS, right=True)\n",
    "\n",
    "    def transform(self, frame: pd.DataFrame) -> FeatureArtifacts:\n",
    "        if frame.empty:\n",
    "            empty_alerts = pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "            return FeatureArtifacts(frame.copy(), empty_alerts)\n",
    "\n",
    "        features = frame.copy()\n",
    "        features[\"date\"] = pd.to_datetime(features[\"date\"], errors=\"coerce\", utc=True)\n",
    "        features[\"customer_type\"] = self._derive_customer_type(features)\n",
    "        features[\"segment_code\"] = self._segmentation(features)\n",
    "        features[\"delinquency_bucket\"] = self._delinquency_bucket(features).astype(str)\n",
    "        features[\"dpd\"] = pd.to_numeric(features.get(\"dpd\", features.get(\"days_past_due\", 0)), errors=\"coerce\").fillna(0).astype(int)\n",
    "        features[\"dpd_30_flag\"] = (features[\"dpd\"] >= 30).astype(int)\n",
    "        features[\"dpd_60_flag\"] = (features[\"dpd\"] >= 60).astype(int)\n",
    "        features[\"dpd_90_flag\"] = (features[\"dpd\"] >= 90).astype(int)\n",
    "        features[\"utilization_ratio\"] = features[\"balance\"] / features.get(\"credit_limit\", features[\"balance\"].clip(lower=1))\n",
    "        features[\"utilization_ratio\"] = features[\"utilization_ratio\"].clip(upper=5).fillna(0)\n",
    "        features[\"apr\"] = pd.to_numeric(features.get(\"apr\", features.get(\"nominal_rate\", np.nan)), errors=\"coerce\")\n",
    "        features[\"apr\"] = features[\"apr\"].fillna(features[\"apr\"].median(skipna=True))\n",
    "        features[\"weighted_apr\"] = features.groupby(\"customer_id\")[\"balance\"].transform(lambda x: x / x.sum()).fillna(0) * features[\"apr\"]\n",
    "        features[\"balance_zscore\"] = (features[\"balance\"] - features[\"balance\"].mean()) / features[\"balance\"].std(ddof=0)\n",
    "        features[\"balance_zscore\"] = features[\"balance_zscore\"].fillna(0).clip(-3, 3)\n",
    "        features[\"industry\"] = features.get(\"industry\", pd.Series(\"unspecified\", index=features.index)).fillna(\"unspecified\")\n",
    "        features[\"kam_owner\"] = features.get(\"kam_owner\", pd.Series(\"unassigned\", index=features.index)).fillna(\"unassigned\")\n",
    "        features[\"b2g_flag\"] = features[\"industry\"].str.lower().str.contains(\"government|public\").fillna(False).astype(int)\n",
    "        orig_col = features.get(\"origination_date\", features[\"date\"])\n",
    "        features[\"days_since_origination\"] = (self.reference_date - pd.to_datetime(orig_col, utc=True)).dt.days\n",
    "        features[\"days_since_origination\"] = features[\"days_since_origination\"].clip(lower=0).fillna(0).astype(int)\n",
    "        features[\"roll_rate_key\"] = features[\"customer_id\"].astype(str) + \"_\" + features[\"product_code\"].astype(str)\n",
    "        features = features.sort_values([\"roll_rate_key\", \"date\"])\n",
    "        features[\"prev_dpd\"] = features.groupby(\"roll_rate_key\")[\"dpd\"].shift(1).fillna(0)\n",
    "        features[\"roll_rate_delta\"] = features[\"dpd\"] - features[\"prev_dpd\"]\n",
    "        features[\"roll_rate_direction\"] = np.select([features[\"roll_rate_delta\"] > 0, features[\"roll_rate_delta\"] < 0], [\"deteriorating\", \"improving\"], default=\"stable\")\n",
    "        features[\"alert_usury_micro\"] = ((features[\"customer_type\"] == \"micro\") & (features[\"apr\"] > 0.85)).astype(int)\n",
    "        features[\"alert_high_utilization\"] = (features[\"utilization_ratio\"] > 0.95).astype(int)\n",
    "        features[\"alert_high_dpd\"] = (features[\"dpd\"] >= 90).astype(int)\n",
    "        features[\"alert_pdf_gap\"] = 0\n",
    "\n",
    "        alert_columns: Dict[str, str] = {\n",
    "            \"alert_usury_micro\": \"critical\",\n",
    "            \"alert_high_utilization\": \"high\",\n",
    "            \"alert_high_dpd\": \"critical\",\n",
    "            \"alert_pdf_gap\": \"medium\",\n",
    "        }\n",
    "\n",
    "        alerts_records: List[Dict[str, Any]] = []\n",
    "        for alert_col, severity in alert_columns.items():\n",
    "            flagged = features[features[alert_col] == 1]\n",
    "            for _, row in flagged.iterrows():\n",
    "                alerts_records.append(dict(\n",
    "                    customer_id=row.get(\"customer_id\"),\n",
    "                    rule=alert_col,\n",
    "                    severity=severity,\n",
    "                    details=f\"DPD={row.get('dpd')}|Util={row.get('utilization_ratio', 0):.2f}\",\n",
    "                ))\n",
    "\n",
    "        alerts = pd.DataFrame(alerts_records) if alerts_records else pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "\n",
    "        return FeatureArtifacts(features=features.reset_index(drop=True), alerts=alerts)\n",
    "\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_artifacts = feature_engineer.transform(master_frame)\n",
    "feature_frame = feature_artifacts.features\n",
    "alerts_frame = feature_artifacts.alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI Calculation Engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    alerts_frame\n",
    "except NameError:\n",
    "    alerts_frame = pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "\n",
    "class KPIEngine:\n",
    "    def __init__(self, frame: pd.DataFrame) -> None:\n",
    "        self.frame = frame.copy()\n",
    "        if not self.frame.empty:\n",
    "            self.frame[\"date\"] = pd.to_datetime(self.frame[\"date\"], utc=True)\n",
    "            self.frame[\"month\"] = self.frame[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    def _ratio(self, numerator: pd.Series, denominator: pd.Series) -> float:\n",
    "        denom = denominator.sum()\n",
    "        if denom == 0:\n",
    "            return float(\"nan\")\n",
    "        return numerator.sum() / denom\n",
    "\n",
    "    def compute(self) -> Dict[str, Any]:\n",
    "        if self.frame.empty:\n",
    "            return {}\n",
    "\n",
    "        result: Dict[str, Any] = {}\n",
    "        current_frame = self.frame.copy()\n",
    "\n",
    "        result[\"aum\"] = current_frame[\"balance\"].sum()\n",
    "        result[\"active_clients\"] = current_frame[\"customer_id\"].nunique()\n",
    "        result[\"credit_lines\"] = current_frame.get(\"credit_limit\", pd.Series(0, index=current_frame.index)).sum()\n",
    "\n",
    "        churn_mask = current_frame.get(\"status\", pd.Series(\"active\", index=current_frame.index)).str.lower().eq(\"churned\")\n",
    "        result[\"churn_rate\"] = churn_mask.mean()\n",
    "\n",
    "        default_mask = current_frame.get(\"default_flag\", pd.Series(0, index=current_frame.index)).astype(int)\n",
    "        result[\"default_rate\"] = default_mask.mean()\n",
    "\n",
    "        dpd_group = current_frame.groupby(\"delinquency_bucket\")[\"balance\"].sum().rename(\"aum\")\n",
    "        result[\"dpd_buckets\"] = dpd_group\n",
    "\n",
    "        result[\"rotation\"] = self._ratio(\n",
    "            current_frame.get(\"payments\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"balance\", pd.Series(0, index=current_frame.index)),\n",
    "        )\n",
    "\n",
    "        result[\"weighted_apr\"] = current_frame[\"weighted_apr\"].mean()\n",
    "\n",
    "        result[\"revenue\"] = current_frame.get(\"interest_income\", pd.Series(0, index=current_frame.index)).sum()\n",
    "        result[\"ebitda\"] = current_frame.get(\"ebitda\", pd.Series(0, index=current_frame.index)).sum()\n",
    "\n",
    "        result[\"concentration_top10\"] = (\n",
    "            current_frame.groupby(\"customer_id\")[\"balance\"].sum().nlargest(10).sum() / result[\"aum\"]\n",
    "            if result[\"aum\"]\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "\n",
    "        ltv = current_frame.get(\"ltv\", pd.Series(0, index=current_frame.index))\n",
    "        cac = current_frame.get(\"cac\", pd.Series(np.nan, index=current_frame.index))\n",
    "        current_frame[\"ltv_cac_ratio\"] = np.where(cac.fillna(0) == 0, np.nan, ltv / cac)\n",
    "\n",
    "        channel_col = \"channel\" if \"channel\" in current_frame.columns else \"source_name\"\n",
    "        result[\"ltv_cac_by_segment\"] = current_frame.groupby([\"segment_code\", channel_col]).ltv_cac_ratio.mean()\n",
    "\n",
    "        result[\"nrr\"] = self._ratio(\n",
    "            current_frame.get(\"recurring_revenue\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"starting_revenue\", pd.Series(1, index=current_frame.index)),\n",
    "        )\n",
    "\n",
    "        result[\"nsm\"] = current_frame.get(\"north_star_metric\", pd.Series(0, index=current_frame.index)).mean()\n",
    "\n",
    "        result[\"penetration\"] = self._ratio(\n",
    "            current_frame.get(\"active_products\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"available_products\", pd.Series(1, index=current_frame.index)),\n",
    "        )\n",
    "\n",
    "        result[\"b2g_percent\"] = current_frame[\"b2g_flag\"].mean()\n",
    "\n",
    "        status_column = current_frame.get(\"status\", pd.Series(\"active\", index=current_frame.index)).str.lower()\n",
    "        result[\"new_recurrent_recovered\"] = status_column.value_counts(dropna=False)\n",
    "\n",
    "        group_cols = [\"industry\", \"kam_owner\", \"segment_code\", \"customer_type\"]\n",
    "        aggregation = current_frame.groupby(group_cols)[\"balance\"].sum().rename(\"aum\")\n",
    "        result[\"aum_by_group\"] = aggregation\n",
    "\n",
    "        behavior_mask = (current_frame[\"customer_type\"] == \"micro\") & (current_frame[\"apr\"] > 0.85)\n",
    "        result[\"usury_micro_share\"] = behavior_mask.mean()\n",
    "\n",
    "        result[\"pod\"] = current_frame.get(\"probability_of_default\", pd.Series(np.nan, index=current_frame.index)).mean()\n",
    "\n",
    "        if not alerts_frame.empty:\n",
    "            result[\"alerts_active\"] = alerts_frame.groupby(\"severity\").size()\n",
    "\n",
    "        return result\n",
    "\n",
    "kpi_engine = KPIEngine(feature_frame)\n",
    "kpi_summary = kpi_engine.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db129b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Growth Analysis & Projections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    kpi_summary\n",
    "except NameError:\n",
    "    kpi_summary = {}\n",
    "\n",
    "DEFAULT_TARGETS = dict(\n",
    "    aum=1_200_000_000,\n",
    "    revenue=95_000_000,\n",
    "    ebitda=42_000_000,\n",
    "    b2g_percent=0.35,\n",
    "    churn_rate=0.03,\n",
    ")\n",
    "\n",
    "def build_monthly_path(frame: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    if frame.empty or metric not in frame.columns:\n",
    "        return pd.DataFrame()\n",
    "    monthly = frame.groupby(\"month\")[metric].sum().reset_index()\n",
    "    monthly[\"monthly_growth\"] = monthly[metric].pct_change().fillna(0)\n",
    "    monthly[\"ramp_type\"] = np.select(\n",
    "        [monthly[\"monthly_growth\"] > 0.1, monthly[\"monthly_growth\"] < -0.05],\n",
    "        [\"ramp\", \"trough\"],\n",
    "        default=\"steady\",\n",
    "    )\n",
    "    return monthly\n",
    "\n",
    "def growth_gap_analysis(kpis: Dict[str, Any], targets: Dict[str, float]) -> pd.DataFrame:\n",
    "    if not kpis:\n",
    "        return pd.DataFrame()\n",
    "    records = []\n",
    "    for metric, target in targets.items():\n",
    "        actual = kpis.get(metric, np.nan)\n",
    "        gap = target - actual if not pd.isna(actual) else np.nan\n",
    "        progress = actual / target if target else np.nan\n",
    "        records.append(dict(metric=metric, actual=actual, target=target, gap=gap, progress=progress))\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"status\"] = np.select(\n",
    "        [df[\"progress\"] >= 1, df[\"progress\"] >= 0.8],\n",
    "        [\"achieved\", \"on_track\"],\n",
    "        default=\"gap\",\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def acquisition_vs_churn(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame.empty:\n",
    "        return pd.DataFrame()\n",
    "    status_series = frame.get(\"status\", pd.Series(\"active\", index=frame.index)).str.lower()\n",
    "    summary = status_series.value_counts().rename_axis(\"status\").reset_index(name=\"count\")\n",
    "    summary[\"group\"] = np.where(\n",
    "        summary[\"status\"].str.contains(\"new\"),\n",
    "        \"acquisition\",\n",
    "        np.where(summary[\"status\"].str.contains(\"churn\"), \"churn\", \"retained\"),\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "growth_targets = DEFAULT_TARGETS\n",
    "growth_gap_table = growth_gap_analysis(kpi_summary, growth_targets)\n",
    "growth_path_balance = build_monthly_path(feature_frame, \"balance\") if not feature_frame.empty else pd.DataFrame()\n",
    "acquisition_table = acquisition_vs_churn(feature_frame)\n",
    "\n",
    "implication_multiplier = 8.9\n",
    "if not growth_gap_table.empty:\n",
    "    growth_gap_table[\"implied_scale\"] = growth_gap_table[\"target\"] * implication_multiplier\n",
    "\n",
    "b2g_gap = (\n",
    "    growth_targets.get(\"b2g_percent\", np.nan) - kpi_summary.get(\"b2g_percent\", np.nan)\n",
    ") if kpi_summary else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e29e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing & Sales Analysis\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "def marketing_sales_breakdown(frame: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    if frame.empty:\n",
    "        return {}\n",
    "\n",
    "    aggregations: Dict[str, pd.DataFrame] = {}\n",
    "    group_fields = {\n",
    "        \"industry\": [\"industry\"],\n",
    "        \"kam\": [\"kam_owner\"],\n",
    "        \"channel\": [\"channel\"] if \"channel\" in frame.columns else [\"source_name\"],\n",
    "    }\n",
    "\n",
    "    for label, fields in group_fields.items():\n",
    "        grouped = frame.groupby(fields).agg(\n",
    "            aum=(\"balance\", \"sum\"),\n",
    "            clients=(\"customer_id\", \"nunique\"),\n",
    "            weighted_apr=(\"weighted_apr\", \"mean\"),\n",
    "            ltv_cac=(\"ltv_cac_ratio\", \"mean\"),\n",
    "        ).reset_index()\n",
    "        aggregations[label] = grouped\n",
    "\n",
    "    return aggregations\n",
    "\n",
    "marketing_sales_tables = marketing_sales_breakdown(feature_frame)\n",
    "treemap_ready = marketing_sales_tables.get(\"industry\") if marketing_sales_tables else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d20f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Analysis & Roll Rate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "def delinquency_summary(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame.empty:\n",
    "        return pd.DataFrame()\n",
    "    summary = frame.groupby([\"delinquency_bucket\", \"industry\", \"segment_code\"]).agg(\n",
    "        balance=(\"balance\", \"sum\"),\n",
    "        clients=(\"customer_id\", \"nunique\"),\n",
    "        pod_mean=(\"probability_of_default\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    total_balance = summary[\"balance\"].sum()\n",
    "    summary[\"delinquency_percent\"] = summary[\"balance\"] / total_balance if total_balance else np.nan\n",
    "    return summary\n",
    "\n",
    "def roll_rate_matrix(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame.empty:\n",
    "        return pd.DataFrame()\n",
    "    roll_data = frame.copy()\n",
    "    roll_data[\"next_bucket\"] = roll_data.groupby(\"roll_rate_key\")[\"delinquency_bucket\"].shift(-1)\n",
    "    matrix = pd.crosstab(\n",
    "        roll_data[\"delinquency_bucket\"],\n",
    "        roll_data[\"next_bucket\"],\n",
    "        normalize=\"index\",\n",
    "    ).fillna(0)\n",
    "    return matrix\n",
    "\n",
    "def npl_metrics(frame: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if frame.empty:\n",
    "        return {}\n",
    "    balance_total = frame[\"balance\"].sum()\n",
    "    npl = frame[frame[\"dpd\"] >= 180]\n",
    "    npl_balance = npl[\"balance\"].sum()\n",
    "    result = dict(\n",
    "        npl180_balance=npl_balance,\n",
    "        npl_ratio=(npl_balance / balance_total) if balance_total else np.nan,\n",
    "    )\n",
    "    if \"ltv\" in frame.columns:\n",
    "        ltv_45 = frame[frame[\"dpd\"] > 45][\"ltv\"].mean()\n",
    "        ltv_90 = frame[frame[\"dpd\"] > 90][\"ltv\"].mean()\n",
    "        result[\"ltv_delta_45_90\"] = ltv_45 - ltv_90 if not pd.isna(ltv_45) and not pd.isna(ltv_90) else np.nan\n",
    "    else:\n",
    "        result[\"ltv_delta_45_90\"] = np.nan\n",
    "    result[\"rotation_multiple\"] = (\n",
    "        frame.get(\"payments\", pd.Series(0, index=frame.index)).sum() / balance_total\n",
    "        if balance_total\n",
    "        else np.nan\n",
    "    )\n",
    "    return result\n",
    "\n",
    "risk_table = delinquency_summary(feature_frame)\n",
    "roll_matrix = roll_rate_matrix(feature_frame)\n",
    "risk_metrics = npl_metrics(feature_frame)\n",
    "equifax_vs_dpd = (\n",
    "    feature_frame[[\"equifax_score\", \"dpd\"]].dropna()\n",
    "    if \"equifax_score\" in feature_frame.columns\n",
    "    else pd.DataFrame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Audit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "try:\n",
    "    display\n",
    "except NameError:\n",
    "    from IPython.display import display\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    pdfplumber\n",
    "except NameError:\n",
    "    pdfplumber = None\n",
    "\n",
    "CRITICAL_COLUMNS = {\"customer_id\", \"date\", \"balance\", \"dpd\"}\n",
    "\n",
    "def data_quality_audit(frame: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if frame.empty:\n",
    "        return {\"score\": np.nan, \"table\": pd.DataFrame(), \"styled\": None, \"pdf_completeness\": 0.0}\n",
    "\n",
    "    total_rows = len(frame)\n",
    "    audit_records: List[Dict[str, Any]] = []\n",
    "    penalties = 0.0\n",
    "\n",
    "    for column in frame.columns:\n",
    "        nulls = frame[column].isna().sum()\n",
    "        zeros = (frame[column] == 0).sum() if pd.api.types.is_numeric_dtype(frame[column]) else np.nan\n",
    "        coverage = 1 - (nulls / total_rows) if total_rows else np.nan\n",
    "        if column in CRITICAL_COLUMNS and coverage < 0.9:\n",
    "            penalties += 0.1\n",
    "        audit_records.append(\n",
    "            dict(column=column, nulls=int(nulls), zeros=int(zeros) if not pd.isna(zeros) else np.nan, coverage=coverage)\n",
    "        )\n",
    "\n",
    "    audit_table = pd.DataFrame(audit_records)\n",
    "    coverage_mean = audit_table[\"coverage\"].mean()\n",
    "    quality_score = max(0.0, min(1.0, (coverage_mean if not pd.isna(coverage_mean) else 0.0) - penalties))\n",
    "\n",
    "    def _color(value: float) -> str:\n",
    "        if pd.isna(value):\n",
    "            return \"color: #E6E6EF; background-color: #3730A3\"\n",
    "        if value >= 0.95:\n",
    "            return \"color: #05101a; background-color: #22E7CC\"\n",
    "        if value >= 0.85:\n",
    "            return \"color: #F5F3FF; background-color: #2563EB\"\n",
    "        return \"color: #F5F3FF; background-color: #B91C1C\"\n",
    "\n",
    "    styled: Styler = audit_table.style.applymap(_color, subset=[\"coverage\"]).format({\"coverage\": \"{:.2%}\"})\n",
    "\n",
    "    pdf_completeness = 1.0 if pdfplumber else 0.0\n",
    "\n",
    "    return dict(score=quality_score, table=audit_table, styled=styled, pdf_completeness=pdf_completeness)\n",
    "\n",
    "quality_artifacts = data_quality_audit(feature_frame)\n",
    "quality_score = quality_artifacts.get(\"score\")\n",
    "quality_table = quality_artifacts.get(\"table\")\n",
    "quality_styled = quality_artifacts.get(\"styled\")\n",
    "if quality_styled is not None:\n",
    "    display(quality_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400473ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Summary & Insights\n",
    "import os\n",
    "import textwrap\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    logger\n",
    "except NameError:\n",
    "    import logging\n",
    "    logger = logging.getLogger(\"abaco_ingestion\")\n",
    "\n",
    "try:\n",
    "    kpi_summary\n",
    "except NameError:\n",
    "    kpi_summary = {}\n",
    "\n",
    "try:\n",
    "    risk_metrics\n",
    "except NameError:\n",
    "    risk_metrics = {}\n",
    "\n",
    "try:\n",
    "    growth_gap_table\n",
    "except NameError:\n",
    "    growth_gap_table = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    alerts_frame\n",
    "except NameError:\n",
    "    alerts_frame = pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "\n",
    "def generate_ai_summary(\n",
    "    kpis: Dict[str, Any],\n",
    "    risk: Dict[str, Any],\n",
    "    growth: pd.DataFrame,\n",
    "    alerts: pd.DataFrame,\n",
    "    market: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    prompt_lines: List[str] = []\n",
    "    prompt_lines.append(\"ABACO Financial Intelligence Executive Summary\")\n",
    "\n",
    "    if kpis:\n",
    "        prompt_lines.append(f\"AUM: {kpis.get('aum', np.nan):,.0f}\")\n",
    "        prompt_lines.append(f\"Default Rate: {kpis.get('default_rate', np.nan):.2%}\")\n",
    "        prompt_lines.append(f\"B2G Share: {kpis.get('b2g_percent', np.nan):.2%}\")\n",
    "\n",
    "    if risk:\n",
    "        prompt_lines.append(f\"NPL 180+: {risk.get('npl180_balance', np.nan):,.0f}\")\n",
    "\n",
    "    if market:\n",
    "        prompt_lines.append(f\"MYPE GDP Share: {market.get('gdp_share', '48.8%')}\")\n",
    "\n",
    "    prompt_lines.append(f\"Alerts Count: {len(alerts) if alerts is not None else 0}\")\n",
    "    prompt = \"\\n\".join(prompt_lines)\n",
    "\n",
    "    summary_text: Optional[str] = None\n",
    "\n",
    "    if gemini_key:\n",
    "        try:\n",
    "            from google.generativeai import GenerativeModel, configure\n",
    "\n",
    "            configure(api_key=gemini_key)\n",
    "            model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "            response = model.generate_content(prompt)\n",
    "            summary_text = response.text\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Gemini summary failed: %s\", exc)\n",
    "\n",
    "    if not summary_text:\n",
    "        themes: List[str] = []\n",
    "        if kpis:\n",
    "            themes.append(\n",
    "                f\"Assets under management currently {kpis.get('aum', np.nan):,.0f} with weighted APR {kpis.get('weighted_apr', np.nan):.2%}.\"\n",
    "            )\n",
    "            themes.append(\n",
    "                f\"Default rate holding at {kpis.get('default_rate', np.nan):.2%} while churn {kpis.get('churn_rate', np.nan):.2%}.\"\n",
    "            )\n",
    "        if risk:\n",
    "            themes.append(\n",
    "                f\"NPL ratio stands at {risk.get('npl_ratio', np.nan):.2%} with rotation multiple {risk.get('rotation_multiple', np.nan):.2f}.\"\n",
    "            )\n",
    "        if growth is not None and not growth.empty:\n",
    "            gap_row = growth.sort_values(\"gap\", ascending=False).head(1).to_dict(orient=\"records\")\n",
    "            if gap_row:\n",
    "                themes.append(\n",
    "                    f\"Largest growth delta resides in {gap_row[0]['metric']} with gap {gap_row[0]['gap']:,.0f}.\"\n",
    "                )\n",
    "        if market:\n",
    "            themes.append(\n",
    "                f\"Market TAM updated to {market.get('tam_update', '31,666')} with penetration insights incorporated.\"\n",
    "            )\n",
    "        summary_text = \" \".join(textwrap.fill(theme, width=120) for theme in themes)\n",
    "\n",
    "    return dict(prompt=prompt, summary=summary_text)\n",
    "\n",
    "ai_summary = generate_ai_summary(kpi_summary, risk_metrics, growth_gap_table, alerts_frame, market=None)\n",
    "print(ai_summary.get(\"summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations & Exports\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "try:\n",
    "    feature_frame\n",
    "except NameError:\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    growth_path_balance\n",
    "except NameError:\n",
    "    growth_path_balance = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    roll_matrix\n",
    "except NameError:\n",
    "    roll_matrix = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    treemap_ready\n",
    "except NameError:\n",
    "    treemap_ready = pd.DataFrame()\n",
    "\n",
    "EXPORT_DIR = pathlib.Path(\"artifacts/exports\")\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_figures(\n",
    "    frame: pd.DataFrame,\n",
    "    growth: pd.DataFrame,\n",
    "    risk: pd.DataFrame,\n",
    "    treemap_source: pd.DataFrame,\n",
    ") -> Dict[str, go.Figure]:\n",
    "    figures: Dict[str, go.Figure] = {}\n",
    "\n",
    "    if not frame.empty:\n",
    "        line_df = frame.groupby(\"date\")[\"balance\"].sum().reset_index()\n",
    "        figures[\"growth_line\"] = px.line(line_df, x=\"date\", y=\"balance\", title=\"Growth Trajectory (4K Ready)\")\n",
    "\n",
    "        churn_df = frame.groupby(\"delinquency_bucket\")[\"customer_id\"].nunique().reset_index(name=\"clients\")\n",
    "        figures[\"churn_bar\"] = px.bar(churn_df, x=\"delinquency_bucket\", y=\"clients\", title=\"Churn & Delinquency Bar\")\n",
    "\n",
    "        pie_df = frame.groupby(\"segment_code\")[\"balance\"].sum().reset_index()\n",
    "        figures[\"segment_pie\"] = px.pie(pie_df, names=\"segment_code\", values=\"balance\", title=\"Segment Penetration Pie\")\n",
    "\n",
    "        if \"equifax_score\" in frame.columns:\n",
    "            figures[\"risk_scatter\"] = px.scatter(\n",
    "                frame,\n",
    "                x=\"equifax_score\",\n",
    "                y=\"dpd\",\n",
    "                color=\"delinquency_bucket\",\n",
    "                title=\"Equifax vs DPD Scatter\",\n",
    "            )\n",
    "\n",
    "    if not treemap_source.empty:\n",
    "        figures[\"industry_treemap\"] = px.treemap(\n",
    "            treemap_source,\n",
    "            path=[treemap_source.columns[0]],\n",
    "            values=\"aum\",\n",
    "            color=\"weighted_apr\",\n",
    "            title=\"Industry TPV Treemap\",\n",
    "        )\n",
    "\n",
    "    if isinstance(risk, pd.DataFrame) and not risk.empty:\n",
    "        figures[\"roll_rate_heatmap\"] = go.Figure(\n",
    "            data=go.Heatmap(z=risk.values, x=risk.columns, y=risk.index, colorscale=\"Viridis\")\n",
    "        )\n",
    "        figures[\"roll_rate_heatmap\"].update_layout(title=\"Roll Rate Transition Matrix\")\n",
    "\n",
    "    return figures\n",
    "\n",
    "def export_figures(figures: Dict[str, go.Figure], directory: pathlib.Path, scale: int = 1) -> None:\n",
    "    for name, fig in figures.items():\n",
    "        fig.update_layout(template=\"abaco_dark_ultra\", width=3840, height=2160)\n",
    "        export_path = directory / f\"{name}.png\"\n",
    "        try:\n",
    "            fig.write_image(export_path, width=3840, height=2160, scale=scale)\n",
    "        except ValueError:\n",
    "            print(f\"Static export skipped for {name} (kaleido missing)\")\n",
    "\n",
    "def export_fact_table(frame: pd.DataFrame, directory: pathlib.Path) -> pathlib.Path:\n",
    "    if frame.empty:\n",
    "        raise ValueError(\"Feature frame is empty; nothing to export\")\n",
    "    csv_path = directory / \"abaco_fact_table.csv\"\n",
    "    frame.to_csv(csv_path, index=False)\n",
    "    return csv_path\n",
    "\n",
    "figures = build_figures(feature_frame, growth_path_balance, roll_matrix, treemap_ready)\n",
    "export_figures(figures, EXPORT_DIR)\n",
    "\n",
    "fact_table_path: Optional[pathlib.Path] = None\n",
    "if not feature_frame.empty:\n",
    "    fact_table_path = export_fact_table(feature_frame, EXPORT_DIR)\n",
    "    print(f\"Fact table exported to {fact_table_path}\")\n",
    "\n",
    "html_path = EXPORT_DIR / \"abaco_fact_table.html\"\n",
    "if not feature_frame.empty:\n",
    "    feature_frame.sample(min(1000, len(feature_frame))).to_html(html_path, index=False)\n",
    "    print(f\"HTML table exported to {html_path}\")\n",
    "\n",
    "looker_placeholder = EXPORT_DIR / \"looker_ready_manifest.json\"\n",
    "with open(looker_placeholder, \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(\n",
    "        dict(\n",
    "            dataset=\"abaco_financial_intelligence\",\n",
    "            exported=str(fact_table_path) if fact_table_path else \"pending\",\n",
    "        ),\n",
    "        handle,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(f\"Looker placeholder created at {looker_placeholder}\")\n",
    "print(\"Slack and HubSpot export hooks pending integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Analysis from MYPE 2025 PDF\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    pdfplumber\n",
    "except NameError:\n",
    "    pdfplumber = None\n",
    "\n",
    "try:\n",
    "    generate_ai_summary\n",
    "except NameError:\n",
    "    def generate_ai_summary(*args: Any, **kwargs: Any) -> Dict[str, Any]:\n",
    "        return {\"prompt\": \"\", \"summary\": None}\n",
    "\n",
    "try:\n",
    "    kpi_summary\n",
    "except NameError:\n",
    "    kpi_summary = {}\n",
    "\n",
    "try:\n",
    "    risk_metrics\n",
    "except NameError:\n",
    "    risk_metrics = {}\n",
    "\n",
    "try:\n",
    "    growth_gap_table\n",
    "except NameError:\n",
    "    growth_gap_table = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    alerts_frame\n",
    "except NameError:\n",
    "    alerts_frame = pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "\n",
    "MYPE_PDF_PATH = pathlib.Path(\"data/mype_report_2025.pdf\")\n",
    "\n",
    "def extract_market_insights(pdf_path: pathlib.Path) -> Dict[str, Any]:\n",
    "    stats: Dict[str, Any] = dict(\n",
    "        gdp_share=\"48.8%\",\n",
    "        tam_update=\"31,666\",\n",
    "        challenges=[\"Limited formal credit access\", \"Fragmented guarantee schemes\"],\n",
    "        opportunities=[\"Digital onboarding expansion\", \"Supply chain financing partnerships\"],\n",
    "        behavior_links=\"Micro-segment displays high cash-cycle volatility\",\n",
    "    )\n",
    "\n",
    "    if not pdfplumber or not pdf_path.exists():\n",
    "        stats[\"source\"] = \"fallback\"\n",
    "        return stats\n",
    "\n",
    "    insights_text: List[str] = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages, start=1):\n",
    "            if page_number in {1, 2, 35}:\n",
    "                text = page.extract_text() or \"\"\n",
    "                insights_text.append(text)\n",
    "\n",
    "    combined = \" \\n\".join(insights_text)\n",
    "    if combined:\n",
    "        stats[\"source\"] = \"pdf\"\n",
    "        stats[\"raw_text_excerpt\"] = combined[:2000]\n",
    "    else:\n",
    "        stats[\"source\"] = \"fallback\"\n",
    "\n",
    "    return stats\n",
    "\n",
    "market_insights = extract_market_insights(MYPE_PDF_PATH)\n",
    "if market_insights:\n",
    "    print(json.dumps(market_insights, indent=2))\n",
    "\n",
    "ai_summary = generate_ai_summary(kpi_summary, risk_metrics, growth_gap_table, alerts_frame, market=market_insights)\n",
    "print(\"Updated AI summary with market context:\", ai_summary.get(\"summary\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
