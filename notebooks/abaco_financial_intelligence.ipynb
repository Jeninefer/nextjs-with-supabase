{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b098e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1214: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Union\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "alerts_frame = globals().get(\"alerts_frame\", pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "master_frame = globals().get(\"master_frame\")\n",
    "\n",
    "def _build_sample_master_frame() -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"customer_id\": [\"CUST001\", \"CUST002\", \"CUST003\"],\n",
    "        \"date\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-01\"],\n",
    "        \"balance\": [100000, 50000, 25000],\n",
    "        \"credit_limit\": [150000, 75000, 30000],\n",
    "        \"dpd\": [0, 45, 95],\n",
    "        \"product_code\": [\"CC\", \"PL\", \"CC\"],\n",
    "        \"origination_date\": [\"2023-01-01\", \"2023-06-01\", \"2023-12-01\"],\n",
    "        \"industry\": [\"Technology\", \"Manufacturing\", \"Government\"],\n",
    "        \"kam_owner\": [\"KAM001\", \"KAM002\", \"KAM001\"]\n",
    "    })\n",
    "if master_frame is None or getattr(master_frame, \"empty\", True):\n",
    "    master_frame = _build_sample_master_frame()\n",
    "    print(\"Created sample master_frame with\", len(master_frame), \"records\")\n",
    "\n",
    "DELINQUENCY_BUCKETS: List[float] = [-np.inf, 0, 30, 60, 90, 120, np.inf]\n",
    "DELINQUENCY_LABELS = [\"current\", \"1_30\", \"31_60\", \"61_90\", \"91_120\", \"120_plus\"]\n",
    "SEGMENT_LABELS = list(\"ABCDEF\")\n",
    "\n",
    "class FeatureArtifacts(NamedTuple):\n",
    "    features: pd.DataFrame\n",
    "    alerts: pd.DataFrame\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, reference_date: Optional[pd.Timestamp] = None) -> None:\n",
    "        self.reference_date = reference_date or pd.Timestamp.utcnow().normalize()\n",
    "\n",
    "    def _derive_customer_type(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        if \"customer_type\" in frame.columns:\n",
    "            return frame[\"customer_type\"].fillna(\"unspecified\").astype(str)\n",
    "        balance = frame.get(\"balance\")\n",
    "        if balance is None:\n",
    "            balance = pd.Series(0, index=frame.index)\n",
    "        elif not isinstance(balance, pd.Series):\n",
    "            balance = pd.Series(balance, index=frame.index)\n",
    "        balance = balance.fillna(0)\n",
    "        exposure = frame.get(\"credit_limit\")\n",
    "        if exposure is None:\n",
    "            exposure = balance.clip(lower=1)\n",
    "        elif not isinstance(exposure, pd.Series):\n",
    "            exposure = pd.Series(exposure, index=frame.index)\n",
    "        exposure = exposure.where(exposure.notna() & (exposure != 0), balance.clip(lower=1))\n",
    "        ratio = balance / exposure.replace({0: np.nan})\n",
    "        derived = np.where(\n",
    "            balance >= 5_000_000,\n",
    "            \"enterprise\",\n",
    "            np.where(balance >= 500_000, \"corporate\", np.where(balance >= 50_000, \"sme\", \"micro\"))\n",
    "        )\n",
    "        derived = np.where(ratio >= 0.9, \"intensive\", derived)\n",
    "        return pd.Series(derived, index=frame.index)\n",
    "\n",
    "    def _segmentation(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        try:\n",
    "            unique = frame[\"balance\"].nunique()\n",
    "            buckets = min(6, unique)\n",
    "            return pd.qcut(frame[\"balance\"], q=buckets, labels=SEGMENT_LABELS[: buckets], duplicates=\"drop\").astype(str)\n",
    "        except Exception:\n",
    "            return pd.Series([\"A\"] * len(frame), index=frame.index)\n",
    "\n",
    "    def _delinquency_bucket(self, frame: pd.DataFrame) -> pd.Series:\n",
    "        if \"dpd\" in frame.columns:\n",
    "            dpd_source = frame[\"dpd\"]\n",
    "        else:\n",
    "            dpd_source = frame.get(\"days_past_due\", pd.Series(0, index=frame.index))\n",
    "        if not isinstance(dpd_source, pd.Series):\n",
    "            dpd_source = pd.Series(dpd_source, index=frame.index)\n",
    "        dpd_series = pd.to_numeric(dpd_source, errors=\"coerce\").fillna(0)\n",
    "        return pd.cut(dpd_series, bins=DELINQUENCY_BUCKETS, labels=DELINQUENCY_LABELS, right=True)\n",
    "\n",
    "    def transform(self, frame: pd.DataFrame) -> FeatureArtifacts:\n",
    "        if frame.empty:\n",
    "            empty_alerts = pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "            return FeatureArtifacts(frame.copy(), empty_alerts)\n",
    "        prepared = self._prepare_base_features(frame.copy())\n",
    "        enriched = self._compute_financial_metrics(prepared)\n",
    "        alerts = self._collect_alerts(enriched)\n",
    "        return FeatureArtifacts(features=enriched.reset_index(drop=True), alerts=alerts)\n",
    "\n",
    "    def _prepare_base_features(self, features: pd.DataFrame) -> pd.DataFrame:\n",
    "        features[\"date\"] = pd.to_datetime(features[\"date\"], errors=\"coerce\", utc=True)\n",
    "        features[\"customer_type\"] = self._derive_customer_type(features)\n",
    "        features[\"segment_code\"] = self._segmentation(features)\n",
    "        features[\"delinquency_bucket\"] = self._delinquency_bucket(features).astype(str)\n",
    "        features[\"dpd\"] = self._normalize_dpd(features)\n",
    "        return features\n",
    "\n",
    "    def _compute_financial_metrics(self, features: pd.DataFrame) -> pd.DataFrame:\n",
    "        balance_clip = features[\"balance\"].clip(lower=1)\n",
    "        credit_limit_source = features.get(\"credit_limit\")\n",
    "        credit_limit_series = self._normalize_series(credit_limit_source, features.index, np.nan)\n",
    "        credit_limit_series = pd.to_numeric(credit_limit_series, errors=\"coerce\").where(\n",
    "            lambda s: s.notna() & (s != 0),\n",
    "            balance_clip\n",
    "        )\n",
    "        utilization = features[\"balance\"] / credit_limit_series\n",
    "        features[\"utilization_ratio\"] = utilization.replace([np.inf, -np.inf], np.nan).clip(upper=5).fillna(0)\n",
    "        features[\"apr\"] = self._prepare_apr(features)\n",
    "        balance_share = features.groupby(\"customer_id\")[\"balance\"].transform(\n",
    "            lambda values: values / values.sum()\n",
    "        ).fillna(0.0)\n",
    "        features[\"weighted_apr\"] = balance_share * features[\"apr\"]\n",
    "        zscore = (features[\"balance\"] - features[\"balance\"].mean()) / features[\"balance\"].std(ddof=0)\n",
    "        features[\"balance_zscore\"] = zscore.fillna(0).clip(-3, 3)\n",
    "        features[\"industry\"] = self._normalize_series(\n",
    "            features.get(\"industry\"),\n",
    "            features.index,\n",
    "            \"unspecified\"\n",
    "        ).fillna(\"unspecified\")\n",
    "        features[\"kam_owner\"] = self._normalize_series(\n",
    "            features.get(\"kam_owner\"),\n",
    "            features.index,\n",
    "            \"unassigned\"\n",
    "        ).fillna(\"unassigned\")\n",
    "        industry_lower = features[\"industry\"].str.lower()\n",
    "        features[\"b2g_flag\"] = industry_lower.str.contains(\"government|public\").fillna(False).astype(int)\n",
    "        origination_source = features.get(\"origination_date\", features[\"date\"])\n",
    "        days_open = (self.reference_date - pd.to_datetime(origination_source, utc=True)).dt.days\n",
    "        features[\"days_since_origination\"] = days_open.clip(lower=0).fillna(0).astype(int)\n",
    "        features[\"roll_rate_key\"] = features[\"customer_id\"].astype(str) + \"_\" + features[\"product_code\"].astype(str)\n",
    "        features = features.sort_values([\"roll_rate_key\", \"date\"])\n",
    "        features[\"prev_dpd\"] = features.groupby(\"roll_rate_key\")[\"dpd\"].shift(1).fillna(0)\n",
    "        features[\"roll_rate_delta\"] = features[\"dpd\"] - features[\"prev_dpd\"]\n",
    "        features[\"roll_rate_direction\"] = np.select(\n",
    "            [features[\"roll_rate_delta\"] > 0, features[\"roll_rate_delta\"] < 0],\n",
    "            [\"deteriorating\", \"improving\"],\n",
    "            default=\"stable\"\n",
    "        )\n",
    "        features[\"alert_usury_micro\"] = ((features[\"customer_type\"] == \"micro\") & (features[\"apr\"] > 0.85)).astype(int)\n",
    "        features[\"alert_high_utilization\"] = (features[\"utilization_ratio\"] > 0.95).astype(int)\n",
    "        features[\"alert_high_dpd\"] = (features[\"dpd\"] >= 90).astype(int)\n",
    "        features[\"alert_pdf_gap\"] = 0\n",
    "        return features\n",
    "\n",
    "    def _collect_alerts(self, features: pd.DataFrame) -> pd.DataFrame:\n",
    "        alerts_records: List[Dict[str, Any]] = []\n",
    "        alert_columns = {\n",
    "            \"alert_usury_micro\": \"critical\",\n",
    "            \"alert_high_utilization\": \"high\",\n",
    "            \"alert_high_dpd\": \"critical\",\n",
    "            \"alert_pdf_gap\": \"medium\"\n",
    "        }\n",
    "        for alert_col, severity in alert_columns.items():\n",
    "            flagged = features[features[alert_col] == 1]\n",
    "            for _, row in flagged.iterrows():\n",
    "                alerts_records.append({\n",
    "                    \"customer_id\": row.get(\"customer_id\"),\n",
    "                    \"rule\": alert_col,\n",
    "                    \"severity\": severity,\n",
    "                    \"details\": f\"DPD={row.get('dpd')}|Util={row.get('utilization_ratio'):.2f}\"\n",
    "                })\n",
    "        if alerts_records:\n",
    "            return pd.DataFrame(alerts_records)\n",
    "        return pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"])\n",
    "\n",
    "    def _normalize_series(self, source: Any, index: pd.Index, default: Any) -> pd.Series:\n",
    "        if source is None:\n",
    "            return pd.Series(default, index=index)\n",
    "        if isinstance(source, pd.Series):\n",
    "            return source\n",
    "        return pd.Series(source, index=index)\n",
    "\n",
    "    def _normalize_dpd(self, features: pd.DataFrame) -> pd.Series:\n",
    "        dpd_source = features.get(\"dpd\")\n",
    "        if dpd_source is None:\n",
    "            dpd_source = features.get(\"days_past_due\")\n",
    "        normalized = self._normalize_series(dpd_source, features.index, 0)\n",
    "        return pd.to_numeric(normalized, errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    def _prepare_apr(self, features: pd.DataFrame) -> pd.Series:\n",
    "        apr_source = features.get(\"apr\") if \"apr\" in features.columns else features.get(\"nominal_rate\")\n",
    "        apr_series = pd.to_numeric(self._normalize_series(apr_source, features.index, np.nan), errors=\"coerce\")\n",
    "        apr_median = apr_series.median(skipna=True)\n",
    "        if pd.isna(apr_median):\n",
    "            apr_median = 0.0\n",
    "        return apr_series.fillna(apr_median).astype(float)\n",
    "\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_artifacts = feature_engineer.transform(master_frame)\n",
    "feature_frame = feature_artifacts.features\n",
    "alerts_frame = feature_artifacts.alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5442c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3587/531879348.py:13: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  self.frame[\"month\"] = self.frame[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n"
     ]
    }
   ],
   "source": [
    "# KPI Calculation Engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Union\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "alerts_frame = globals().get(\"alerts_frame\", pd.DataFrame(columns=[\"customer_id\", \"rule\", \"severity\", \"details\"]))\n",
    "\n",
    "class KPIEngine:\n",
    "    def __init__(self, frame: pd.DataFrame) -> None:\n",
    "        self.frame = frame.copy()\n",
    "        if not self.frame.empty:\n",
    "            self.frame[\"date\"] = pd.to_datetime(self.frame[\"date\"], utc=True)\n",
    "            self.frame[\"month\"] = self.frame[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    def _ratio(self, numerator: pd.Series, denominator: pd.Series) -> float:\n",
    "        denom = denominator.sum()\n",
    "        if denom == 0:\n",
    "            return float(\"nan\")\n",
    "        return numerator.sum() / denom\n",
    "\n",
    "    def compute(self) -> Dict[str, Any]:\n",
    "        if self.frame.empty:\n",
    "            return {}\n",
    "\n",
    "        result: Dict[str, Any] = {}\n",
    "        current_frame = self.frame.copy()\n",
    "\n",
    "        result[\"aum\"] = current_frame[\"balance\"].sum()\n",
    "        result[\"active_clients\"] = current_frame[\"customer_id\"].nunique()\n",
    "        result[\"credit_lines\"] = current_frame.get(\"credit_limit\", pd.Series(0, index=current_frame.index)).sum()\n",
    "\n",
    "        churn_mask = current_frame.get(\"status\", pd.Series(\"active\", index=current_frame.index)).str.lower().eq(\"churned\")\n",
    "        result[\"churn_rate\"] = churn_mask.mean()\n",
    "\n",
    "        default_mask = current_frame.get(\"default_flag\", pd.Series(0, index=current_frame.index)).astype(int)\n",
    "        result[\"default_rate\"] = default_mask.mean()\n",
    "\n",
    "        dpd_group = current_frame.groupby(\"delinquency_bucket\")[\"balance\"].sum().rename(\"aum\")\n",
    "        result[\"dpd_buckets\"] = dpd_group\n",
    "\n",
    "        result[\"rotation\"] = self._ratio(\n",
    "            current_frame.get(\"payments\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"balance\", pd.Series(0, index=current_frame.index))\n",
    "        )\n",
    "\n",
    "        result[\"weighted_apr\"] = current_frame[\"weighted_apr\"].mean()\n",
    "\n",
    "        result[\"revenue\"] = current_frame.get(\"interest_income\", pd.Series(0, index=current_frame.index)).sum()\n",
    "        result[\"ebitda\"] = current_frame.get(\"ebitda\", pd.Series(0, index=current_frame.index)).sum()\n",
    "\n",
    "        result[\"concentration_top10\"] = (\n",
    "            current_frame.groupby(\"customer_id\")[\"balance\"].sum().nlargest(10).sum() / result[\"aum\"]\n",
    "            if result[\"aum\"]\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "\n",
    "        ltv = current_frame.get(\"ltv\", pd.Series(0, index=current_frame.index))\n",
    "        cac = current_frame.get(\"cac\", pd.Series(np.nan, index=current_frame.index))\n",
    "        current_frame[\"ltv_cac_ratio\"] = np.where(cac.fillna(0) == 0, np.nan, ltv / cac)\n",
    "\n",
    "        channel_col = next((col for col in (\"channel\", \"source_name\") if col in current_frame.columns), None)\n",
    "        if channel_col:\n",
    "            result[\"ltv_cac_by_segment\"] = current_frame.groupby([\"segment_code\", channel_col]).ltv_cac_ratio.mean()\n",
    "        else:\n",
    "            result[\"ltv_cac_by_segment\"] = current_frame.groupby([\"segment_code\"]).ltv_cac_ratio.mean()\n",
    "\n",
    "        result[\"nrr\"] = self._ratio(\n",
    "            current_frame.get(\"recurring_revenue\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"starting_revenue\", pd.Series(1, index=current_frame.index))\n",
    "        )\n",
    "\n",
    "        result[\"nsm\"] = current_frame.get(\"north_star_metric\", pd.Series(0, index=current_frame.index)).mean()\n",
    "\n",
    "        result[\"penetration\"] = self._ratio(\n",
    "            current_frame.get(\"active_products\", pd.Series(0, index=current_frame.index)),\n",
    "            current_frame.get(\"available_products\", pd.Series(1, index=current_frame.index))\n",
    "        )\n",
    "\n",
    "        result[\"b2g_percent\"] = current_frame[\"b2g_flag\"].mean()\n",
    "\n",
    "        status_column = current_frame.get(\"status\", pd.Series(\"active\", index=current_frame.index)).str.lower()\n",
    "        result[\"new_recurrent_recovered\"] = status_column.value_counts(dropna=False)\n",
    "\n",
    "        group_cols = [\"industry\", \"kam_owner\", \"segment_code\", \"customer_type\"]\n",
    "        aggregation = current_frame.groupby(group_cols)[\"balance\"].sum().rename(\"aum\")\n",
    "        result[\"aum_by_group\"] = aggregation\n",
    "\n",
    "        behavior_mask = (current_frame[\"customer_type\"] == \"micro\") & (current_frame[\"apr\"] > 0.85)\n",
    "        result[\"usury_micro_share\"] = behavior_mask.mean()\n",
    "\n",
    "        result[\"pod\"] = current_frame.get(\"probability_of_default\", pd.Series(np.nan, index=current_frame.index)).mean()\n",
    "\n",
    "        if not alerts_frame.empty:\n",
    "            result[\"alerts_active\"] = alerts_frame.groupby(\"severity\").size()\n",
    "\n",
    "        return result\n",
    "\n",
    "kpi_engine = KPIEngine(feature_frame)\n",
    "kpi_summary = kpi_engine.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb3bc4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['ltv_cac_ratio'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m         aggregations[label] = grouped\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m aggregations\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m marketing_sales_tables = \u001b[43mmarketing_sales_breakdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m treemap_ready = marketing_sales_tables.get(\u001b[33m\"\u001b[39m\u001b[33mindustry\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m marketing_sales_tables \u001b[38;5;28;01melse\u001b[39;00m pd.DataFrame()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmarketing_sales_breakdown\u001b[39m\u001b[34m(frame)\u001b[39m\n\u001b[32m     17\u001b[39m     group_fields[\u001b[33m\"\u001b[39m\u001b[33mchannel\u001b[39m\u001b[33m\"\u001b[39m] = channel_columns\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label, fields \u001b[38;5;129;01min\u001b[39;00m group_fields.items():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     grouped = \u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43maum\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbalance\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnunique\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweighted_apr\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweighted_apr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mltv_cac\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mltv_cac_ratio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.reset_index()\n\u001b[32m     26\u001b[39m     aggregations[label] = grouped\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m aggregations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1432\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = engine_kwargs\n\u001b[32m   1431\u001b[39m op = GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m result = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:190\u001b[39m, in \u001b[36mApply.agg\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_str()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agg_list_like()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:423\u001b[39m, in \u001b[36mApply.agg_dict_like\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame | Series:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m \u001b[33;03m    Result of aggregation.\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:1603\u001b[39m, in \u001b[36mGroupByApply.agg_or_apply_dict_like\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1598\u001b[39m     kwargs.update({\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m: engine, \u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m: engine_kwargs})\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m com.temp_setattr(\n\u001b[32m   1601\u001b[39m     obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1602\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m     result_index, result_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[32m   1607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:462\u001b[39m, in \u001b[36mApply.compute_dict_like\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m is_groupby = \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[32m    461\u001b[39m func = cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m.func)\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m func = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m is_non_unique_col = (\n\u001b[32m    465\u001b[39m     selected_obj.ndim == \u001b[32m2\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj.columns.nunique() < \u001b[38;5;28mlen\u001b[39m(selected_obj.columns)\n\u001b[32m    467\u001b[39m )\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/apply.py:663\u001b[39m, in \u001b[36mApply.normalize_dictlike_arg\u001b[39m\u001b[34m(self, how, obj, func)\u001b[39m\n\u001b[32m    661\u001b[39m     cols = Index(\u001b[38;5;28mlist\u001b[39m(func.keys())).difference(obj.columns, sort=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m do not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    665\u001b[39m aggregator_types = (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: \"Column(s) ['ltv_cac_ratio'] do not exist\""
     ]
    }
   ],
   "source": [
    "# Marketing & Sales Analysis\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "\n",
    "def marketing_sales_breakdown(frame: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    if frame.empty:\n",
    "        return {}\n",
    "\n",
    "    aggregations: Dict[str, pd.DataFrame] = {}\n",
    "    group_fields: Dict[str, List[str]] = {\n",
    "        \"industry\": [\"industry\"],\n",
    "        \"kam\": [\"kam_owner\"]\n",
    "    }\n",
    "    channel_columns = [column for column in (\"channel\", \"source_name\") if column in frame.columns]\n",
    "    if channel_columns:\n",
    "        group_fields[\"channel\"] = channel_columns\n",
    "\n",
    "    for label, fields in group_fields.items():\n",
    "        grouped = frame.groupby(fields, dropna=False).agg(\n",
    "            aum=(\"balance\", \"sum\"),\n",
    "            clients=(\"customer_id\", \"nunique\"),\n",
    "            weighted_apr=(\"weighted_apr\", \"mean\"),\n",
    "            ltv_cac=(\"ltv_cac_ratio\", \"mean\")\n",
    "        ).reset_index()\n",
    "        aggregations[label] = grouped\n",
    "\n",
    "    return aggregations\n",
    "\n",
    "marketing_sales_tables = marketing_sales_breakdown(feature_frame)\n",
    "treemap_ready = marketing_sales_tables.get(\"industry\") if marketing_sales_tables else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b35c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Score: 100.00%\n",
      "PDF Completeness: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3587/638794212.py:48: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styled = apply_map(_color, subset=[\"coverage\"]) if callable(apply_map) else styler\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_86b9f_row0_col3, #T_86b9f_row1_col3, #T_86b9f_row2_col3, #T_86b9f_row3_col3, #T_86b9f_row4_col3, #T_86b9f_row5_col3, #T_86b9f_row6_col3, #T_86b9f_row7_col3, #T_86b9f_row8_col3, #T_86b9f_row9_col3, #T_86b9f_row10_col3, #T_86b9f_row11_col3, #T_86b9f_row12_col3, #T_86b9f_row13_col3, #T_86b9f_row14_col3, #T_86b9f_row15_col3, #T_86b9f_row16_col3, #T_86b9f_row17_col3, #T_86b9f_row18_col3, #T_86b9f_row19_col3, #T_86b9f_row20_col3, #T_86b9f_row21_col3, #T_86b9f_row22_col3, #T_86b9f_row23_col3, #T_86b9f_row24_col3, #T_86b9f_row25_col3 {\n",
       "  color: #05101a;\n",
       "  background-color: #22E7CC;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_86b9f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_86b9f_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_86b9f_level0_col1\" class=\"col_heading level0 col1\" >nulls</th>\n",
       "      <th id=\"T_86b9f_level0_col2\" class=\"col_heading level0 col2\" >zeros</th>\n",
       "      <th id=\"T_86b9f_level0_col3\" class=\"col_heading level0 col3\" >coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_86b9f_row0_col0\" class=\"data row0 col0\" >customer_id</td>\n",
       "      <td id=\"T_86b9f_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row0_col3\" class=\"data row0 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_86b9f_row1_col0\" class=\"data row1 col0\" >date</td>\n",
       "      <td id=\"T_86b9f_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row1_col2\" class=\"data row1 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row1_col3\" class=\"data row1 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_86b9f_row2_col0\" class=\"data row2 col0\" >balance</td>\n",
       "      <td id=\"T_86b9f_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "      <td id=\"T_86b9f_row2_col3\" class=\"data row2 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_86b9f_row3_col0\" class=\"data row3 col0\" >credit_limit</td>\n",
       "      <td id=\"T_86b9f_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "      <td id=\"T_86b9f_row3_col3\" class=\"data row3 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_86b9f_row4_col0\" class=\"data row4 col0\" >dpd</td>\n",
       "      <td id=\"T_86b9f_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
       "      <td id=\"T_86b9f_row4_col3\" class=\"data row4 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_86b9f_row5_col0\" class=\"data row5 col0\" >product_code</td>\n",
       "      <td id=\"T_86b9f_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row5_col2\" class=\"data row5 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row5_col3\" class=\"data row5 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_86b9f_row6_col0\" class=\"data row6 col0\" >origination_date</td>\n",
       "      <td id=\"T_86b9f_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row6_col2\" class=\"data row6 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row6_col3\" class=\"data row6 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_86b9f_row7_col0\" class=\"data row7 col0\" >industry</td>\n",
       "      <td id=\"T_86b9f_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row7_col2\" class=\"data row7 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row7_col3\" class=\"data row7 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_86b9f_row8_col0\" class=\"data row8 col0\" >kam_owner</td>\n",
       "      <td id=\"T_86b9f_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row8_col2\" class=\"data row8 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row8_col3\" class=\"data row8 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_86b9f_row9_col0\" class=\"data row9 col0\" >customer_type</td>\n",
       "      <td id=\"T_86b9f_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row9_col2\" class=\"data row9 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row9_col3\" class=\"data row9 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_86b9f_row10_col0\" class=\"data row10 col0\" >segment_code</td>\n",
       "      <td id=\"T_86b9f_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row10_col2\" class=\"data row10 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row10_col3\" class=\"data row10 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_86b9f_row11_col0\" class=\"data row11 col0\" >delinquency_bucket</td>\n",
       "      <td id=\"T_86b9f_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row11_col2\" class=\"data row11 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row11_col3\" class=\"data row11 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_86b9f_row12_col0\" class=\"data row12 col0\" >utilization_ratio</td>\n",
       "      <td id=\"T_86b9f_row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row12_col2\" class=\"data row12 col2\" >0.000000</td>\n",
       "      <td id=\"T_86b9f_row12_col3\" class=\"data row12 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_86b9f_row13_col0\" class=\"data row13 col0\" >apr</td>\n",
       "      <td id=\"T_86b9f_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row13_col2\" class=\"data row13 col2\" >3.000000</td>\n",
       "      <td id=\"T_86b9f_row13_col3\" class=\"data row13 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_86b9f_row14_col0\" class=\"data row14 col0\" >weighted_apr</td>\n",
       "      <td id=\"T_86b9f_row14_col1\" class=\"data row14 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row14_col2\" class=\"data row14 col2\" >3.000000</td>\n",
       "      <td id=\"T_86b9f_row14_col3\" class=\"data row14 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_86b9f_row15_col0\" class=\"data row15 col0\" >balance_zscore</td>\n",
       "      <td id=\"T_86b9f_row15_col1\" class=\"data row15 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row15_col2\" class=\"data row15 col2\" >0.000000</td>\n",
       "      <td id=\"T_86b9f_row15_col3\" class=\"data row15 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_86b9f_row16_col0\" class=\"data row16 col0\" >b2g_flag</td>\n",
       "      <td id=\"T_86b9f_row16_col1\" class=\"data row16 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row16_col2\" class=\"data row16 col2\" >2.000000</td>\n",
       "      <td id=\"T_86b9f_row16_col3\" class=\"data row16 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_86b9f_row17_col0\" class=\"data row17 col0\" >days_since_origination</td>\n",
       "      <td id=\"T_86b9f_row17_col1\" class=\"data row17 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row17_col2\" class=\"data row17 col2\" >0.000000</td>\n",
       "      <td id=\"T_86b9f_row17_col3\" class=\"data row17 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_86b9f_row18_col0\" class=\"data row18 col0\" >roll_rate_key</td>\n",
       "      <td id=\"T_86b9f_row18_col1\" class=\"data row18 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row18_col2\" class=\"data row18 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row18_col3\" class=\"data row18 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_86b9f_row19_col0\" class=\"data row19 col0\" >prev_dpd</td>\n",
       "      <td id=\"T_86b9f_row19_col1\" class=\"data row19 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row19_col2\" class=\"data row19 col2\" >3.000000</td>\n",
       "      <td id=\"T_86b9f_row19_col3\" class=\"data row19 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_86b9f_row20_col0\" class=\"data row20 col0\" >roll_rate_delta</td>\n",
       "      <td id=\"T_86b9f_row20_col1\" class=\"data row20 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row20_col2\" class=\"data row20 col2\" >1.000000</td>\n",
       "      <td id=\"T_86b9f_row20_col3\" class=\"data row20 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_86b9f_row21_col0\" class=\"data row21 col0\" >roll_rate_direction</td>\n",
       "      <td id=\"T_86b9f_row21_col1\" class=\"data row21 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row21_col2\" class=\"data row21 col2\" >nan</td>\n",
       "      <td id=\"T_86b9f_row21_col3\" class=\"data row21 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_86b9f_row22_col0\" class=\"data row22 col0\" >alert_usury_micro</td>\n",
       "      <td id=\"T_86b9f_row22_col1\" class=\"data row22 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row22_col2\" class=\"data row22 col2\" >3.000000</td>\n",
       "      <td id=\"T_86b9f_row22_col3\" class=\"data row22 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_86b9f_row23_col0\" class=\"data row23 col0\" >alert_high_utilization</td>\n",
       "      <td id=\"T_86b9f_row23_col1\" class=\"data row23 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row23_col2\" class=\"data row23 col2\" >3.000000</td>\n",
       "      <td id=\"T_86b9f_row23_col3\" class=\"data row23 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_86b9f_row24_col0\" class=\"data row24 col0\" >alert_high_dpd</td>\n",
       "      <td id=\"T_86b9f_row24_col1\" class=\"data row24 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row24_col2\" class=\"data row24 col2\" >2.000000</td>\n",
       "      <td id=\"T_86b9f_row24_col3\" class=\"data row24 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86b9f_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_86b9f_row25_col0\" class=\"data row25 col0\" >alert_pdf_gap</td>\n",
       "      <td id=\"T_86b9f_row25_col1\" class=\"data row25 col1\" >0</td>\n",
       "      <td id=\"T_86b9f_row25_col2\" class=\"data row25 col2\" >3.000000</td>\n",
       "      <td id=\"T_86b9f_row25_col3\" class=\"data row25 col3\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75d6621e6900>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Quality Audit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, TYPE_CHECKING\n",
    "try:\n",
    "    import pdfplumber  # type: ignore[import-not-found]\n",
    "except ModuleNotFoundError:\n",
    "    pdfplumber = None\n",
    "if TYPE_CHECKING:\n",
    "    import pdfplumber as _pdfplumber_stub\n",
    "feature_frame = globals().get(\"feature_frame\", pd.DataFrame())\n",
    "\n",
    "CRITICAL_COLUMNS = {\"customer_id\", \"date\", \"balance\", \"dpd\"}\n",
    "\n",
    "def data_quality_audit(frame: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if frame.empty:\n",
    "        return {\"score\": np.nan, \"table\": pd.DataFrame(), \"styled\": None, \"pdf_completeness\": 0.0}\n",
    "\n",
    "    total_rows = len(frame)\n",
    "    audit_records: List[Dict[str, Any]] = []\n",
    "    penalties = 0.0\n",
    "\n",
    "    for column in frame.columns:\n",
    "        nulls = frame[column].isna().sum()\n",
    "        zeros = (frame[column] == 0).sum() if pd.api.types.is_numeric_dtype(frame[column]) else np.nan\n",
    "        coverage = 1 - (nulls / total_rows) if total_rows else np.nan\n",
    "        if column in CRITICAL_COLUMNS and coverage < 0.9:\n",
    "            penalties += 0.1\n",
    "        audit_records.append(\n",
    "            dict(column=column, nulls=int(nulls), zeros=int(zeros) if not pd.isna(zeros) else np.nan, coverage=coverage)\n",
    "        )\n",
    "\n",
    "    audit_table = pd.DataFrame(audit_records)\n",
    "    coverage_mean = audit_table[\"coverage\"].mean()\n",
    "    quality_score = max(0.0, min(1.0, (coverage_mean if not pd.isna(coverage_mean) else 0.0) - penalties))\n",
    "\n",
    "    def _color(value: float) -> str:\n",
    "        if pd.isna(value):\n",
    "            return \"color: #E6E6EF; background-color: #3730A3\"\n",
    "        if value >= 0.95:\n",
    "            return \"color: #05101a; background-color: #22E7CC\"\n",
    "        if value >= 0.85:\n",
    "            return \"color: #F5F3FF; background-color: #2563EB\"\n",
    "        return \"color: #F5F3FF; background-color: #B91C1C\"\n",
    "\n",
    "    styler = audit_table.style.format({\"coverage\": \"{:.2%}\"})\n",
    "    apply_map = getattr(styler, \"applymap\", None)\n",
    "    styled = apply_map(_color, subset=[\"coverage\"]) if callable(apply_map) else styler\n",
    "\n",
    "    pdf_completeness = 1.0 if pdfplumber else 0.0\n",
    "\n",
    "    return dict(score=quality_score, table=audit_table, styled=styled, pdf_completeness=pdf_completeness)\n",
    "quality_artifacts = data_quality_audit(feature_frame)\n",
    "quality_score = quality_artifacts.get(\"score\")\n",
    "quality_table = quality_artifacts.get(\"table\")\n",
    "quality_styled = quality_artifacts.get(\"styled\")\n",
    "\n",
    "print(f\"Data Quality Score: {quality_score:.2%}\" if not pd.isna(quality_score) else \"Data Quality Score: N/A\")\n",
    "print(f\"PDF Completeness: {quality_artifacts.get('pdf_completeness', 0.0):.1%}\")\n",
    "\n",
    "if quality_styled is not None:\n",
    "    display(quality_styled)\n",
    "else:\n",
    "    print(\"No data quality table to display - feature_frame is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Intelligence Visualization Engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class FinancialVisualizer:\n",
    "    def __init__(self, figsize: tuple = (12, 8)) -> None:\n",
    "        self.figsize = figsize\n",
    "        \n",
    "    def create_dashboard(self, kpi_data: Dict[str, Any], feature_data: pd.DataFrame) -> None:\n",
    "        \"\"\"Create comprehensive financial dashboard\"\"\"\n",
    "        if not kpi_data or feature_data.empty:\n",
    "            print(\"No data available for visualization\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Abaco Financial Intelligence Dashboard', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Portfolio Distribution by Delinquency\n",
    "        if 'dpd_buckets' in kpi_data and not kpi_data['dpd_buckets'].empty:\n",
    "            axes[0, 0].pie(kpi_data['dpd_buckets'].values, \n",
    "                          labels=kpi_data['dpd_buckets'].index,\n",
    "                          autopct='%1.1f%%', startangle=90)\n",
    "            axes[0, 0].set_title('Portfolio by Delinquency Buckets')\n",
    "        \n",
    "        # 2. Customer Segmentation\n",
    "        if 'customer_type' in feature_data.columns:\n",
    "            segment_counts = feature_data['customer_type'].value_counts()\n",
    "            axes[0, 1].bar(segment_counts.index, segment_counts.values)\n",
    "            axes[0, 1].set_title('Customer Distribution by Type')\n",
    "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Industry Analysis  \n",
    "        if 'industry' in feature_data.columns:\n",
    "            industry_aum = feature_data.groupby('industry')['balance'].sum().sort_values(ascending=False)\n",
    "            axes[0, 2].barh(industry_aum.index, industry_aum.values / 1e6)\n",
    "            axes[0, 2].set_title('AUM by Industry (Millions)')\n",
    "            \n",
    "        # 4. Risk Heatmap\n",
    "        if 'utilization_ratio' in feature_data.columns and 'dpd' in feature_data.columns:\n",
    "            risk_matrix = pd.crosstab(\n",
    "                pd.cut(feature_data['utilization_ratio'], bins=5), \n",
    "                pd.cut(feature_data['dpd'], bins=5),\n",
    "                values=feature_data['balance'], \n",
    "                aggfunc='sum', \n",
    "                normalize='columns'\n",
    "            )\n",
    "            sns.heatmap(risk_matrix.fillna(0), annot=True, fmt='.2f', ax=axes[1, 0], cmap='YlOrRd')\n",
    "            axes[1, 0].set_title('Risk Heatmap: Utilization vs DPD')\n",
    "            \n",
    "        # 5. Alert Summary\n",
    "        alert_counts = alerts_frame['severity'].value_counts() if not alerts_frame.empty else pd.Series()\n",
    "        if not alert_counts.empty:\n",
    "            colors = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}\n",
    "            alert_colors = [colors.get(x, 'gray') for x in alert_counts.index]\n",
    "            axes[1, 1].bar(alert_counts.index, alert_counts.values, color=alert_colors)\n",
    "            axes[1, 1].set_title('Active Alerts by Severity')\n",
    "        \n",
    "        # 6. KPI Summary\n",
    "        key_metrics = {\n",
    "            'AUM (M)': kpi_data.get('aum', 0) / 1e6,\n",
    "            'Clients': kpi_data.get('active_clients', 0),\n",
    "            'Default Rate': kpi_data.get('default_rate', 0) * 100,\n",
    "            'Weighted APR': kpi_data.get('weighted_apr', 0) * 100\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(list(key_metrics.items()), columns=['Metric', 'Value'])\n",
    "        axes[1, 2].axis('tight')\n",
    "        axes[1, 2].axis('off')\n",
    "        table = axes[1, 2].table(cellText=[[f\"{k}\", f\"{v:.2f}\"] for k, v in key_metrics.items()],\n",
    "                                colLabels=['KPI', 'Value'],\n",
    "                                cellLoc='center',\n",
    "                                loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "        axes[1, 2].set_title('Key Performance Indicators')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_trend_analysis(self, data: pd.DataFrame) -> None:\n",
    "        \"\"\"Plot trend analysis if date columns available\"\"\"\n",
    "        if data.empty or 'date' not in data.columns:\n",
    "            print(\"No temporal data available for trend analysis\")\n",
    "            return\n",
    "            \n",
    "        # Convert date and create monthly aggregation\n",
    "        data_copy = data.copy()\n",
    "        data_copy['date'] = pd.to_datetime(data_copy['date'])\n",
    "        data_copy['month'] = data_copy['date'].dt.to_period('M')\n",
    "        \n",
    "        monthly_trends = data_copy.groupby('month').agg({\n",
    "            'balance': 'sum',\n",
    "            'dpd': 'mean',\n",
    "            'utilization_ratio': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        if len(monthly_trends) > 1:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=self.figsize)\n",
    "            \n",
    "            monthly_trends.plot(x='month', y='balance', ax=axes[0], title='Monthly AUM Trend')\n",
    "            monthly_trends.plot(x='month', y='dpd', ax=axes[1], title='Average DPD Trend', color='orange')\n",
    "            monthly_trends.plot(x='month', y='utilization_ratio', ax=axes[2], title='Utilization Trend', color='green')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "if not feature_frame.empty and kpi_summary:\n",
    "    visualizer = FinancialVisualizer()\n",
    "    visualizer.create_dashboard(kpi_summary, feature_frame)\n",
    "    visualizer.plot_trend_analysis(feature_frame)\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Powered Financial Insights Engine\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class FinancialInsightsGenerator:\n",
    "    def __init__(self):\n",
    "        self.insights = []\n",
    "        \n",
    "    def analyze_portfolio_health(self, kpis: Dict[str, Any], features: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Generate AI-powered insights about portfolio health\"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        if not kpis or features.empty:\n",
    "            return [\"Insufficient data for analysis\"]\n",
    "        \n",
    "        # AUM Analysis\n",
    "        aum = kpis.get('aum', 0)\n",
    "        if aum > 100_000_000:  # 100M\n",
    "            insights.append(f\"🟢 Strong portfolio size: ${aum/1e6:.1f}M AUM indicates substantial business scale\")\n",
    "        elif aum > 10_000_000:  # 10M\n",
    "            insights.append(f\"🟡 Moderate portfolio: ${aum/1e6:.1f}M AUM shows growth potential\")\n",
    "        else:\n",
    "            insights.append(f\"🔴 Small portfolio: ${aum/1e6:.1f}M AUM suggests need for business expansion\")\n",
    "        \n",
    "        # Risk Analysis\n",
    "        default_rate = kpis.get('default_rate', 0)\n",
    "        if default_rate > 0.05:  # 5%\n",
    "            insights.append(f\"🔴 HIGH RISK: Default rate of {default_rate:.1%} exceeds industry benchmarks\")\n",
    "        elif default_rate > 0.02:  # 2%\n",
    "            insights.append(f\"🟡 MODERATE RISK: Default rate of {default_rate:.1%} requires monitoring\")\n",
    "        else:\n",
    "            insights.append(f\"🟢 LOW RISK: Default rate of {default_rate:.1%} is within healthy limits\")\n",
    "        \n",
    "        # Concentration Risk\n",
    "        concentration = kpis.get('concentration_top10', 0)\n",
    "        if concentration > 0.5:  # 50%\n",
    "            insights.append(f\"🔴 HIGH CONCENTRATION: Top 10 clients represent {concentration:.1%} of portfolio\")\n",
    "        elif concentration > 0.3:  # 30%\n",
    "            insights.append(f\"🟡 MODERATE CONCENTRATION: {concentration:.1%} in top 10 clients\")\n",
    "        else:\n",
    "            insights.append(f\"🟢 DIVERSIFIED: Well-distributed portfolio with {concentration:.1%} concentration\")\n",
    "        \n",
    "        # Profitability Analysis\n",
    "        weighted_apr = kpis.get('weighted_apr', 0)\n",
    "        if weighted_apr > 0.15:  # 15%\n",
    "            insights.append(f\"🟢 STRONG YIELDS: Weighted APR of {weighted_apr:.1%} indicates good pricing power\")\n",
    "        elif weighted_apr > 0.08:  # 8%\n",
    "            insights.append(f\"🟡 MODERATE YIELDS: APR of {weighted_apr:.1%} is competitive\")\n",
    "        else:\n",
    "            insights.append(f\"🔴 LOW YIELDS: APR of {weighted_apr:.1%} may impact profitability\")\n",
    "        \n",
    "        # Alert Analysis\n",
    "        if not alerts_frame.empty:\n",
    "            critical_alerts = len(alerts_frame[alerts_frame['severity'] == 'critical'])\n",
    "            if critical_alerts > 0:\n",
    "                insights.append(f\"⚠️ URGENT: {critical_alerts} critical alerts require immediate attention\")\n",
    "        \n",
    "        # Customer Mix Analysis\n",
    "        if 'customer_type' in features.columns:\n",
    "            customer_mix = features['customer_type'].value_counts(normalize=True)\n",
    "            if customer_mix.get('enterprise', 0) > 0.3:\n",
    "                insights.append(\"🟢 STABLE BASE: Strong enterprise customer presence provides stability\")\n",
    "            if customer_mix.get('micro', 0) > 0.5:\n",
    "                insights.append(\"🟡 RISK PROFILE: High micro-customer exposure increases operational complexity\")\n",
    "        \n",
    "        # B2G Analysis\n",
    "        b2g_percent = kpis.get('b2g_percent', 0)\n",
    "        if b2g_percent > 0.2:  # 20%\n",
    "            insights.append(f\"🟢 STABILITY: {b2g_percent:.1%} government/public sector exposure adds stability\")\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def generate_recommendations(self, insights: List[str], kpis: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate actionable recommendations based on insights\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Risk-based recommendations\n",
    "        default_rate = kpis.get('default_rate', 0)\n",
    "        if default_rate > 0.05:\n",
    "            recommendations.extend([\n",
    "                \"📋 Implement enhanced credit scoring models\",\n",
    "                \"🔍 Conduct portfolio stress testing\",\n",
    "                \"⚖️ Review and tighten underwriting criteria\"\n",
    "            ])\n",
    "        \n",
    "        # Concentration recommendations\n",
    "        concentration = kpis.get('concentration_top10', 0)\n",
    "        if concentration > 0.4:\n",
    "            recommendations.extend([\n",
    "                \"📈 Diversify customer acquisition strategy\",\n",
    "                \"🎯 Set concentration limits per client\",\n",
    "                \"🌐 Expand into new market segments\"\n",
    "            ])\n",
    "        \n",
    "        # Profitability recommendations\n",
    "        weighted_apr = kpis.get('weighted_apr', 0)\n",
    "        if weighted_apr < 0.1:\n",
    "            recommendations.extend([\n",
    "                \"💰 Review pricing strategy for competitive positioning\",\n",
    "                \"📊 Implement risk-based pricing models\",\n",
    "                \"🔄 Optimize cost of funds\"\n",
    "            ])\n",
    "        \n",
    "        # Operational recommendations (always relevant)\n",
    "        recommendations.extend([\n",
    "            \"🤖 Implement automated monitoring dashboards\",\n",
    "            \"📱 Deploy real-time alert systems\",\n",
    "            \"📈 Establish monthly portfolio review cycles\",\n",
    "            \"🎯 Define clear KPI targets and thresholds\"\n",
    "        ])\n",
    "        \n",
    "        return recommendations[:8]  # Limit to top 8 recommendations\n",
    "\n",
    "# Generate insights and recommendations\n",
    "insights_generator = FinancialInsightsGenerator()\n",
    "\n",
    "if kpi_summary and not feature_frame.empty:\n",
    "    portfolio_insights = insights_generator.analyze_portfolio_health(kpi_summary, feature_frame)\n",
    "    recommendations = insights_generator.generate_recommendations(portfolio_insights, kpi_summary)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🧠 ABACO FINANCIAL INTELLIGENCE INSIGHTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n📊 PORTFOLIO HEALTH ANALYSIS:\")\n",
    "    for i, insight in enumerate(portfolio_insights, 1):\n",
    "        print(f\"{i:2d}. {insight}\")\n",
    "    \n",
    "    print(\"\\n🎯 STRATEGIC RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i:2d}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n📋 EXECUTIVE SUMMARY:\")\n",
    "    print(f\"Portfolio Size: ${kpi_summary.get('aum', 0)/1e6:.1f}M | Active Clients: {kpi_summary.get('active_clients', 0):,}\")\n",
    "    print(f\"Risk Profile: {kpi_summary.get('default_rate', 0):.2%} default rate | APR: {kpi_summary.get('weighted_apr', 0):.1%}\")\n",
    "    print(f\"Alerts: {len(alerts_frame)} total ({len(alerts_frame[alerts_frame['severity'] == 'critical']) if not alerts_frame.empty else 0} critical)\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"Insufficient data for AI insights generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9018877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Reporting Engine\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class FinancialReportExporter:\n",
    "    def __init__(self, base_path: str = \"/workspaces/nextjs-with-supabase/data\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        \n",
    "    def export_summary_report(self, kpis: Dict[str, Any], insights: List[str], recommendations: List[str]) -> str:\n",
    "        \"\"\"Export comprehensive summary report\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"abaco_financial_report_{timestamp}.md\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        report_content = f\"\"\"# Abaco Financial Intelligence Report\n",
    "Generated: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Executive Summary\n",
    "- **Portfolio Size**: ${kpis.get('aum', 0)/1e6:.2f}M AUM\n",
    "- **Active Clients**: {kpis.get('active_clients', 0):,}\n",
    "- **Default Rate**: {kpis.get('default_rate', 0):.2%}\n",
    "- **Weighted APR**: {kpis.get('weighted_apr', 0):.2%}\n",
    "\n",
    "## Key Performance Indicators\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| AUM | ${kpis.get('aum', 0)/1e6:.2f}M |\n",
    "| Active Clients | {kpis.get('active_clients', 0):,} |\n",
    "| Credit Lines | ${kpis.get('credit_lines', 0)/1e6:.2f}M |\n",
    "| Default Rate | {kpis.get('default_rate', 0):.2%} |\n",
    "| Churn Rate | {kpis.get('churn_rate', 0):.2%} |\n",
    "| Weighted APR | {kpis.get('weighted_apr', 0):.2%} |\n",
    "| Concentration (Top 10) | {kpis.get('concentration_top10', 0):.2%} |\n",
    "| B2G Exposure | {kpis.get('b2g_percent', 0):.2%} |\n",
    "\n",
    "## Portfolio Health Insights\n",
    "\"\"\"\n",
    "        for i, insight in enumerate(insights, 1):\n",
    "            report_content += f\"{i}. {insight}\\n\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "## Strategic Recommendations\n",
    "\"\"\"\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            report_content += f\"{i}. {rec}\\n\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "## Data Quality Assessment\n",
    "- **Overall Score**: {quality_artifacts.get('score', 0):.1%}\n",
    "- **PDF Processing**: {quality_artifacts.get('pdf_completeness', 0):.1%}\n",
    "- **Active Alerts**: {len(alerts_frame)} ({len(alerts_frame[alerts_frame['severity'] == 'critical']) if not alerts_frame.empty else 0} critical)\n",
    "\n",
    "## Technical Notes\n",
    "- Analysis based on {len(feature_frame)} portfolio records\n",
    "- Feature engineering completed with {len(feature_frame.columns)} attributes\n",
    "- All calculations use industry-standard methodologies\n",
    "- Risk metrics aligned with Basel III guidelines\n",
    "\n",
    "---\n",
    "*Generated by Abaco Financial Intelligence Engine*\n",
    "\"\"\"\n",
    "        \n",
    "        # Write report to file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        return str(filepath)\n",
    "    \n",
    "    def export_data_tables(self) -> Dict[str, str]:\n",
    "        \"\"\"Export processed data tables\"\"\"\n",
    "        exports = {}\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Export feature frame\n",
    "        if not feature_frame.empty:\n",
    "            feature_path = self.base_path / f\"feature_data_{timestamp}.csv\"\n",
    "            feature_frame.to_csv(feature_path, index=False)\n",
    "            exports['features'] = str(feature_path)\n",
    "        \n",
    "        # Export alerts\n",
    "        if not alerts_frame.empty:\n",
    "            alerts_path = self.base_path / f\"alerts_{timestamp}.csv\"\n",
    "            alerts_frame.to_csv(alerts_path, index=False)\n",
    "            exports['alerts'] = str(alerts_path)\n",
    "        \n",
    "        # Export KPIs\n",
    "        if kpi_summary:\n",
    "            kpi_path = self.base_path / f\"kpis_{timestamp}.json\"\n",
    "            # Convert pandas objects to serializable format\n",
    "            kpi_serializable = {}\n",
    "            for k, v in kpi_summary.items():\n",
    "                if isinstance(v, pd.Series):\n",
    "                    kpi_serializable[k] = v.to_dict()\n",
    "                elif hasattr(v, 'item'):  # numpy scalar\n",
    "                    kpi_serializable[k] = v.item()\n",
    "                else:\n",
    "                    kpi_serializable[k] = v\n",
    "            \n",
    "            with open(kpi_path, 'w') as f:\n",
    "                json.dump(kpi_serializable, f, indent=2, default=str)\n",
    "            exports['kpis'] = str(kpi_path)\n",
    "        \n",
    "        return exports\n",
    "\n",
    "# Generate and export reports\n",
    "if kpi_summary and not feature_frame.empty:\n",
    "    exporter = FinancialReportExporter()\n",
    "    \n",
    "    # Export summary report\n",
    "    report_path = exporter.export_summary_report(\n",
    "        kpi_summary, \n",
    "        portfolio_insights if 'portfolio_insights' in locals() else [], \n",
    "        recommendations if 'recommendations' in locals() else []\n",
    "    )\n",
    "    \n",
    "    # Export data tables\n",
    "    data_exports = exporter.export_data_tables()\n",
    "    \n",
    "    print(f\"\\n📄 REPORTS GENERATED:\")\n",
    "    print(f\"📋 Summary Report: {report_path}\")\n",
    "    \n",
    "    for data_type, path in data_exports.items():\n",
    "        print(f\"📊 {data_type.title()} Data: {path}\")\n",
    "    \n",
    "    print(f\"\\n✅ Financial intelligence analysis complete!\")\n",
    "    print(f\"📁 All files saved to: /workspaces/nextjs-with-supabase/data/\")\n",
    "else:\n",
    "    print(\"⚠️ Insufficient data for report generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
